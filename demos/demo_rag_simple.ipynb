{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Simple RAG System\n",
    "\n",
    "**NLP Final Lecture - Live Demo**\n",
    "\n",
    "This notebook demonstrates a basic Retrieval-Augmented Generation (RAG) pipeline:\n",
    "1. Load documents\n",
    "2. Create embeddings and store in vector database\n",
    "3. Retrieve relevant chunks for a query\n",
    "4. Generate grounded response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install openai langchain langchain-openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Sample Documents\n",
    "\n",
    "For this demo, we'll use some recent AI news that GPT-4's training data wouldn't know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about recent events (post-training cutoff)\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"DeepSeek-R1 Release\",\n",
    "        \"content\": \"\"\"In January 2025, DeepSeek released R1, an open-source reasoning model \n",
    "        that matches OpenAI o1's performance on many benchmarks. The model was trained using \n",
    "        a novel approach called GRPO (Group Relative Policy Optimization) which doesn't \n",
    "        require a separate reward model. DeepSeek-R1 achieved 71% on AIME 2024 math problems, \n",
    "        up from 15.6% for the base model. The company released distilled versions ranging \n",
    "        from 1.5B to 70B parameters.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"OpenAI o1 Capabilities\",\n",
    "        \"content\": \"\"\"OpenAI's o1 model, released in September 2024, introduced a new paradigm \n",
    "        of test-time compute scaling. Unlike previous models that generate answers directly, \n",
    "        o1 'thinks' before responding, using hidden reasoning tokens. This allows the model \n",
    "        to solve complex problems that require multi-step reasoning. The o1-pro version \n",
    "        generates up to 5000+ tokens of reasoning for difficult problems.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Anthropic Claude 3.5\",\n",
    "        \"content\": \"\"\"Anthropic released Claude 3.5 Sonnet in 2024, which became known for \n",
    "        its strong coding abilities and longer context windows. Claude uses Constitutional AI \n",
    "        for alignment, where the model critiques its own outputs against a set of principles \n",
    "        rather than relying solely on human feedback. This approach allows for more scalable \n",
    "        alignment training.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"AI Agent Frameworks 2025\",\n",
    "        \"content\": \"\"\"By early 2025, AI agent frameworks have matured significantly. \n",
    "        LangChain and LlamaIndex remain popular choices for building RAG applications. \n",
    "        Microsoft's AutoGen enables multi-agent collaboration where specialized agents \n",
    "        work together on complex tasks. CrewAI focuses on role-based agent orchestration. \n",
    "        The main challenges remain reliability and cost management for production deployments.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Get embedding for a text using OpenAI API.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Create embeddings for all documents\n",
    "for doc in documents:\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"content\"])\n",
    "    print(f\"Embedded: {doc['title']} ({len(doc['embedding'])} dimensions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simple Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve(query, documents, top_k=2):\n",
    "    \"\"\"Retrieve most similar documents for a query.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for doc in documents:\n",
    "        sim = cosine_similarity(query_embedding, doc[\"embedding\"])\n",
    "        similarities.append((doc, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "query = \"What is GRPO and how does it work?\"\n",
    "\n",
    "results = retrieve(query, documents, top_k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for doc, score in results:\n",
    "    print(f\"  [{score:.3f}] {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Grounded Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(query, documents, top_k=2):\n",
    "    \"\"\"Generate a response using RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved = retrieve(query, documents, top_k=top_k)\n",
    "    \n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[Source: {doc['title']}]\\n{doc['content']}\" \n",
    "        for doc, _ in retrieved\n",
    "    ])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Use the following context to answer the question. \n",
    "If the answer is not in the context, say \"I don't have information about that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (cite your sources):\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content, retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Question about recent events\n",
    "query = \"What is GRPO and which model uses it?\"\n",
    "\n",
    "answer, sources = generate_rag_response(query, documents)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer:\\n{answer}\\n\")\n",
    "print(\"Sources used:\")\n",
    "for doc, score in sources:\n",
    "    print(f\"  - {doc['title']} (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Question NOT in the documents\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "answer, sources = generate_rag_response(query, documents)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")\n",
    "print(\"\\n(Note: The model correctly indicates this is not in the context)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive demo\n",
    "print(\"Try your own questions about:\")\n",
    "print(\"- DeepSeek-R1\")\n",
    "print(\"- OpenAI o1\")\n",
    "print(\"- Constitutional AI\")\n",
    "print(\"- AI Agent frameworks\")\n",
    "\n",
    "# Uncomment to use:\n",
    "# your_query = input(\"Your question: \")\n",
    "# answer, sources = generate_rag_response(your_query, documents)\n",
    "# print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **RAG separates knowledge from reasoning**: The LLM reasons, the database stores facts\n",
    "2. **Embeddings enable semantic search**: Similar meaning = similar vectors\n",
    "3. **Grounding reduces hallucination**: Model cites sources, can say \"I don't know\"\n",
    "4. **Simple implementation**: Core RAG is just embed -> search -> generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
