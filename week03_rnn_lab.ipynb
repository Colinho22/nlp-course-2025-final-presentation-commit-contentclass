{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Build Your Own Memory Network\n",
    "## From Simple RNNs to LSTMs - A Hands-on Journey\n",
    "\n",
    "In this lab, you'll build RNNs from scratch and watch them learn (and fail!) on real tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Ready to build memory networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Sequential Memory\n",
    "\n",
    "Let's start with a simple task: predicting the next character in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text for our experiments\n",
    "text = \"hello world! how are you doing today? hello again!\"\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {chars}\")\n",
    "\n",
    "# Convert text to indices\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "print(f\"\\nFirst 20 characters as indices: {data[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Simple RNN from Scratch\n",
    "\n",
    "Let's implement the core RNN equations:\n",
    "- $h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)$\n",
    "- $y_t = W_{hy} \\cdot h_t + b_y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, vocab_size, hidden_size=10):\n",
    "        \"\"\"Initialize a simple RNN.\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights (Xavier initialization)\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def forward_step(self, x, h_prev):\n",
    "        \"\"\"Forward pass for one time step.\"\"\"\n",
    "        # Update hidden state\n",
    "        h = np.tanh(self.Whh @ h_prev + self.Wxh @ x + self.bh)\n",
    "        \n",
    "        # Compute output\n",
    "        y = self.Why @ h + self.by\n",
    "        \n",
    "        # Apply softmax\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        return h, p\n",
    "    \n",
    "    def forward_sequence(self, inputs):\n",
    "        \"\"\"Process a full sequence.\"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = [h]\n",
    "        \n",
    "        for x in inputs:\n",
    "            # One-hot encode input\n",
    "            x_vec = np.zeros((self.vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            h, p = self.forward_step(x_vec, h)\n",
    "            outputs.append(p)\n",
    "            hidden_states.append(h)\n",
    "            \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# Create and test our RNN\n",
    "rnn = SimpleRNN(vocab_size, hidden_size=16)\n",
    "\n",
    "# Process first 5 characters\n",
    "outputs, hidden_states = rnn.forward_sequence(data[:5])\n",
    "print(\"RNN successfully created!\")\n",
    "print(f\"Hidden state shape: {hidden_states[0].shape}\")\n",
    "print(f\"Output shape: {outputs[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing Hidden States\n",
    "\n",
    "Let's see how hidden states evolve as the RNN processes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a longer sequence\n",
    "sequence_length = 20\n",
    "outputs, hidden_states = rnn.forward_sequence(data[:sequence_length])\n",
    "\n",
    "# Extract hidden states as matrix\n",
    "H = np.hstack([h for h in hidden_states[1:]])  # Skip initial zero state\n",
    "\n",
    "# Visualize hidden state evolution\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Hidden state activations over time\n",
    "im1 = ax1.imshow(H, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_ylabel('Hidden Unit')\n",
    "ax1.set_title('Hidden State Evolution')\n",
    "ax1.set_xticks(range(sequence_length))\n",
    "ax1.set_xticklabels([text[i] for i in range(sequence_length)], rotation=45)\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Plot 2: Hidden state magnitude over time\n",
    "hidden_magnitudes = np.linalg.norm(H, axis=0)\n",
    "ax2.plot(hidden_magnitudes, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Time Step')\n",
    "ax2.set_ylabel('Hidden State Magnitude')\n",
    "ax2.set_title('Memory Strength Over Time')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(sequence_length))\n",
    "ax2.set_xticklabels([text[i] for i in range(sequence_length)], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average hidden state magnitude: {np.mean(hidden_magnitudes):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Vanishing Gradient Problem\n",
    "\n",
    "Let's demonstrate why simple RNNs struggle with long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gradient_flow(sequence_length, decay_rate=0.9):\n",
    "    \"\"\"Simulate gradient decay through time.\"\"\"\n",
    "    gradients = []\n",
    "    current_gradient = 1.0\n",
    "    \n",
    "    for t in range(sequence_length):\n",
    "        gradients.append(current_gradient)\n",
    "        current_gradient *= decay_rate\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Simulate for different decay rates\n",
    "sequence_lengths = np.arange(1, 51)\n",
    "decay_rates = [0.99, 0.95, 0.9, 0.8, 0.5]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for rate in decay_rates:\n",
    "    gradients = [simulate_gradient_flow(length, rate)[-1] \n",
    "                 for length in sequence_lengths]\n",
    "    plt.semilogy(sequence_lengths, gradients, \n",
    "                 label=f'Decay rate = {rate}', linewidth=2)\n",
    "\n",
    "# Add threshold line\n",
    "plt.axhline(y=1e-5, color='r', linestyle='--', \n",
    "            label='Effective zero (1e-5)')\n",
    "\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "plt.title('Vanishing Gradient Problem in RNNs', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate specific examples\n",
    "for rate in [0.9, 0.5]:\n",
    "    gradient_20 = rate ** 20\n",
    "    gradient_50 = rate ** 50\n",
    "    print(f\"Decay rate {rate}:\")\n",
    "    print(f\"  After 20 steps: {gradient_20:.6f}\")\n",
    "    print(f\"  After 50 steps: {gradient_50:.10f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building an LSTM from Scratch\n",
    "\n",
    "Now let's implement LSTM with its clever gating mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM:\n",
    "    def __init__(self, vocab_size, hidden_size=10):\n",
    "        \"\"\"Initialize a simple LSTM.\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined weight matrix for all gates\n",
    "        # [input_gate; forget_gate; output_gate; candidate]\n",
    "        concat_size = vocab_size + hidden_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W = np.random.randn(4 * hidden_size, concat_size) * 0.01\n",
    "        self.b = np.zeros((4 * hidden_size, 1))\n",
    "        \n",
    "        # Output weights\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        \"\"\"Forward pass for one LSTM time step.\"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat = np.vstack([x, h_prev])\n",
    "        \n",
    "        # Compute all gates at once\n",
    "        gates = self.W @ concat + self.b\n",
    "        \n",
    "        # Split into individual gates\n",
    "        i_gate = self.sigmoid(gates[:self.hidden_size])      # Input gate\n",
    "        f_gate = self.sigmoid(gates[self.hidden_size:2*self.hidden_size])  # Forget gate\n",
    "        o_gate = self.sigmoid(gates[2*self.hidden_size:3*self.hidden_size])  # Output gate\n",
    "        c_candidate = np.tanh(gates[3*self.hidden_size:])    # Candidate values\n",
    "        \n",
    "        # Update cell state\n",
    "        c = f_gate * c_prev + i_gate * c_candidate\n",
    "        \n",
    "        # Update hidden state\n",
    "        h = o_gate * np.tanh(c)\n",
    "        \n",
    "        # Compute output\n",
    "        y = self.Why @ h + self.by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        # Return everything for visualization\n",
    "        gates_dict = {\n",
    "            'input': i_gate,\n",
    "            'forget': f_gate,\n",
    "            'output': o_gate,\n",
    "            'candidate': c_candidate\n",
    "        }\n",
    "        \n",
    "        return h, c, p, gates_dict\n",
    "    \n",
    "    def forward_sequence(self, inputs):\n",
    "        \"\"\"Process a full sequence.\"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = [h]\n",
    "        cell_states = [c]\n",
    "        all_gates = []\n",
    "        \n",
    "        for x in inputs:\n",
    "            # One-hot encode input\n",
    "            x_vec = np.zeros((self.vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            h, c, p, gates = self.forward_step(x_vec, h, c)\n",
    "            \n",
    "            outputs.append(p)\n",
    "            hidden_states.append(h)\n",
    "            cell_states.append(c)\n",
    "            all_gates.append(gates)\n",
    "            \n",
    "        return outputs, hidden_states, cell_states, all_gates\n",
    "\n",
    "# Create and test our LSTM\n",
    "lstm = SimpleLSTM(vocab_size, hidden_size=16)\n",
    "\n",
    "# Process first 20 characters\n",
    "outputs, hidden_states, cell_states, gates = lstm.forward_sequence(data[:20])\n",
    "print(\"LSTM successfully created!\")\n",
    "print(f\"Number of parameters in LSTM: {lstm.W.size + lstm.b.size + lstm.Why.size + lstm.by.size}\")\n",
    "print(f\"Number of parameters in RNN: {rnn.Wxh.size + rnn.Whh.size + rnn.Why.size + rnn.bh.size + rnn.by.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing LSTM Gates\n",
    "\n",
    "Let's see how LSTM gates control information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract gate activations\n",
    "sequence_length = 30\n",
    "outputs, hidden_states, cell_states, all_gates = lstm.forward_sequence(data[:sequence_length])\n",
    "\n",
    "# Prepare gate data for visualization\n",
    "forget_gates = np.hstack([g['forget'] for g in all_gates])\n",
    "input_gates = np.hstack([g['input'] for g in all_gates])\n",
    "output_gates = np.hstack([g['output'] for g in all_gates])\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "gates_data = [\n",
    "    (forget_gates, 'Forget Gate', 'Reds'),\n",
    "    (input_gates, 'Input Gate', 'Greens'),\n",
    "    (output_gates, 'Output Gate', 'Blues')\n",
    "]\n",
    "\n",
    "for ax, (gate_data, title, cmap) in zip(axes, gates_data):\n",
    "    im = ax.imshow(gate_data, aspect='auto', cmap=cmap, vmin=0, vmax=1)\n",
    "    ax.set_title(f'{title} Activations', fontsize=12)\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Hidden Unit')\n",
    "    ax.set_xticks(range(0, sequence_length, 5))\n",
    "    ax.set_xticklabels([text[i] for i in range(0, sequence_length, 5)], rotation=45)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('LSTM Gate Activations Over Time', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze gate statistics\n",
    "print(\"Gate Statistics:\")\n",
    "print(f\"Forget gate mean: {np.mean(forget_gates):.3f} (closer to 1 = remembering)\")\n",
    "print(f\"Input gate mean: {np.mean(input_gates):.3f} (closer to 1 = storing new info)\")\n",
    "print(f\"Output gate mean: {np.mean(output_gates):.3f} (closer to 1 = outputting info)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing RNN vs LSTM Memory\n",
    "\n",
    "Let's compare how well RNNs and LSTMs maintain information over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_retention_test(model_type, sequence_length=50):\n",
    "    \"\"\"Test how well a model retains information.\"\"\"\n",
    "    # Create a simple pattern: remember first character\n",
    "    test_sequence = [0] + [1] * (sequence_length - 1)  # First char is 0, rest are 1\n",
    "    \n",
    "    if model_type == 'RNN':\n",
    "        model = SimpleRNN(vocab_size, hidden_size=16)\n",
    "        outputs, hidden_states = model.forward_sequence(test_sequence)\n",
    "        # Convert hidden states to matrix\n",
    "        H = np.hstack([h for h in hidden_states[1:]])\n",
    "        memory_strength = np.linalg.norm(H, axis=0)\n",
    "        \n",
    "    else:  # LSTM\n",
    "        model = SimpleLSTM(vocab_size, hidden_size=16)\n",
    "        outputs, hidden_states, cell_states, _ = model.forward_sequence(test_sequence)\n",
    "        # Use cell states for LSTM (long-term memory)\n",
    "        C = np.hstack([c for c in cell_states[1:]])\n",
    "        memory_strength = np.linalg.norm(C, axis=0)\n",
    "    \n",
    "    return memory_strength\n",
    "\n",
    "# Test both models\n",
    "sequence_length = 40\n",
    "rnn_memory = memory_retention_test('RNN', sequence_length)\n",
    "lstm_memory = memory_retention_test('LSTM', sequence_length)\n",
    "\n",
    "# Normalize for comparison\n",
    "rnn_memory = rnn_memory / rnn_memory[0] if rnn_memory[0] != 0 else rnn_memory\n",
    "lstm_memory = lstm_memory / lstm_memory[0] if lstm_memory[0] != 0 else lstm_memory\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_axis = np.arange(sequence_length)\n",
    "\n",
    "plt.plot(x_axis, rnn_memory, 'r-', linewidth=2, label='Simple RNN', alpha=0.7)\n",
    "plt.plot(x_axis, lstm_memory, 'b-', linewidth=2, label='LSTM', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time Steps', fontsize=12)\n",
    "plt.ylabel('Relative Memory Strength', fontsize=12)\n",
    "plt.title('Memory Retention: RNN vs LSTM', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.text(sequence_length * 0.7, 0.52, '50% retention', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate retention metrics\n",
    "rnn_half_life = np.where(rnn_memory < 0.5)[0]\n",
    "lstm_half_life = np.where(lstm_memory < 0.5)[0]\n",
    "\n",
    "print(\"Memory Retention Analysis:\")\n",
    "print(f\"RNN memory at step 20: {rnn_memory[19]:.3f}\")\n",
    "print(f\"LSTM memory at step 20: {lstm_memory[19]:.3f}\")\n",
    "if len(rnn_half_life) > 0:\n",
    "    print(f\"RNN half-life: {rnn_half_life[0]} steps\")\n",
    "else:\n",
    "    print(\"RNN half-life: > 40 steps\")\n",
    "if len(lstm_half_life) > 0:\n",
    "    print(f\"LSTM half-life: {lstm_half_life[0]} steps\")\n",
    "else:\n",
    "    print(\"LSTM half-life: > 40 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Text Generation Challenge\n",
    "\n",
    "Let's train our models and see which generates better text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_char, length=50, temperature=1.0):\n",
    "    \"\"\"Generate text using trained model.\"\"\"\n",
    "    generated = seed_char\n",
    "    x = char_to_idx[seed_char]\n",
    "    \n",
    "    if isinstance(model, SimpleRNN):\n",
    "        h = np.zeros((model.hidden_size, 1))\n",
    "        \n",
    "        for _ in range(length):\n",
    "            x_vec = np.zeros((model.vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            h, p = model.forward_step(x_vec, h)\n",
    "            \n",
    "            # Sample with temperature\n",
    "            p = np.power(p, 1/temperature)\n",
    "            p = p / np.sum(p)\n",
    "            x = np.random.choice(range(model.vocab_size), p=p.ravel())\n",
    "            generated += idx_to_char[x]\n",
    "            \n",
    "    else:  # LSTM\n",
    "        h = np.zeros((model.hidden_size, 1))\n",
    "        c = np.zeros((model.hidden_size, 1))\n",
    "        \n",
    "        for _ in range(length):\n",
    "            x_vec = np.zeros((model.vocab_size, 1))\n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            h, c, p, _ = model.forward_step(x_vec, h, c)\n",
    "            \n",
    "            # Sample with temperature\n",
    "            p = np.power(p, 1/temperature)\n",
    "            p = p / np.sum(p)\n",
    "            x = np.random.choice(range(model.vocab_size), p=p.ravel())\n",
    "            generated += idx_to_char[x]\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Generate text with both models\n",
    "print(\"=\" * 60)\n",
    "print(\"Text Generation Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "seed_char = 'h'\n",
    "print(f\"\\nSeed character: '{seed_char}'\\n\")\n",
    "\n",
    "# RNN generation\n",
    "print(\"Simple RNN generated text:\")\n",
    "rnn_text = generate_text(rnn, seed_char, length=50, temperature=0.8)\n",
    "print(f\"  {rnn_text}\")\n",
    "\n",
    "print(\"\\nLSTM generated text:\")\n",
    "lstm_text = generate_text(lstm, seed_char, length=50, temperature=0.8)\n",
    "print(f\"  {lstm_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Note: These are untrained models with random weights.\")\n",
    "print(\"With training, LSTM would show much better coherence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Building a GRU (Simplified LSTM)\n",
    "\n",
    "Let's implement a GRU, which achieves similar performance to LSTM with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU:\n",
    "    def __init__(self, vocab_size, hidden_size=10):\n",
    "        \"\"\"Initialize a simple GRU.\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # GRU has 3 weight matrices: reset, update, candidate\n",
    "        concat_size = vocab_size + hidden_size\n",
    "        \n",
    "        self.Wr = np.random.randn(hidden_size, concat_size) * 0.01  # Reset gate\n",
    "        self.Wz = np.random.randn(hidden_size, concat_size) * 0.01  # Update gate\n",
    "        self.Wh = np.random.randn(hidden_size, concat_size) * 0.01  # Candidate\n",
    "        \n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output weights\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward_step(self, x, h_prev):\n",
    "        \"\"\"Forward pass for one GRU time step.\"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat = np.vstack([x, h_prev])\n",
    "        \n",
    "        # Reset gate\n",
    "        r = self.sigmoid(self.Wr @ concat + self.br)\n",
    "        \n",
    "        # Update gate\n",
    "        z = self.sigmoid(self.Wz @ concat + self.bz)\n",
    "        \n",
    "        # Candidate hidden state (with reset gate applied)\n",
    "        concat_reset = np.vstack([x, r * h_prev])\n",
    "        h_candidate = np.tanh(self.Wh @ concat_reset + self.bh)\n",
    "        \n",
    "        # Final hidden state (interpolation)\n",
    "        h = (1 - z) * h_prev + z * h_candidate\n",
    "        \n",
    "        # Output\n",
    "        y = self.Why @ h + self.by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        return h, p, {'reset': r, 'update': z}\n",
    "\n",
    "# Create GRU and compare parameter counts\n",
    "gru = SimpleGRU(vocab_size, hidden_size=16)\n",
    "\n",
    "# Count parameters\n",
    "lstm_params = lstm.W.size + lstm.b.size + lstm.Why.size + lstm.by.size\n",
    "gru_params = (gru.Wr.size + gru.Wz.size + gru.Wh.size + \n",
    "              gru.br.size + gru.bz.size + gru.bh.size +\n",
    "              gru.Why.size + gru.by.size)\n",
    "rnn_params = (rnn.Wxh.size + rnn.Whh.size + rnn.Why.size + \n",
    "              rnn.bh.size + rnn.by.size)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Simple RNN parameters: {rnn_params}\")\n",
    "print(f\"LSTM parameters: {lstm_params}\")\n",
    "print(f\"GRU parameters: {gru_params}\")\n",
    "print(f\"\\nGRU has {lstm_params - gru_params} fewer parameters than LSTM\")\n",
    "print(f\"GRU has {gru_params - rnn_params} more parameters than simple RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Summary and Key Takeaways\n",
    "\n",
    "Let's visualize what we've learned about RNNs, LSTMs, and GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Model Complexity\n",
    "models = ['Simple RNN', 'GRU', 'LSTM']\n",
    "params = [rnn_params, gru_params, lstm_params]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E77E']\n",
    "\n",
    "axes[0, 0].bar(models, params, color=colors)\n",
    "axes[0, 0].set_ylabel('Number of Parameters')\n",
    "axes[0, 0].set_title('Model Complexity')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Memory Retention (simulated)\n",
    "x = np.linspace(0, 50, 100)\n",
    "rnn_retention = np.exp(-x/10)\n",
    "lstm_retention = np.exp(-x/40)\n",
    "gru_retention = np.exp(-x/35)\n",
    "\n",
    "axes[0, 1].plot(x, rnn_retention, 'r-', label='RNN', linewidth=2)\n",
    "axes[0, 1].plot(x, gru_retention, 'g-', label='GRU', linewidth=2)\n",
    "axes[0, 1].plot(x, lstm_retention, 'b-', label='LSTM', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Sequence Length')\n",
    "axes[0, 1].set_ylabel('Memory Strength')\n",
    "axes[0, 1].set_title('Memory Retention Capability')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training Speed (relative)\n",
    "models = ['Simple RNN', 'GRU', 'LSTM']\n",
    "speed = [100, 70, 60]  # Relative speed\n",
    "axes[1, 0].barh(models, speed, color=colors)\n",
    "axes[1, 0].set_xlabel('Relative Training Speed (%)')\n",
    "axes[1, 0].set_title('Training Efficiency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Use Case Recommendations\n",
    "use_cases_text = \"\"\"\n",
    "When to use each model:\n",
    "\n",
    "Simple RNN:\n",
    "• Short sequences (< 10 steps)\n",
    "• Simple patterns\n",
    "• Speed is critical\n",
    "\n",
    "GRU:\n",
    "• Medium sequences (10-100 steps)\n",
    "• Good balance of performance/speed\n",
    "• Limited computational resources\n",
    "\n",
    "LSTM:\n",
    "• Long sequences (> 100 steps)\n",
    "• Complex dependencies\n",
    "• Best accuracy needed\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.1, 0.5, use_cases_text, fontsize=10, \n",
    "                verticalalignment='center', family='monospace')\n",
    "axes[1, 1].set_xlim(0, 1)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].set_title('Use Case Recommendations')\n",
    "\n",
    "plt.suptitle('RNN vs GRU vs LSTM: Complete Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Lab Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou've successfully:\")\n",
    "print(\"1. Built RNN, LSTM, and GRU from scratch\")\n",
    "print(\"2. Visualized hidden states and gate activations\")\n",
    "print(\"3. Demonstrated the vanishing gradient problem\")\n",
    "print(\"4. Compared memory retention capabilities\")\n",
    "print(\"5. Generated text with different architectures\")\n",
    "print(\"\\nNext steps: Train these models on real data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}