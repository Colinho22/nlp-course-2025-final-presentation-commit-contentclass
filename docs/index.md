---
layout: default
title: Home
---

<div class="hero">
  <h1>Natural Language Processing</h1>
  <p class="hero-subtitle">
    Graduate-level course covering the journey from statistical language models to modern transformer architectures
  </p>
  <div class="hero-actions">
    <a href="{{ '/embeddings/' | relative_url }}" class="btn btn-primary">Start with Embeddings</a>
    <a href="{{ '/embeddings/gallery/' | relative_url }}" class="btn btn-secondary">View Charts</a>
  </div>
</div>

## Course Overview

This comprehensive NLP course takes you from foundational concepts to cutting-edge techniques:

- **12 weeks** of structured content
- **500+ slides** across all modules
- **250+ visualizations** generated with Python
- **Interactive notebooks** with hands-on exercises

## Current Modules

<div class="card-grid" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1.5rem; margin: 2rem 0;">

<div class="card">
  <h3 class="card-title">Word Embeddings</h3>
  <p class="card-description">
    From one-hot encoding to dense vector representations. Covers Word2Vec, GloVe, and contextual embeddings.
  </p>
  <span class="badge badge-success">Available</span>
  <div style="margin-top: 1rem;">
    <a href="{{ '/embeddings/' | relative_url }}" class="btn btn-sm btn-primary">Explore Module</a>
    <a href="{{ '/embeddings/gallery/' | relative_url }}" class="btn btn-sm btn-secondary">View Charts</a>
  </div>
</div>

<div class="card">
  <h3 class="card-title">Week 1: Foundations</h3>
  <p class="card-description">
    Statistical language models, n-grams, and probability foundations.
  </p>
  <span class="badge badge-primary">Coming Soon</span>
</div>

<div class="card">
  <h3 class="card-title">Week 3: RNN/LSTM</h3>
  <p class="card-description">
    Recurrent neural networks and long short-term memory architectures.
  </p>
  <span class="badge badge-primary">Coming Soon</span>
</div>

<div class="card">
  <h3 class="card-title">Week 5: Transformers</h3>
  <p class="card-description">
    Attention mechanisms and the transformer architecture that revolutionized NLP.
  </p>
  <span class="badge badge-primary">Coming Soon</span>
</div>

</div>

## Features

### Interactive Learning

- **Jupyter Notebooks**: Hands-on exercises with real code
- **Binder Integration**: Run notebooks in the cloud instantly
- **Google Colab Support**: Alternative cloud environment

### Rich Visualizations

- **33+ Embeddings Charts**: Word2Vec, training dynamics, vector arithmetic
- **Professional Quality**: Publication-ready PDF and PNG formats
- **Interactive Gallery**: Filter and explore all visualizations

### Mathematical Foundations

- **Intuitive Explanations**: Core concepts explained simply
- **Detailed Appendix**: Full mathematical derivations for those who want depth
- **MathJax Rendering**: Beautiful equation display

## Getting Started

1. **Explore Embeddings**: Start with our comprehensive [embeddings module]({{ '/embeddings/' | relative_url }})
2. **View Visualizations**: Browse the [chart gallery]({{ '/embeddings/gallery/' | relative_url }})
3. **Run Notebooks**: Launch interactive labs via Binder or Colab
4. **Deep Dive**: Check the [math appendix]({{ '/embeddings/math-appendix/' | relative_url }}) for derivations

## Resources

- [GitHub Repository](https://github.com/Digital-AI-Finance/Natural-Language-Processing)
- [Bibliography]({{ '/bibliography/' | relative_url }})
- [All Notebooks]({{ '/notebooks/' | relative_url }})

---

<p style="text-align: center; color: #666; margin-top: 3rem;">
  <em>Part of the Digital AI Finance educational initiative</em>
</p>
