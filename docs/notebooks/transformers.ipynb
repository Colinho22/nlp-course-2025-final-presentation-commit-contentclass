{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Build Your Own Transformer - Interactive Lab\n",
    "\n",
    "## Learning Objectives\n",
    "1. Implement self-attention from scratch\n",
    "2. Build multi-head attention\n",
    "3. Create a complete transformer block\n",
    "4. Compare with RNN performance\n",
    "5. Visualize attention patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Self-Attention\n",
    "\n",
    "### 1.1 The Math Behind Attention\n",
    "\n",
    "The attention formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let's implement it step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix [batch_size, seq_len, d_k]\n",
    "        K: Key matrix [batch_size, seq_len, d_k]\n",
    "        V: Value matrix [batch_size, seq_len, d_v]\n",
    "        mask: Optional mask [batch_size, seq_len, seq_len]\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output [batch_size, seq_len, d_v]\n",
    "        attention_weights: Attention weights [batch_size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute QK^T\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Step 3: Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Multiply by values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with a simple example\n",
    "batch_size, seq_len, d_model = 1, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualizing Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, words=None, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \"\"\"\n",
    "    # Get the first batch item\n",
    "    weights = attention_weights[0].detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, cmap='YlOrRd', cbar=True, square=True,\n",
    "                xticklabels=words, yticklabels=words,\n",
    "                vmin=0, vmax=1, annot=True, fmt='.2f')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Keys')\n",
    "    plt.ylabel('Queries')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example sentence\n",
    "words = ['The', 'cat', 'sat', 'mat']\n",
    "visualize_attention(weights, words, \"Self-Attention: Each word attends to all words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Head Attention\n",
    "\n",
    "### 2.1 Why Multiple Heads?\n",
    "Different heads can focus on different types of relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # 1. Linear projections in batch from d_model => h x d_k\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. Apply attention on all the projected vectors in batch\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # 3. Concatenate heads and put through final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights\n",
    "    \n",
    "    def attention(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Multi-head output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Each head dimension: {d_model // n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Different Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multihead_attention(attn_weights, n_heads_to_show=4):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns from multiple heads.\n",
    "    \"\"\"\n",
    "    batch_size, n_heads, seq_len, _ = attn_weights.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_heads_to_show, figsize=(15, 4))\n",
    "    \n",
    "    for head_idx in range(n_heads_to_show):\n",
    "        weights = attn_weights[0, head_idx].detach().cpu().numpy()\n",
    "        \n",
    "        im = axes[head_idx].imshow(weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "        axes[head_idx].set_title(f'Head {head_idx + 1}')\n",
    "        axes[head_idx].set_xlabel('Position')\n",
    "        if head_idx == 0:\n",
    "            axes[head_idx].set_ylabel('Position')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes.ravel().tolist(), fraction=0.02)\n",
    "    plt.suptitle('Different Attention Heads Focus on Different Patterns', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_multihead_attention(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Transformer Block\n",
    "\n",
    "### 3.1 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Visualize positional encoding\n",
    "def visualize_positional_encoding(d_model=128, max_len=100):\n",
    "    pe = PositionalEncoding(d_model, max_len)\n",
    "    encoding = pe.pe[:max_len, 0, :].numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(encoding.T, cmap='RdBu', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Dimension')\n",
    "    plt.title('Positional Encoding Pattern (Sinusoidal)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Complete Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, attn_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# Test transformer block\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 20\n",
    "batch_size = 4\n",
    "\n",
    "transformer_block = TransformerBlock(d_model, n_heads, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attn_weights = transformer_block(x)\n",
    "\n",
    "print(f\"Transformer block output shape: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build a Mini Language Model\n",
    "\n",
    "### 4.1 Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=4, \n",
    "                 d_ff=1024, max_len=100, dropout=0.1):\n",
    "        super(MiniTransformer, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Token embeddings and positional encoding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        attention_weights = []\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x, attn_w = transformer(x, mask)\n",
    "            attention_weights.append(attn_w)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)\n",
    "        output = self.fc_out(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create a mini transformer\n",
    "vocab_size = 10000\n",
    "model = MiniTransformer(vocab_size).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Mini Transformer Parameters: {total_params:,}\")\n",
    "print(f\"That's {total_params/1e6:.2f}M parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Text with Your Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_tokens, max_length=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the transformer model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = start_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get predictions\n",
    "            outputs, _ = model(generated)\n",
    "            \n",
    "            # Get the last token predictions\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Example generation (with random weights - not trained)\n",
    "start_tokens = torch.randint(0, vocab_size, (1, 5)).to(device)\n",
    "generated_sequence = generate_text(model, start_tokens, max_length=20)\n",
    "print(f\"Generated sequence shape: {generated_sequence.shape}\")\n",
    "print(f\"Generated tokens: {generated_sequence[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Transformer vs RNN Comparison\n",
    "\n",
    "### 5.1 Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, n_layers=2):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "def compare_speed(seq_lengths=[10, 50, 100, 200, 500]):\n",
    "    \"\"\"\n",
    "    Compare processing speed of Transformer vs RNN.\n",
    "    \"\"\"\n",
    "    vocab_size = 5000\n",
    "    batch_size = 32\n",
    "    \n",
    "    transformer = MiniTransformer(vocab_size, d_model=128, n_heads=4, n_layers=2).to(device)\n",
    "    rnn = SimpleRNN(vocab_size, hidden_size=128, n_layers=2).to(device)\n",
    "    \n",
    "    transformer_times = []\n",
    "    rnn_times = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "        \n",
    "        # Time Transformer\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = transformer(x)\n",
    "        transformer_times.append(time.time() - start)\n",
    "        \n",
    "        # Time RNN\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = rnn(x)\n",
    "        rnn_times.append(time.time() - start)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(seq_lengths, transformer_times, 'o-', label='Transformer', linewidth=2, markersize=8)\n",
    "    plt.plot(seq_lengths, rnn_times, 's-', label='RNN', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Sequence Length', fontsize=12)\n",
    "    plt.ylabel('Processing Time (seconds)', fontsize=12)\n",
    "    plt.title('Transformer vs RNN Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return transformer_times, rnn_times\n",
    "\n",
    "transformer_times, rnn_times = compare_speed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Attention Range Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_range(model, seq_len=50):\n",
    "    \"\"\"\n",
    "    Analyze how far transformer can attend.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.randint(0, 1000, (1, seq_len)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, attention_weights = model(x)\n",
    "    \n",
    "    # Get attention from last layer\n",
    "    last_layer_attention = attention_weights[-1][0].mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    # Calculate average attention distance\n",
    "    distances = []\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if last_layer_attention[i, j] > 0.1:  # Threshold\n",
    "                distances.append(abs(i - j))\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: Attention heatmap\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(last_layer_attention, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title('Attention Pattern (Last Layer Average)')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Position')\n",
    "    \n",
    "    # Subplot 2: Distance histogram\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(distances, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Attention Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Attention Distances')\n",
    "    plt.axvline(np.mean(distances), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(distances):.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average attention distance: {np.mean(distances):.2f}\")\n",
    "    print(f\"Max attention distance: {np.max(distances)}\")\n",
    "\n",
    "analyze_attention_range(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Positional Encoding Explorer\n",
    "\n",
    "### 6.1 Interactive Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_positional_encoding(d_model_values=[64, 128, 256, 512]):\n",
    "    \"\"\"\n",
    "    Explore how positional encoding changes with model dimension.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, d_model in enumerate(d_model_values):\n",
    "        pe = PositionalEncoding(d_model, max_len=100)\n",
    "        encoding = pe.pe[:50, 0, :].numpy()\n",
    "        \n",
    "        im = axes[idx].imshow(encoding.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "        axes[idx].set_title(f'd_model = {d_model}')\n",
    "        axes[idx].set_xlabel('Position')\n",
    "        axes[idx].set_ylabel('Dimension')\n",
    "        plt.colorbar(im, ax=axes[idx])\n",
    "    \n",
    "    plt.suptitle('Positional Encoding Patterns for Different Model Dimensions', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explore_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Effect of Removing Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_without_positional_encoding():\n",
    "    \"\"\"\n",
    "    Show what happens without positional encoding.\n",
    "    \"\"\"\n",
    "    # Create two models: with and without PE\n",
    "    vocab_size = 100\n",
    "    d_model = 64\n",
    "    seq_len = 10\n",
    "    \n",
    "    # Test sequences that are permutations\n",
    "    seq1 = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    seq2 = torch.tensor([[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]])  # Reversed\n",
    "    seq3 = torch.tensor([[5, 3, 8, 1, 10, 2, 7, 4, 9, 6]])  # Shuffled\n",
    "    \n",
    "    # Model with positional encoding\n",
    "    model_with_pe = MiniTransformer(vocab_size, d_model=d_model, n_heads=4, n_layers=1)\n",
    "    model_with_pe.eval()\n",
    "    \n",
    "    # Model without positional encoding (hack: set PE to zero)\n",
    "    model_without_pe = MiniTransformer(vocab_size, d_model=d_model, n_heads=4, n_layers=1)\n",
    "    model_without_pe.pos_encoding.pe.data.fill_(0)  # Zero out positional encoding\n",
    "    model_without_pe.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # With PE\n",
    "        out1_pe, _ = model_with_pe(seq1)\n",
    "        out2_pe, _ = model_with_pe(seq2)\n",
    "        out3_pe, _ = model_with_pe(seq3)\n",
    "        \n",
    "        # Without PE\n",
    "        out1_no_pe, _ = model_without_pe(seq1)\n",
    "        out2_no_pe, _ = model_without_pe(seq2)\n",
    "        out3_no_pe, _ = model_without_pe(seq3)\n",
    "    \n",
    "    # Compare outputs\n",
    "    print(\"With Positional Encoding:\")\n",
    "    print(f\"  Difference between original and reversed: {torch.mean(torch.abs(out1_pe - out2_pe)):.4f}\")\n",
    "    print(f\"  Difference between original and shuffled: {torch.mean(torch.abs(out1_pe - out3_pe)):.4f}\")\n",
    "    \n",
    "    print(\"\\nWithout Positional Encoding:\")\n",
    "    print(f\"  Difference between original and reversed: {torch.mean(torch.abs(out1_no_pe - out2_no_pe)):.4f}\")\n",
    "    print(f\"  Difference between original and shuffled: {torch.mean(torch.abs(out1_no_pe - out3_no_pe)):.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Insight: Without PE, the model can't distinguish between different orders!\")\n",
    "\n",
    "test_without_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Advanced Experiments\n",
    "\n",
    "### 7.1 Attention Masking for Autoregressive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask for autoregressive generation.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    return mask == 0\n",
    "\n",
    "def visualize_causal_mask(seq_len=10):\n",
    "    mask = create_causal_mask(seq_len)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(mask, cmap='RdYlGn', aspect='auto')\n",
    "    plt.colorbar(label='Can Attend')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.title('Causal Mask: Each Position Can Only Attend to Previous Positions')\n",
    "    \n",
    "    # Add annotations\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            text = 'âœ“' if mask[i, j] else 'âœ—'\n",
    "            color = 'white' if mask[i, j] else 'black'\n",
    "            plt.text(j, i, text, ha='center', va='center', color=color, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_causal_mask(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Gradient Flow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow(model, seq_len=20):\n",
    "    \"\"\"\n",
    "    Analyze gradient flow through transformer layers.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    x = torch.randint(0, 1000, (1, seq_len)).to(device)\n",
    "    output, _ = model(x)\n",
    "    \n",
    "    # Create a dummy loss and backward\n",
    "    loss = output.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient magnitudes\n",
    "    grad_magnitudes = []\n",
    "    layer_names = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_magnitudes.append(param.grad.abs().mean().item())\n",
    "            layer_names.append(name.split('.')[0])\n",
    "    \n",
    "    # Plot gradient flow\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(grad_magnitudes)), grad_magnitudes)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Average Gradient Magnitude')\n",
    "    plt.title('Gradient Flow Through Transformer Layers')\n",
    "    plt.xticks(range(len(layer_names)), layer_names, rotation=45, ha='right')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Max gradient: {max(grad_magnitudes):.6f}\")\n",
    "    print(f\"Min gradient: {min(grad_magnitudes):.6f}\")\n",
    "    print(f\"Gradient ratio (max/min): {max(grad_magnitudes)/min(grad_magnitudes):.2f}\")\n",
    "\n",
    "analyze_gradient_flow(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "1. **Self-Attention**: The core mechanism that lets each word look at all other words\n",
    "2. **Multi-Head Attention**: Different heads capture different types of relationships\n",
    "3. **Positional Encoding**: Essential for understanding word order\n",
    "4. **Parallelization**: Transformers process all positions simultaneously\n",
    "5. **Gradient Flow**: Residual connections help with training deep models\n",
    "\n",
    "### Key Insights:\n",
    "- Transformers are **faster** than RNNs for long sequences\n",
    "- They can capture **long-range dependencies** effectively\n",
    "- Without positional encoding, they're **permutation invariant**\n",
    "- The architecture is surprisingly **simple and elegant**\n",
    "\n",
    "### Your Mini-Transformer:\n",
    "- Has ~2.5M parameters (GPT-3 has 175B!)\n",
    "- Can process sequences in parallel\n",
    "- Uses the same architecture as state-of-the-art models\n",
    "- Ready for training on real data!\n",
    "\n",
    "### Next Steps:\n",
    "1. Train your transformer on real text data\n",
    "2. Experiment with different architectures\n",
    "3. Try pre-training and fine-tuning\n",
    "4. Scale up to bigger models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Congratulations! You've built your own Transformer from scratch!\")\n",
    "print(\"\\nðŸ“Š Your achievements:\")\n",
    "print(\"âœ… Implemented scaled dot-product attention\")\n",
    "print(\"âœ… Built multi-head attention mechanism\")\n",
    "print(\"âœ… Created positional encoding\")\n",
    "print(\"âœ… Assembled a complete transformer block\")\n",
    "print(\"âœ… Compared performance with RNNs\")\n",
    "print(\"âœ… Visualized attention patterns\")\n",
    "print(\"\\nðŸš€ You're ready to tackle Week 6: Pre-trained Language Models!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}