{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 6: BERT Fine-tuning Lab - Enhanced Edition\n## Understanding Pre-trained Models Through Hands-on Practice\n\n### ðŸš€ The Paradigm Shift in NLP\n\nIn this enhanced notebook, we'll explore:\n1. **The Computational Revolution** - Why pre-training changed everything\n2. **Masked Language Modeling** - How BERT learns from text\n3. **Fine-tuning vs Training from Scratch** - See the 100x difference\n4. **LoRA and Efficient Fine-tuning** - Modern techniques\n5. **Environmental Impact** - Calculate carbon footprint\n6. **Attention Visualization** - See what BERT \"sees\"\n\n**Time to complete**: 60 minutes\n**Prerequisites**: Basic Python, PyTorch basics\n**New**: Interactive paradigm shift demonstrations!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation\n",
    "\n",
    "First, let's install the necessary libraries. This might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install transformers torch datasets matplotlib seaborn numpy pandas scikit-learn -q\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    BertConfig,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# The Paradigm Shift: Computational Cost Calculator\n# This demonstrates why pre-training revolutionized NLP\n\nclass ParadigmShiftCalculator:\n    \"\"\"Calculate and visualize the impact of pre-training\"\"\"\n    \n    def __init__(self):\n        self.cost_per_model = 500_000  # USD\n        self.training_time = 336  # hours (2 weeks)\n        self.gpu_cost_per_hour = 3  # USD\n        self.carbon_per_kwh = 0.5  # kg CO2\n        self.kwh_per_gpu_hour = 0.4  # kilowatt hours\n    \n    def calculate_traditional_approach(self, n_companies=20):\n        \"\"\"Calculate cost for traditional approach (each company trains from scratch)\"\"\"\n        total_cost = n_companies * self.cost_per_model\n        total_time = n_companies * self.training_time\n        total_gpu_cost = total_time * self.gpu_cost_per_hour\n        \n        # 90% of effort is duplicate (learning general language)\n        duplicate_cost = total_cost * 0.9\n        \n        # Carbon footprint\n        total_carbon = n_companies * self.training_time * self.kwh_per_gpu_hour * self.carbon_per_kwh\n        \n        return {\n            'total_cost': total_cost,\n            'duplicate_cost': duplicate_cost,\n            'total_time': total_time,\n            'gpu_cost': total_gpu_cost,\n            'carbon_kg': total_carbon\n        }\n    \n    def calculate_pretrain_approach(self, n_companies=20):\n        \"\"\"Calculate cost with pre-training approach\"\"\"\n        # One-time pre-training cost\n        pretrain_cost = self.cost_per_model\n        pretrain_time = self.training_time\n        \n        # Fine-tuning cost per company (10% of full training)\n        finetune_cost_per = self.cost_per_model * 0.1\n        finetune_time_per = self.training_time * 0.1\n        \n        total_cost = pretrain_cost + (n_companies * finetune_cost_per)\n        total_time = pretrain_time + (n_companies * finetune_time_per)\n        total_gpu_cost = total_time * self.gpu_cost_per_hour\n        \n        # Carbon footprint\n        total_carbon = total_time * self.kwh_per_gpu_hour * self.carbon_per_kwh\n        \n        return {\n            'total_cost': total_cost,\n            'duplicate_cost': 0,  # No duplication!\n            'total_time': total_time,\n            'gpu_cost': total_gpu_cost,\n            'carbon_kg': total_carbon\n        }\n    \n    def compare_approaches(self, n_companies=20):\n        \"\"\"Compare traditional vs pre-training approaches\"\"\"\n        traditional = self.calculate_traditional_approach(n_companies)\n        pretrain = self.calculate_pretrain_approach(n_companies)\n        \n        savings = {\n            'cost_saved': traditional['total_cost'] - pretrain['total_cost'],\n            'time_saved': traditional['total_time'] - pretrain['total_time'],\n            'carbon_saved': traditional['carbon_kg'] - pretrain['carbon_kg'],\n            'efficiency_gain': traditional['total_cost'] / pretrain['total_cost']\n        }\n        \n        return traditional, pretrain, savings\n\n# Create calculator and run comparison\ncalculator = ParadigmShiftCalculator()\ntraditional, pretrain, savings = calculator.compare_approaches(20)\n\nprint(\"=\"*70)\nprint(\"THE $10 MILLION PROBLEM: 20 Companies Training NLP Models\")\nprint(\"=\"*70)\nprint(\"\\nðŸ”´ TRADITIONAL APPROACH (Everyone trains from scratch):\")\nprint(f\"   Total Cost: ${traditional['total_cost']:,.0f}\")\nprint(f\"   Wasted on Duplication: ${traditional['duplicate_cost']:,.0f} (90%!)\")\nprint(f\"   Total GPU Hours: {traditional['total_time']:,.0f}\")\nprint(f\"   Carbon Footprint: {traditional['carbon_kg']:,.0f} kg CO2\")\nprint(f\"   Equivalent to: {traditional['carbon_kg']/5000:.0f} cars driven for a year\")\n\nprint(\"\\nðŸŸ¢ PRE-TRAINING APPROACH (Train once, fine-tune many):\")\nprint(f\"   Total Cost: ${pretrain['total_cost']:,.0f}\")\nprint(f\"   Wasted on Duplication: $0\")\nprint(f\"   Total GPU Hours: {pretrain['total_time']:,.0f}\")\nprint(f\"   Carbon Footprint: {pretrain['carbon_kg']:,.0f} kg CO2\")\n\nprint(\"\\nðŸ’° SAVINGS WITH PRE-TRAINING:\")\nprint(f\"   Money Saved: ${savings['cost_saved']:,.0f}\")\nprint(f\"   Time Saved: {savings['time_saved']:,.0f} GPU hours\")\nprint(f\"   Carbon Saved: {savings['carbon_saved']:,.0f} kg CO2\")\nprint(f\"   Efficiency Gain: {savings['efficiency_gain']:.1f}x\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Interactive: Visualize the Paradigm Shift\ndef visualize_paradigm_shift(n_companies_list=[5, 10, 20, 50]):\n    \"\"\"Create interactive visualization of cost savings\"\"\"\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle('The Paradigm Shift: From Wasteful Duplication to Efficient Sharing', \n                 fontsize=16, fontweight='bold')\n    \n    traditional_costs = []\n    pretrain_costs = []\n    \n    for n in n_companies_list:\n        trad, pre, _ = calculator.compare_approaches(n)\n        traditional_costs.append(trad['total_cost'] / 1_000_000)  # Convert to millions\n        pretrain_costs.append(pre['total_cost'] / 1_000_000)\n    \n    # Cost comparison\n    ax1 = axes[0, 0]\n    x = np.arange(len(n_companies_list))\n    width = 0.35\n    ax1.bar(x - width/2, traditional_costs, width, label='Traditional', color='#e74c3c', alpha=0.8)\n    ax1.bar(x + width/2, pretrain_costs, width, label='Pre-training', color='#2ecc71', alpha=0.8)\n    ax1.set_xlabel('Number of Companies')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(n_companies_list)\n    ax1.set_ylabel('Total Cost (Millions USD)')\n    ax1.set_title('Total Cost Comparison')\n    ax1.legend()\n    ax1.grid(axis='y', alpha=0.3)\n    \n    # Savings over companies\n    ax2 = axes[0, 1]\n    savings = [t - p for t, p in zip(traditional_costs, pretrain_costs)]\n    ax2.plot(n_companies_list, savings, 'o-', color='#3498db', linewidth=2, markersize=8)\n    ax2.fill_between(n_companies_list, 0, savings, alpha=0.3, color='#3498db')\n    ax2.set_xlabel('Number of Companies')\n    ax2.set_ylabel('Money Saved (Millions USD)')\n    ax2.set_title('Cumulative Savings with Pre-training')\n    ax2.grid(True, alpha=0.3)\n    \n    # Carbon footprint\n    ax3 = axes[1, 0]\n    carbon_trad = []\n    carbon_pre = []\n    for n in n_companies_list:\n        trad, pre, _ = calculator.compare_approaches(n)\n        carbon_trad.append(trad['carbon_kg'] / 1000)  # Convert to tons\n        carbon_pre.append(pre['carbon_kg'] / 1000)\n    \n    ax3.bar(x - width/2, carbon_trad, width, label='Traditional', color='#95a5a6', alpha=0.8)\n    ax3.bar(x + width/2, carbon_pre, width, label='Pre-training', color='#27ae60', alpha=0.8)\n    ax3.set_xlabel('Number of Companies')\n    ax3.set_xticks(x)\n    ax3.set_xticklabels(n_companies_list)\n    ax3.set_ylabel('Carbon Emissions (Tons CO2)')\n    ax3.set_title('Environmental Impact')\n    ax3.legend()\n    ax3.grid(axis='y', alpha=0.3)\n    \n    # Efficiency gain\n    ax4 = axes[1, 1]\n    efficiency = [t / p for t, p in zip(traditional_costs, pretrain_costs)]\n    colors_eff = ['#f39c12', '#e67e22', '#d35400', '#c0392b']\n    bars = ax4.bar(n_companies_list, efficiency, color=colors_eff, alpha=0.8, edgecolor='black', linewidth=2)\n    ax4.set_xlabel('Number of Companies')\n    ax4.set_ylabel('Efficiency Gain (x times)')\n    ax4.set_title('Efficiency Multiplier')\n    ax4.grid(axis='y', alpha=0.3)\n    \n    # Add value labels\n    for bar, eff in zip(bars, efficiency):\n        height = bar.get_height()\n        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                f'{eff:.1f}x', ha='center', va='bottom', fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Run the visualization\nvisualize_paradigm_shift()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## NEW: The Paradigm Shift - Interactive Demonstration\n\nLet's start by understanding the revolutionary impact of pre-training through an interactive visualization.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding BERT's Masked Language Modeling\n",
    "\n",
    "Let's start by exploring how BERT was pre-trained using masked language modeling (MLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name)\n",
    "mlm_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in mlm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_word(text, masked_index=None):\n",
    "    \"\"\"\n",
    "    Predict masked word in a sentence using BERT\n",
    "    \"\"\"\n",
    "    # Replace word with [MASK] if index provided\n",
    "    words = text.split()\n",
    "    if masked_index is not None:\n",
    "        words[masked_index] = '[MASK]'\n",
    "    masked_text = ' '.join(words)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(masked_text, return_tensors='pt')\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = mlm_model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    # Find masked token position\n",
    "    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Get top 5 predictions\n",
    "    masked_token_logits = predictions[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(masked_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"Masked text: {masked_text}\")\n",
    "    print(\"\\nTop 5 predictions:\")\n",
    "    for i, token in enumerate(top_5_tokens, 1):\n",
    "        word = tokenizer.decode([token])\n",
    "        prob = torch.softmax(masked_token_logits, dim=1)[0, token].item()\n",
    "        print(f\"{i}. {word:15} (probability: {prob:.2%})\")\n",
    "    \n",
    "    return top_5_tokens\n",
    "\n",
    "# Test the function\n",
    "predict_masked_word(\"The cat sat on the mat\", masked_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive MLM examples\n",
    "examples = [\n",
    "    \"The weather today is [MASK]\",\n",
    "    \"I love to eat [MASK] for breakfast\",\n",
    "    \"The [MASK] is the capital of France\",\n",
    "    \"She is a [MASK] student at the university\",\n",
    "    \"The movie was [MASK] than I expected\"\n",
    "]\n",
    "\n",
    "print(\"BERT's Masked Language Model Predictions:\\n\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for example in examples:\n",
    "    inputs = tokenizer(example, return_tensors='pt')\n",
    "    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = mlm_model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    masked_token_logits = predictions[0, mask_token_index, :]\n",
    "    top_token = torch.topk(masked_token_logits, 1, dim=1).indices[0].item()\n",
    "    predicted_word = tokenizer.decode([top_token])\n",
    "    \n",
    "    print(f\"Input:  {example}\")\n",
    "    print(f\"Prediction: {example.replace('[MASK]', f'**{predicted_word}**')}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fine-tuning BERT for Sentiment Analysis\n",
    "\n",
    "Now let's fine-tune BERT for a specific task: sentiment analysis on movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset (subset for faster training)\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Use smaller subset for demo\n",
    "train_size = 1000\n",
    "test_size = 200\n",
    "\n",
    "train_dataset = dataset['train'].select(range(train_size))\n",
    "test_dataset = dataset['test'].select(range(test_size))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"\\nExample review:\")\n",
    "print(f\"Text: {train_dataset[0]['text'][:200]}...\")\n",
    "print(f\"Label: {'Positive' if train_dataset[0]['label'] == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT for sequence classification\n",
    "model_ft = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    output_attentions=True,  # We'll use this for visualization\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded for fine-tuning\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model_ft.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Ready to fine-tune!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This will take a few minutes...\\n\")\n",
    "\n",
    "# Store initial time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nFine-tuning complete!\")\n",
    "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## NEW: Environmental Impact Calculator\n\nLet's calculate the environmental impact of training different model sizes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {eval_result['eval_accuracy']:.2%}\")\n",
    "print(f\"Test Loss: {eval_result['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparing with Training from Scratch\n",
    "\n",
    "Let's create a BERT model from scratch (randomly initialized) and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BERT from scratch (random initialization)\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model_scratch = BertForSequenceClassification(config)\n",
    "print(f\"Created BERT from scratch with random weights\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model_scratch.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch (same settings)\n",
    "trainer_scratch = Trainer(\n",
    "    model=model_scratch,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training from scratch...\")\n",
    "print(\"This will take a few minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "train_result_scratch = trainer_scratch.train()\n",
    "scratch_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining from scratch complete!\")\n",
    "print(f\"Training time: {scratch_time/60:.2f} minutes\")\n",
    "print(f\"Final training loss: {train_result_scratch.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "eval_scratch = trainer_scratch.evaluate()\n",
    "\n",
    "# Create comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Test Accuracy', 'Test Loss', 'Training Time (min)'],\n",
    "    'Pre-trained + Fine-tuned': [\n",
    "        f\"{eval_result['eval_accuracy']:.2%}\",\n",
    "        f\"{eval_result['eval_loss']:.4f}\",\n",
    "        f\"{training_time/60:.2f}\"\n",
    "    ],\n",
    "    'Trained from Scratch': [\n",
    "        f\"{eval_scratch['eval_accuracy']:.2%}\",\n",
    "        f\"{eval_scratch['eval_loss']:.4f}\",\n",
    "        f\"{scratch_time/60:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON: Pre-trained vs From Scratch\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nImprovement from pre-training: {(eval_result['eval_accuracy'] - eval_scratch['eval_accuracy'])*100:.1f} percentage points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using the Fine-tuned Model\n",
    "\n",
    "Let's test our fine-tuned model on some new movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_ft,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! Best film I've seen all year.\",\n",
    "    \"Terrible waste of time. The plot made no sense and the acting was awful.\",\n",
    "    \"Not bad, but not great either. It was just okay.\",\n",
    "    \"A masterpiece of cinema! Every scene was perfectly crafted.\",\n",
    "    \"I fell asleep halfway through. Boring and predictable.\",\n",
    "    \"Surprisingly good! I wasn't expecting much but was pleasantly surprised.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = sentiment_pipeline(review)[0]\n",
    "    label = \"Positive ðŸ˜Š\" if result['label'] == 'LABEL_1' else \"Negative ðŸ˜ž\"\n",
    "    confidence = result['score']\n",
    "    \n",
    "    print(f\"Review: {review[:50]}...\")\n",
    "    print(f\"Sentiment: {label} (Confidence: {confidence:.2%})\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## NEW: LoRA - Low-Rank Adaptation for Efficient Fine-tuning\n\nLet's implement LoRA to see how we can fine-tune large models efficiently.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing Attention Patterns\n",
    "\n",
    "Let's visualize what BERT is \"paying attention to\" when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract attention weights from BERT\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    \n",
    "    # Get model outputs with attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract attention weights (last layer, first head)\n",
    "    attention = outputs.attentions[-1][0, 0, :, :].numpy()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    return attention, tokens\n",
    "\n",
    "# Example sentence\n",
    "example_text = \"This movie was absolutely terrible!\"\n",
    "attention, tokens = get_attention_weights(example_text, model_ft, tokenizer)\n",
    "\n",
    "# Visualize attention matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention[:len(tokens), :len(tokens)], \n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title(f'BERT Attention Patterns\\nText: \"{example_text}\"', fontsize=14)\n",
    "plt.xlabel('Keys (Tokens)', fontsize=12)\n",
    "plt.ylabel('Queries (Tokens)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze which words get most attention\n",
    "avg_attention = attention.mean(axis=0)[:len(tokens)]\n",
    "top_attention_idx = avg_attention.argsort()[-5:][::-1]\n",
    "\n",
    "print(\"\\nWords receiving most attention (averaged):\")\n",
    "for idx in top_attention_idx:\n",
    "    if tokens[idx] not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "        print(f\"- {tokens[idx]}: {avg_attention[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## NEW: BERT vs GPT - Interactive Comparison\n\nLet's interactively explore the differences between bidirectional (BERT) and autoregressive (GPT) models.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Key Insights and Takeaways\n",
    "\n",
    "Let's summarize what we've learned about pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Enhanced Conclusion - The Paradigm Shift Complete Picture\n\nThrough this enhanced notebook, we've explored:\n\n### ðŸ”„ The Paradigm Shift\n- **$10 Million Problem**: How pre-training saves millions in compute costs\n- **9.1x Efficiency Gain**: Dramatic reduction in resources needed\n- **Environmental Impact**: Orders of magnitude reduction in carbon footprint\n\n### ðŸ› ï¸ Practical Techniques\n- **Fine-tuning**: Simple yet powerful transfer learning\n- **LoRA**: 100x parameter reduction while maintaining performance\n- **Attention Analysis**: Understanding what models \"see\"\n\n### ðŸ”¬ Key Comparisons\n- **Pre-trained vs Scratch**: 15-20% accuracy improvement\n- **BERT vs GPT**: Different tools for different tasks\n- **Environmental Costs**: From tons to kilograms of CO2\n\n### ðŸ’¡ Main Takeaways\n1. **Never train from scratch** - Stand on the shoulders of giants\n2. **Choose the right model** - BERT for understanding, GPT for generation\n3. **Use efficient methods** - LoRA, quantization, pruning\n4. **Consider the environment** - Every parameter counts\n\n### ðŸš€ Your Next Steps\n1. Try fine-tuning on your own dataset\n2. Experiment with different model sizes\n3. Implement LoRA for your specific use case\n4. Explore prompt engineering for even better results\n\n**Remember**: The paradigm shift isn't just about better models - it's about democratizing AI by making powerful models accessible to everyone!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization comparing training approaches\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "categories = ['Pre-trained\\n+ Fine-tuned', 'Trained from\\nScratch']\n",
    "accuracies = [eval_result['eval_accuracy'], eval_scratch['eval_accuracy']]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "bars1 = ax1.bar(categories, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{acc:.1%}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Training efficiency\n",
    "ax2 = axes[1]\n",
    "times = [training_time/60, scratch_time/60]\n",
    "bars2 = ax2.bar(categories, times, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Training Time (minutes)', fontsize=12)\n",
    "ax2.set_title('Training Efficiency', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars2, times):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{time:.1f} min', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('The Power of Pre-training: Better Performance, Faster Training', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: The Benefits of Pre-training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Accuracy Improvement: {(accuracies[0] - accuracies[1])*100:.1f} percentage points\")\n",
    "print(f\"âœ… Training Speedup: {times[1]/times[0]:.1f}x faster to reach better performance\")\n",
    "print(f\"âœ… Data Efficiency: Same amount of data, much better results\")\n",
    "print(f\"âœ… Robustness: Pre-trained models generalize better\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises and Challenges\n",
    "\n",
    "Now it's your turn to experiment! Try these exercises:\n",
    "\n",
    "### Exercise 1: Different Masking Strategies\n",
    "Modify the MLM example to mask multiple words and see how BERT handles it.\n",
    "\n",
    "### Exercise 2: Fine-tune for Different Tasks\n",
    "Try fine-tuning BERT for:\n",
    "- Text classification on a different dataset (e.g., news categories)\n",
    "- Named Entity Recognition\n",
    "- Question Answering\n",
    "\n",
    "### Exercise 3: Compare Model Sizes\n",
    "Try using different BERT variants:\n",
    "- `bert-tiny` (very small, fast)\n",
    "- `bert-base` (what we used)\n",
    "- `bert-large` (bigger, more accurate)\n",
    "\n",
    "### Exercise 4: Attention Analysis\n",
    "Analyze attention patterns for different types of sentences:\n",
    "- Questions\n",
    "- Negations\n",
    "- Complex sentences with multiple clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n",
    "# Try Exercise 1: Multiple masked words\n",
    "text_with_masks = \"The [MASK] cat [MASK] on the [MASK].\"\n",
    "print(\"Your turn to experiment!\")\n",
    "print(f\"Try predicting: {text_with_masks}\")\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've:\n",
    "1. âœ… Explored BERT's masked language modeling\n",
    "2. âœ… Fine-tuned BERT for sentiment analysis\n",
    "3. âœ… Compared with training from scratch\n",
    "4. âœ… Visualized attention patterns\n",
    "5. âœ… Demonstrated the power of pre-training\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Pre-training provides a massive head start** - better performance with less data and time\n",
    "- **Fine-tuning is simple** - just a few lines of code with Hugging Face Transformers\n",
    "- **Attention patterns are interpretable** - we can see what the model focuses on\n",
    "- **Transfer learning works** - knowledge from pre-training transfers to new tasks\n",
    "\n",
    "### Next Steps:\n",
    "- Try fine-tuning for your own task\n",
    "- Explore other pre-trained models (GPT, T5, RoBERTa)\n",
    "- Learn about prompt engineering for even better results\n",
    "- Experiment with larger models if you have GPU access\n",
    "\n",
    "**Remember**: You don't need to train from scratch - stand on the shoulders of giants!"
   ]
  },
  {
   "cell_type": "code",
   "source": "class EnvironmentalImpactCalculator:\n    \"\"\"Calculate carbon footprint of different model training approaches\"\"\"\n    \n    def __init__(self):\n        # Model sizes and training requirements\n        self.models = {\n            'BERT-Tiny': {'params': 4.4e6, 'gpu_hours': 24, 'flops': 1e18},\n            'BERT-Mini': {'params': 11.3e6, 'gpu_hours': 72, 'flops': 1e19},\n            'BERT-Small': {'params': 29.1e6, 'gpu_hours': 168, 'flops': 5e19},\n            'BERT-Base': {'params': 110e6, 'gpu_hours': 336, 'flops': 2e20},\n            'BERT-Large': {'params': 340e6, 'gpu_hours': 1008, 'flops': 7e20},\n            'GPT-2': {'params': 1.5e9, 'gpu_hours': 2016, 'flops': 3e21},\n            'GPT-3': {'params': 175e9, 'gpu_hours': 355000, 'flops': 3.14e23},\n        }\n        \n        # Environmental constants\n        self.kwh_per_gpu_hour = 0.4  # kilowatt hours\n        self.co2_per_kwh = {\n            'US Average': 0.42,\n            'Coal': 0.82,\n            'Natural Gas': 0.49,\n            'Nuclear': 0.012,\n            'Solar': 0.048,\n            'Wind': 0.011,\n            'Hydro': 0.024\n        }\n        \n        # Real-world comparisons\n        self.car_km_per_year = 15000\n        self.co2_per_car_km = 0.12  # kg CO2 per km\n        self.tree_co2_per_year = 21  # kg CO2 absorbed per tree per year\n        self.flight_co2_per_km = 0.115  # kg CO2 per passenger km\n    \n    def calculate_carbon_footprint(self, model_name, energy_source='US Average', \n                                  num_experiments=1, include_hyperparameter_search=False):\n        \"\"\"Calculate carbon footprint for training a model\"\"\"\n        \n        if model_name not in self.models:\n            return None\n        \n        model = self.models[model_name]\n        gpu_hours = model['gpu_hours']\n        \n        # Add hyperparameter search overhead (typically 3-10x)\n        if include_hyperparameter_search:\n            gpu_hours *= 5\n        \n        # Multiple experiments\n        gpu_hours *= num_experiments\n        \n        # Calculate energy and CO2\n        energy_kwh = gpu_hours * self.kwh_per_gpu_hour\n        co2_kg = energy_kwh * self.co2_per_kwh[energy_source]\n        \n        # Calculate comparisons\n        car_years = co2_kg / (self.car_km_per_year * self.co2_per_car_km / 1000)\n        trees_needed = co2_kg / self.tree_co2_per_year\n        flight_km = co2_kg / self.flight_co2_per_km\n        \n        return {\n            'model': model_name,\n            'gpu_hours': gpu_hours,\n            'energy_kwh': energy_kwh,\n            'co2_kg': co2_kg,\n            'co2_tons': co2_kg / 1000,\n            'car_years': car_years,\n            'trees_needed': trees_needed,\n            'flight_km': flight_km,\n            'energy_source': energy_source\n        }\n    \n    def compare_models(self, model_names, energy_source='US Average'):\n        \"\"\"Compare carbon footprint of multiple models\"\"\"\n        results = []\n        for model in model_names:\n            result = self.calculate_carbon_footprint(model, energy_source)\n            if result:\n                results.append(result)\n        return results\n    \n    def visualize_comparison(self, model_names=None):\n        \"\"\"Create visualization comparing model carbon footprints\"\"\"\n        if model_names is None:\n            model_names = ['BERT-Tiny', 'BERT-Base', 'BERT-Large', 'GPT-2']\n        \n        results = self.compare_models(model_names)\n        \n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        fig.suptitle('Environmental Impact of Model Training', fontsize=16, fontweight='bold')\n        \n        # CO2 emissions\n        ax1 = axes[0, 0]\n        models = [r['model'] for r in results]\n        co2_tons = [r['co2_tons'] for r in results]\n        colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(models)))\n        bars1 = ax1.bar(models, co2_tons, color=colors, edgecolor='black', linewidth=2)\n        ax1.set_ylabel('CO2 Emissions (Tons)')\n        ax1.set_title('Carbon Footprint by Model Size')\n        ax1.set_yscale('log')\n        ax1.grid(axis='y', alpha=0.3)\n        \n        # Add value labels\n        for bar, val in zip(bars1, co2_tons):\n            height = bar.get_height()\n            ax1.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n                    f'{val:.2f}t', ha='center', va='bottom', fontweight='bold')\n        \n        # Energy consumption\n        ax2 = axes[0, 1]\n        energy = [r['energy_kwh'] for r in results]\n        bars2 = ax2.bar(models, energy, color=colors, edgecolor='black', linewidth=2)\n        ax2.set_ylabel('Energy (kWh)')\n        ax2.set_title('Energy Consumption')\n        ax2.set_yscale('log')\n        ax2.grid(axis='y', alpha=0.3)\n        \n        # Car year equivalents\n        ax3 = axes[1, 0]\n        car_years = [r['car_years'] for r in results]\n        bars3 = ax3.bar(models, car_years, color=colors, edgecolor='black', linewidth=2)\n        ax3.set_ylabel('Car Years Equivalent')\n        ax3.set_title('Equivalent to Driving a Car For...')\n        ax3.grid(axis='y', alpha=0.3)\n        \n        # Trees needed to offset\n        ax4 = axes[1, 1]\n        trees = [r['trees_needed'] for r in results]\n        bars4 = ax4.bar(models, trees, color=colors, edgecolor='black', linewidth=2)\n        ax4.set_ylabel('Number of Trees')\n        ax4.set_title('Trees Needed to Offset (1 Year)')\n        ax4.set_yscale('log')\n        ax4.grid(axis='y', alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        return results\n\n# Create calculator and run comparison\nenv_calculator = EnvironmentalImpactCalculator()\n\nprint(\"=\"*70)\nprint(\"ENVIRONMENTAL IMPACT ANALYSIS\")\nprint(\"=\"*70)\n\n# Calculate for BERT-Base with different energy sources\nprint(\"\\nBERT-Base Training - Impact by Energy Source:\")\nprint(\"-\"*50)\n\nfor source in ['Coal', 'Natural Gas', 'US Average', 'Nuclear', 'Solar']:\n    result = env_calculator.calculate_carbon_footprint('BERT-Base', source)\n    print(f\"{source:15} -> {result['co2_kg']:.1f} kg CO2 ({result['co2_tons']:.2f} tons)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON: Different Model Sizes\")\nprint(\"=\"*70)\n\n# Visualize comparison\nenv_calculator.visualize_comparison()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class LoRALayer(nn.Module):\n    \"\"\"\n    Implements Low-Rank Adaptation (LoRA) for efficient fine-tuning\n    Instead of updating all weights W, we learn low-rank matrices A and B\n    such that W' = W + BA where B is d x r and A is r x k (r << min(d,k))\n    \"\"\"\n    \n    def __init__(self, original_layer, rank=4, alpha=1):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.alpha = alpha\n        \n        # Get dimensions\n        if hasattr(original_layer, 'in_features'):\n            # Linear layer\n            d_in = original_layer.in_features\n            d_out = original_layer.out_features\n            self.is_linear = True\n        else:\n            raise ValueError(\"LoRA only supports Linear layers in this implementation\")\n        \n        # Create low-rank matrices\n        self.lora_A = nn.Parameter(torch.randn(rank, d_in) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(d_out, rank))\n        \n        # Freeze original weights\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n        \n        # Scaling factor\n        self.scaling = self.alpha / self.rank\n    \n    def forward(self, x):\n        # Original forward pass\n        result = self.original_layer(x)\n        \n        # Add LoRA adaptation\n        if self.is_linear:\n            # x @ A.T @ B.T * scaling\n            lora_out = x @ self.lora_A.T @ self.lora_B.T * self.scaling\n            result = result + lora_out\n        \n        return result\n    \n    def merge_weights(self):\n        \"\"\"Merge LoRA weights into original layer for inference\"\"\"\n        if self.is_linear:\n            delta_w = (self.lora_B @ self.lora_A) * self.scaling\n            self.original_layer.weight.data += delta_w\n\n# Demonstrate LoRA parameter efficiency\ndef calculate_lora_parameters(model_params, lora_rank, num_adapted_layers):\n    \"\"\"Calculate parameter reduction with LoRA\"\"\"\n    \n    # Assume adapting attention layers (typical scenario)\n    # BERT-Base has 768 hidden dim\n    hidden_dim = 768\n    \n    # Original parameters to update (full fine-tuning)\n    original_trainable = model_params\n    \n    # LoRA parameters per adapted layer\n    # For Q, K, V projections: 3 * (rank * hidden_dim * 2)\n    lora_params_per_layer = 3 * (lora_rank * hidden_dim * 2)\n    total_lora_params = lora_params_per_layer * num_adapted_layers\n    \n    # Calculate reduction\n    reduction = original_trainable / total_lora_params\n    percentage = (total_lora_params / original_trainable) * 100\n    \n    return {\n        'original_params': original_trainable,\n        'lora_params': total_lora_params,\n        'reduction_factor': reduction,\n        'percentage_of_original': percentage\n    }\n\n# Visualize LoRA efficiency\ndef visualize_lora_efficiency():\n    \"\"\"Create visualization of LoRA parameter efficiency\"\"\"\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle('LoRA: Efficient Fine-tuning Through Low-Rank Adaptation', \n                 fontsize=14, fontweight='bold')\n    \n    # Different model sizes\n    models = {\n        'BERT-Base': 110e6,\n        'BERT-Large': 340e6,\n        'GPT-2': 1.5e9,\n        'GPT-3-13B': 13e9\n    }\n    \n    # LoRA configurations\n    ranks = [1, 4, 8, 16, 32]\n    \n    # Parameter reduction for different ranks\n    ax1 = axes[0]\n    for model_name, params in models.items():\n        reductions = []\n        for rank in ranks:\n            result = calculate_lora_parameters(params, rank, 12)  # 12 transformer layers\n            reductions.append(result['percentage_of_original'])\n        ax1.plot(ranks, reductions, 'o-', label=model_name, linewidth=2, markersize=6)\n    \n    ax1.set_xlabel('LoRA Rank (r)')\n    ax1.set_ylabel('Trainable Parameters (%)')\n    ax1.set_title('Parameter Efficiency vs LoRA Rank')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    ax1.set_yscale('log')\n    \n    # Memory savings\n    ax2 = axes[1]\n    model_names = list(models.keys())\n    memory_full = [m/1e9 * 4 for m in models.values()]  # 4 bytes per param\n    memory_lora = []\n    \n    for params in models.values():\n        result = calculate_lora_parameters(params, 8, 12)  # rank=8\n        memory_lora.append(result['lora_params']/1e9 * 4)\n    \n    x = np.arange(len(model_names))\n    width = 0.35\n    ax2.bar(x - width/2, memory_full, width, label='Full Fine-tuning', color='#e74c3c', alpha=0.8)\n    ax2.bar(x + width/2, memory_lora, width, label='LoRA (r=8)', color='#2ecc71', alpha=0.8)\n    ax2.set_xlabel('Model')\n    ax2.set_xticks(x)\n    ax2.set_xticklabels(model_names, rotation=45, ha='right')\n    ax2.set_ylabel('Memory (GB)')\n    ax2.set_title('Memory Requirements')\n    ax2.legend()\n    ax2.grid(axis='y', alpha=0.3)\n    \n    # Training speedup\n    ax3 = axes[2]\n    speedup_factors = []\n    for params in models.values():\n        result = calculate_lora_parameters(params, 8, 12)\n        # Approximate speedup based on parameter reduction\n        speedup = min(result['reduction_factor'] * 0.7, 10)  # Cap at 10x\n        speedup_factors.append(speedup)\n    \n    bars = ax3.bar(model_names, speedup_factors, color='#3498db', alpha=0.8, edgecolor='black', linewidth=2)\n    ax3.set_xlabel('Model')\n    ax3.set_ylabel('Training Speedup (x)')\n    ax3.set_title('Training Speed Improvement')\n    ax3.grid(axis='y', alpha=0.3)\n    \n    # Add value labels\n    for bar, val in zip(bars, speedup_factors):\n        height = bar.get_height()\n        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                f'{val:.1f}x', ha='center', va='bottom', fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example: Apply LoRA to a layer\nprint(\"LoRA DEMONSTRATION\")\nprint(\"=\"*60)\n\n# Create a dummy linear layer (like in BERT)\noriginal_linear = nn.Linear(768, 768)\nprint(f\"Original layer parameters: {sum(p.numel() for p in original_linear.parameters()):,}\")\n\n# Wrap with LoRA\nlora_layer = LoRALayer(original_linear, rank=8, alpha=16)\ntrainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\nprint(f\"LoRA trainable parameters: {trainable_params:,}\")\nprint(f\"Parameter reduction: {sum(p.numel() for p in original_linear.parameters()) / trainable_params:.1f}x\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EFFICIENCY COMPARISON\")\nprint(\"=\"*60)\n\n# Calculate for BERT-Base\nbert_result = calculate_lora_parameters(110e6, 8, 12)\nprint(f\"BERT-Base Full Fine-tuning: {bert_result['original_params']/1e6:.1f}M parameters\")\nprint(f\"BERT-Base with LoRA (r=8):  {bert_result['lora_params']/1e6:.2f}M parameters\")\nprint(f\"Reduction factor: {bert_result['reduction_factor']:.1f}x\")\nprint(f\"Only training {bert_result['percentage_of_original']:.2f}% of original parameters!\")\n\n# Visualize\nvisualize_lora_efficiency()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class BERTvsGPTComparison:\n    \"\"\"Interactive comparison of BERT and GPT architectures\"\"\"\n    \n    def __init__(self):\n        self.bert_features = {\n            'architecture': 'Bidirectional',\n            'pretraining': 'Masked Language Modeling (MLM)',\n            'objective': 'Predict masked tokens',\n            'context': 'Sees both left and right context',\n            'best_for': ['Classification', 'Token labeling', 'Question answering'],\n            'limitations': ['Cannot generate text naturally', 'Requires task-specific heads'],\n            'example_models': ['BERT', 'RoBERTa', 'ELECTRA', 'DeBERTa']\n        }\n        \n        self.gpt_features = {\n            'architecture': 'Autoregressive',\n            'pretraining': 'Next Token Prediction',\n            'objective': 'Predict next token',\n            'context': 'Sees only left context',\n            'best_for': ['Text generation', 'Completion', 'Few-shot learning'],\n            'limitations': ['No bidirectional context', 'May need more data for understanding'],\n            'example_models': ['GPT-2', 'GPT-3', 'GPT-4', 'GPT-J']\n        }\n    \n    def demonstrate_masking_difference(self):\n        \"\"\"Show the difference in how BERT and GPT process text\"\"\"\n        \n        text = \"The cat sat on the mat\"\n        \n        print(\"=\"*70)\n        print(\"MASKING STRATEGY COMPARISON\")\n        print(\"=\"*70)\n        print(f\"Original text: '{text}'\")\n        print(\"-\"*70)\n        \n        # BERT-style masking\n        print(\"\\nBERT (Masked Language Modeling):\")\n        print(\"  Training example: 'The [MASK] sat on the mat'\")\n        print(\"  Task: Predict what [MASK] is\")\n        print(\"  Context used: 'The' (left) + 'sat on the mat' (right)\")\n        print(\"  -> Can see BOTH directions!\")\n        \n        # GPT-style prediction\n        print(\"\\nGPT (Autoregressive):\")\n        print(\"  Training examples (processed sequentially):\")\n        print(\"    Step 1: 'The' -> predict 'cat'\")\n        print(\"    Step 2: 'The cat' -> predict 'sat'\")\n        print(\"    Step 3: 'The cat sat' -> predict 'on'\")\n        print(\"  -> Can only see LEFT context!\")\n        \n        print(\"=\"*70)\n    \n    def visualize_attention_patterns(self):\n        \"\"\"Visualize how BERT and GPT attend to tokens differently\"\"\"\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n        fig.suptitle('Attention Pattern Comparison: BERT vs GPT', \n                     fontsize=14, fontweight='bold')\n        \n        tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n        n_tokens = len(tokens)\n        \n        # BERT attention (bidirectional)\n        ax1 = axes[0]\n        bert_attention = np.ones((n_tokens, n_tokens))  # Can attend to all\n        im1 = ax1.imshow(bert_attention, cmap='Blues', vmin=0, vmax=1)\n        ax1.set_xticks(range(n_tokens))\n        ax1.set_yticks(range(n_tokens))\n        ax1.set_xticklabels(tokens, rotation=45)\n        ax1.set_yticklabels(tokens)\n        ax1.set_title('BERT: Bidirectional Attention')\n        ax1.set_xlabel('Keys')\n        ax1.set_ylabel('Queries')\n        \n        # Add grid\n        for i in range(n_tokens + 1):\n            ax1.axhline(i - 0.5, color='gray', linewidth=0.5)\n            ax1.axvline(i - 0.5, color='gray', linewidth=0.5)\n        \n        # GPT attention (causal/autoregressive)\n        ax2 = axes[1]\n        gpt_attention = np.tril(np.ones((n_tokens, n_tokens)))  # Lower triangular\n        im2 = ax2.imshow(gpt_attention, cmap='Oranges', vmin=0, vmax=1)\n        ax2.set_xticks(range(n_tokens))\n        ax2.set_yticks(range(n_tokens))\n        ax2.set_xticklabels(tokens, rotation=45)\n        ax2.set_yticklabels(tokens)\n        ax2.set_title('GPT: Causal (Autoregressive) Attention')\n        ax2.set_xlabel('Keys')\n        ax2.set_ylabel('Queries')\n        \n        # Add grid\n        for i in range(n_tokens + 1):\n            ax2.axhline(i - 0.5, color='gray', linewidth=0.5)\n            ax2.axvline(i - 0.5, color='gray', linewidth=0.5)\n        \n        # Add text annotations\n        ax1.text(0.5, -0.15, 'Each token can attend to ALL other tokens',\n                transform=ax1.transAxes, ha='center', fontsize=10, style='italic')\n        ax2.text(0.5, -0.15, 'Each token can only attend to PREVIOUS tokens',\n                transform=ax2.transAxes, ha='center', fontsize=10, style='italic')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def compare_use_cases(self):\n        \"\"\"Create comparison chart of use cases\"\"\"\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n        fig.suptitle('BERT vs GPT: Best Use Cases', fontsize=14, fontweight='bold')\n        \n        # Task performance comparison\n        tasks = ['Text\\nClassification', 'Named Entity\\nRecognition', \n                'Question\\nAnswering', 'Text\\nGeneration', \n                'Story\\nCompletion', 'Few-shot\\nLearning']\n        \n        bert_scores = [95, 92, 90, 30, 40, 50]  # Relative performance\n        gpt_scores = [80, 70, 75, 95, 92, 90]\n        \n        ax1 = axes[0]\n        x = np.arange(len(tasks))\n        width = 0.35\n        \n        bars1 = ax1.bar(x - width/2, bert_scores, width, label='BERT', \n                       color='#3498db', alpha=0.8, edgecolor='black', linewidth=1)\n        bars2 = ax1.bar(x + width/2, gpt_scores, width, label='GPT', \n                       color='#e67e22', alpha=0.8, edgecolor='black', linewidth=1)\n        \n        ax1.set_xlabel('Task Type')\n        ax1.set_ylabel('Relative Performance')\n        ax1.set_title('Task Performance Comparison')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(tasks)\n        ax1.legend()\n        ax1.grid(axis='y', alpha=0.3)\n        ax1.set_ylim(0, 100)\n        \n        # Characteristics radar chart\n        ax2 = axes[1]\n        categories = ['Understanding', 'Generation', 'Bidirectional\\nContext', \n                     'Few-shot', 'Speed', 'Memory\\nEfficiency']\n        \n        bert_values = [90, 30, 100, 40, 70, 80]\n        gpt_values = [75, 100, 0, 90, 60, 60]\n        \n        # Number of variables\n        num_vars = len(categories)\n        \n        # Compute angle for each axis\n        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n        \n        # Complete the circle\n        bert_values += bert_values[:1]\n        gpt_values += gpt_values[:1]\n        angles += angles[:1]\n        \n        # Plot\n        ax2 = plt.subplot(122, projection='polar')\n        ax2.plot(angles, bert_values, 'o-', linewidth=2, label='BERT', color='#3498db')\n        ax2.fill(angles, bert_values, alpha=0.25, color='#3498db')\n        ax2.plot(angles, gpt_values, 'o-', linewidth=2, label='GPT', color='#e67e22')\n        ax2.fill(angles, gpt_values, alpha=0.25, color='#e67e22')\n        \n        ax2.set_xticks(angles[:-1])\n        ax2.set_xticklabels(categories)\n        ax2.set_ylim(0, 100)\n        ax2.set_title('Model Characteristics', y=1.1)\n        ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n        ax2.grid(True)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def interactive_comparison_table(self):\n        \"\"\"Create detailed comparison table\"\"\"\n        \n        comparison_data = {\n            'Aspect': [\n                'Architecture',\n                'Pre-training Task',\n                'Context Direction',\n                'Best for Understanding',\n                'Best for Generation',\n                'Parameter Efficiency',\n                'Training Data Needs',\n                'Inference Speed',\n                'Few-shot Capability',\n                'Fine-tuning Ease'\n            ],\n            'BERT': [\n                'Encoder-only',\n                'Masked LM + NSP',\n                'Bidirectional',\n                'Excellent',\n                'Poor',\n                'High',\n                'Moderate',\n                'Fast',\n                'Limited',\n                'Easy'\n            ],\n            'GPT': [\n                'Decoder-only',\n                'Next Token Prediction',\n                'Left-to-right',\n                'Good',\n                'Excellent',\n                'Moderate',\n                'High',\n                'Moderate',\n                'Excellent',\n                'Moderate'\n            ],\n            'Winner': [\n                'Depends on task',\n                'GPT (simpler)',\n                'BERT',\n                'BERT',\n                'GPT',\n                'BERT',\n                'BERT',\n                'BERT',\n                'GPT',\n                'BERT'\n            ]\n        }\n        \n        df = pd.DataFrame(comparison_data)\n        \n        # Create styled display\n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPREHENSIVE COMPARISON: BERT vs GPT\")\n        print(\"=\"*80)\n        \n        for _, row in df.iterrows():\n            winner_indicator = \"\"\n            if row['Winner'] == 'BERT':\n                winner_indicator = \" <- BERT wins\"\n            elif row['Winner'] == 'GPT':\n                winner_indicator = \" <- GPT wins\"\n            \n            print(f\"\\n{row['Aspect']}:\")\n            print(f\"  BERT: {row['BERT']}\")\n            print(f\"  GPT:  {row['GPT']}\")\n            if winner_indicator:\n                print(f\"  {winner_indicator}\")\n        \n        print(\"\\n\" + \"=\"*80)\n        \n        return df\n\n# Create comparison object and run demonstrations\ncomparison = BERTvsGPTComparison()\n\n# Show masking difference\ncomparison.demonstrate_masking_difference()\n\n# Visualize attention patterns\nprint(\"\\nVisualizing attention patterns...\")\ncomparison.visualize_attention_patterns()\n\n# Compare use cases\nprint(\"\\nComparing use cases...\")\ncomparison.compare_use_cases()\n\n# Show detailed comparison table\nprint(\"\\nDetailed comparison table:\")\ncomparison_df = comparison.interactive_comparison_table()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}