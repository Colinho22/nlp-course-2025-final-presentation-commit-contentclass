---
layout: default
title: Bibliography
---

# Bibliography

Complete references for the NLP course materials.

## Word Embeddings

### Foundational Papers

1. **Mikolov, T., Chen, K., Corrado, G., & Dean, J.** (2013).
   *Efficient Estimation of Word Representations in Vector Space*.
   ICLR Workshop.
   [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)

2. **Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J.** (2013).
   *Distributed Representations of Words and Phrases and their Compositionality*.
   NeurIPS.
   [arXiv:1310.4546](https://arxiv.org/abs/1310.4546)

3. **Pennington, J., Socher, R., & Manning, C.** (2014).
   *GloVe: Global Vectors for Word Representation*.
   EMNLP.
   [Paper](https://aclanthology.org/D14-1162/)

4. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T.** (2017).
   *Enriching Word Vectors with Subword Information*.
   TACL.
   [arXiv:1607.04606](https://arxiv.org/abs/1607.04606)

### Contextual Embeddings

5. **Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.** (2018).
   *Deep Contextualized Word Representations*.
   NAACL.
   [arXiv:1802.05365](https://arxiv.org/abs/1802.05365)

6. **Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K.** (2019).
   *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*.
   NAACL.
   [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

## Transformers & Attention

7. **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I.** (2017).
   *Attention Is All You Need*.
   NeurIPS.
   [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

8. **Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I.** (2019).
   *Language Models are Unsupervised Multitask Learners*.
   OpenAI Blog.
   [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## Language Models

9. **Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C.** (2003).
   *A Neural Probabilistic Language Model*.
   JMLR.
   [Paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

10. **Hochreiter, S., & Schmidhuber, J.** (1997).
    *Long Short-Term Memory*.
    Neural Computation.
    [Paper](https://www.bioinf.jku.at/publications/older/2604.pdf)

## Evaluation & Analysis

11. **Baroni, M., Dinu, G., & Kruszewski, G.** (2014).
    *Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors*.
    ACL.
    [Paper](https://aclanthology.org/P14-1023/)

12. **Levy, O., & Goldberg, Y.** (2014).
    *Neural Word Embedding as Implicit Matrix Factorization*.
    NeurIPS.
    [Paper](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html)

---

## How to Cite This Course

If you use these materials in your research or teaching, please cite:

```bibtex
@misc{nlp-course-2025,
  author = {Digital AI Finance},
  title = {Natural Language Processing Course: From N-grams to Transformers},
  year = {2025},
  url = {https://digital-ai-finance.github.io/Natural-Language-Processing/},
  note = {Graduate-level NLP curriculum with discovery-based pedagogy}
}
```
