[
  {
    "title": "N-gram Language Models",
    "url": "/topics/ngrams/",
    "content": "Learn how to predict the next word using probability and counting. Foundation for all language models. Probability basics Bigram models Smoothing techniques Perplexity evaluation Understand probability basics for language modeling Build and evaluate bigram models Apply smoothing techniques for unseen n-grams Calculate perplexity for model evaluation",
    "section": "topics"
  },
  {
    "title": "Word Embeddings",
    "url": "/topics/embeddings/",
    "content": "Transform words into dense vectors that capture meaning. Word2Vec, GloVe, and semantic relationships. One-hot encoding Skip-gram architecture Negative sampling Word analogies Understand one-hot encoding limitations Implement Skip-gram architecture Apply negative sampling for efficient training Solve word analogy problems",
    "section": "topics"
  },
  {
    "title": "RNN & LSTM Networks",
    "url": "/topics/rnn-lstm/",
    "content": "Process sequences with memory. Understand vanishing gradients and how LSTMs solve them. Recurrent connections Vanishing gradients LSTM gates Sequence modeling Understand recurrent connections in RNNs Diagnose vanishing gradient problems Master LSTM gate mechanisms Apply sequence modeling techniques",
    "section": "topics"
  },
  {
    "title": "Sequence-to-Sequence",
    "url": "/topics/seq2seq/",
    "content": "Map sequences to sequences. The foundation of machine translation and summarization. Encoder-decoder Attention mechanism Teacher forcing Beam search Build encoder-decoder architectures Implement attention mechanisms Apply teacher forcing during training Use beam search for decoding",
    "section": "topics"
  },
  {
    "title": "Transformer Architecture",
    "url": "/topics/transformers/",
    "content": "The architecture that revolutionized NLP. Self-attention, multi-head attention, and positional encoding. Self-attention Multi-head attention Positional encoding Feed-forward layers Master self-attention mechanisms Implement multi-head attention Apply positional encoding Build feed-forward layers",
    "section": "topics"
  },
  {
    "title": "Pre-trained Models",
    "url": "/topics/pretrained/",
    "content": "Leverage massive pre-training. BERT, GPT, and the transfer learning revolution. BERT architecture GPT models Fine-tuning Transfer learning Understand BERT architecture Compare GPT model variants Apply fine-tuning techniques Implement transfer learning",
    "section": "topics"
  },
  {
    "title": "Scaling & Advanced Topics",
    "url": "/topics/scaling/",
    "content": "Scale to billions of parameters. Scaling laws, emergent abilities, and modern LLMs. Scaling laws Emergent abilities In-context learning Chain-of-thought Understand scaling laws Identify emergent abilities Apply in-context learning Implement chain-of-thought reasoning",
    "section": "topics"
  },
  {
    "title": "Tokenization",
    "url": "/topics/tokenization/",
    "content": "Break text into tokens. Subword algorithms that power modern language models. BPE algorithm WordPiece SentencePiece Vocabulary optimization Implement BPE algorithm Compare WordPiece variants Apply SentencePiece tokenization Optimize vocabulary size",
    "section": "topics"
  },
  {
    "title": "Decoding Strategies",
    "url": "/topics/decoding/",
    "content": "Generate text from models. Greedy, beam search, temperature, top-k, and nucleus sampling. Greedy decoding Beam search Temperature scaling Top-k and nucleus Implement greedy decoding Apply beam search algorithms Tune temperature scaling Compare top-k and nucleus sampling",
    "section": "topics"
  },
  {
    "title": "Fine-tuning Methods",
    "url": "/topics/finetuning/",
    "content": "Adapt pre-trained models efficiently. LoRA, adapters, and parameter-efficient methods. Full fine-tuning LoRA Adapters Prompt tuning Apply full fine-tuning Implement LoRA adaptation Use adapter layers Master prompt tuning",
    "section": "topics"
  },
  {
    "title": "Efficiency & Optimization",
    "url": "/topics/efficiency/",
    "content": "Make models faster and smaller. Quantization, pruning, and knowledge distillation. Quantization Pruning Knowledge distillation Inference optimization Apply quantization techniques Implement pruning strategies Use knowledge distillation Optimize inference speed",
    "section": "topics"
  },
  {
    "title": "Ethics & Bias",
    "url": "/topics/ethics/",
    "content": "Build responsible AI systems. Bias detection, fairness, and ethical considerations. Bias in models Fairness metrics Mitigation strategies Responsible deployment Identify bias in models Apply fairness metrics Implement mitigation strategies Deploy responsibly",
    "section": "topics"
  },
  {
    "title": "Word Embeddings: Complete Guide",
    "url": "/modules/embeddings-extended/",
    "content": "From one-hot encoding to modern sentence embeddings. Learn how machines understand the meaning of words. One-Hot Word2Vec GloVe Contextual Modern Master word embedding fundamentals Implement Word2Vec from scratch Apply GloVe and FastText Use contextual embeddings",
    "section": "modules"
  },
  {
    "title": "LSTM Primer",
    "url": "/modules/lstm-primer/",
    "content": "Zero-prerequisite introduction to LSTM networks with clear visualizations. Why RNNs fail Gate mechanisms Cell state flow Practical applications Understand why RNNs fail Master gate mechanisms Trace cell state flow Apply to practical tasks",
    "section": "modules"
  },
  {
    "title": "Sentiment Analysis",
    "url": "/modules/sentiment/",
    "content": "BERT fine-tuning for sentiment classification with technical deep dives. BERT classifier head Pre-training objectives Fine-tuning process Deployment pipeline Build BERT classifier head Understand pre-training objectives Apply fine-tuning process Deploy production pipeline",
    "section": "modules"
  },
  {
    "title": "LLM Summarization",
    "url": "/modules/summarization/",
    "content": "Text summarization using extractive and abstractive methods with modern LLMs. Extractive methods Abstractive generation RAG enhancement Evaluation metrics Apply extractive methods Implement abstractive generation Enhance with RAG Evaluate summaries",
    "section": "modules"
  }
]