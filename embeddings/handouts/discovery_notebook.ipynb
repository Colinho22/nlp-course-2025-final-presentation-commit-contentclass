{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Discovery Notebook\n",
    "\n",
    "This notebook accompanies the pre-class discovery handout. Use it to experiment with word embeddings and complete the activities.\n",
    "\n",
    "**Author**: NLP Course Team  \n",
    "**Prerequisites**: Basic Python, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set up visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Educational color scheme\n",
    "COLOR_CURRENT = '#FF6B6B'  # Red\n",
    "COLOR_CONTEXT = '#4ECDC4'  # Teal\n",
    "COLOR_PREDICT = '#95E77E'  # Green\n",
    "COLOR_NEUTRAL = '#E0E0E0'  # Gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: String Similarity vs Semantic Similarity\n",
    "\n",
    "Let's explore why character-based similarity doesn't capture meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(w1: str, w2: str) -> float:\n",
    "    \"\"\"Calculate character-based similarity between two words.\"\"\"\n",
    "    # Count matching characters in same positions\n",
    "    matches = sum(1 for c1, c2 in zip(w1, w2) if c1 == c2)\n",
    "    return matches / max(len(w1), len(w2))\n",
    "\n",
    "# Test words\n",
    "word_pairs = [\n",
    "    ('cat', 'dog'),\n",
    "    ('cat', 'car'),\n",
    "    ('cat', 'kitten'),\n",
    "    ('happy', 'joyful'),\n",
    "    ('bank', 'tank'),  # Similar spelling, different meaning\n",
    "]\n",
    "\n",
    "print(\"Character-based Similarity:\")\n",
    "print(\"-\" * 40)\n",
    "for w1, w2 in word_pairs:\n",
    "    sim = string_similarity(w1, w2)\n",
    "    print(f\"{w1:8} vs {w2:8} = {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question\n",
    "Notice how 'bank' and 'tank' have high character similarity but completely different meanings. This shows why we need better representations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: One-Hot Encoding\n",
    "\n",
    "Let's implement one-hot encoding and see its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    \"\"\"Simple one-hot encoder for words.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "        self.vocab_size = len(vocabulary)\n",
    "    \n",
    "    def encode(self, word: str) -> np.ndarray:\n",
    "        \"\"\"Encode a word as a one-hot vector.\"\"\"\n",
    "        vector = np.zeros(self.vocab_size)\n",
    "        if word in self.word_to_idx:\n",
    "            vector[self.word_to_idx[word]] = 1\n",
    "        return vector\n",
    "    \n",
    "    def similarity(self, word1: str, word2: str) -> float:\n",
    "        \"\"\"Calculate similarity between two words using dot product.\"\"\"\n",
    "        vec1 = self.encode(word1)\n",
    "        vec2 = self.encode(word2)\n",
    "        return np.dot(vec1, vec2)\n",
    "\n",
    "# Create encoder with small vocabulary\n",
    "vocab = ['cat', 'dog', 'mat', 'sat', 'hat', 'kitten', 'puppy']\n",
    "encoder = OneHotEncoder(vocab)\n",
    "\n",
    "# Encode some words\n",
    "print(\"One-Hot Encodings:\")\n",
    "print(\"-\" * 40)\n",
    "for word in ['cat', 'dog', 'kitten']:\n",
    "    vec = encoder.encode(word)\n",
    "    print(f\"{word:8} = {vec}\")\n",
    "\n",
    "# Calculate similarities\n",
    "print(\"\\nSimilarities (using dot product):\")\n",
    "print(\"-\" * 40)\n",
    "test_pairs = [('cat', 'dog'), ('cat', 'kitten'), ('cat', 'cat')]\n",
    "for w1, w2 in test_pairs:\n",
    "    sim = encoder.similarity(w1, w2)\n",
    "    print(f\"{w1:8} vs {w2:8} = {sim:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sparsity problem\n",
    "def visualize_one_hot_sparsity(vocab_size: int):\n",
    "    \"\"\"Show how sparse one-hot vectors become with large vocabularies.\"\"\"\n",
    "    \n",
    "    sizes = [10, 100, 1000, 10000, 50000]\n",
    "    sparsity = [1 - 1/s for s in sizes]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(sizes)), sparsity, color=COLOR_CURRENT)\n",
    "    plt.xticks(range(len(sizes)), [f'{s:,}' for s in sizes])\n",
    "    plt.ylabel('Percentage of Zeros')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.title('Sparsity of One-Hot Vectors')\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, s in enumerate(sparsity):\n",
    "        plt.text(i, s + 0.02, f'{s*100:.2f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show a single one-hot vector for vocab_size=50\n",
    "    vec = np.zeros(50)\n",
    "    vec[5] = 1  # Word at index 5\n",
    "    plt.bar(range(50), vec, color=np.where(vec == 1, COLOR_CURRENT, COLOR_NEUTRAL))\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Example: One-Hot Vector (50-word vocabulary)')\n",
    "    plt.ylim(0, 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_one_hot_sparsity(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: Dense Embeddings - 2D Word Space\n",
    "\n",
    "Now let's work with dense vectors where each word is represented by just a few numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D embeddings (normally these would be learned from data)\n",
    "word_embeddings = {\n",
    "    # Animals cluster\n",
    "    'cat': [2.0, 3.0],\n",
    "    'dog': [3.0, 3.0],\n",
    "    'kitten': [1.5, 2.5],\n",
    "    'puppy': [3.5, 2.5],\n",
    "    'pet': [2.5, 3.5],\n",
    "    \n",
    "    # Vehicles cluster\n",
    "    'car': [8.0, 1.0],\n",
    "    'truck': [9.0, 1.5],\n",
    "    'vehicle': [8.5, 0.5],\n",
    "    'bus': [8.5, 2.0],\n",
    "    \n",
    "    # Emotions cluster\n",
    "    'happy': [5.0, 8.0],\n",
    "    'joyful': [5.5, 8.5],\n",
    "    'sad': [5.0, 6.0],\n",
    "    'angry': [4.0, 6.5]\n",
    "}\n",
    "\n",
    "def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate Euclidean distance between two vectors.\"\"\"\n",
    "    return np.sqrt(sum((a - b)**2 for a, b in zip(vec1, vec2)))\n",
    "\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    norm1 = np.sqrt(sum(a**2 for a in vec1))\n",
    "    norm2 = np.sqrt(sum(b**2 for b in vec2))\n",
    "    return dot_product / (norm1 * norm2) if norm1 * norm2 > 0 else 0\n",
    "\n",
    "# Calculate distances\n",
    "print(\"Dense Embedding Distances:\")\n",
    "print(\"-\" * 50)\n",
    "test_pairs = [\n",
    "    ('cat', 'dog'),      # Same category\n",
    "    ('cat', 'kitten'),   # Very similar\n",
    "    ('cat', 'car'),      # Different categories\n",
    "    ('happy', 'joyful'), # Synonyms\n",
    "    ('happy', 'sad'),    # Antonyms\n",
    "]\n",
    "\n",
    "for w1, w2 in test_pairs:\n",
    "    vec1 = word_embeddings[w1]\n",
    "    vec2 = word_embeddings[w2]\n",
    "    dist = euclidean_distance(vec1, vec2)\n",
    "    cos_sim = cosine_similarity(vec1, vec2)\n",
    "    print(f\"{w1:8} vs {w2:8}: distance={dist:.2f}, cosine_sim={cos_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_space(embeddings: Dict[str, List[float]], highlight_pairs=None):\n",
    "    \"\"\"Visualize words in 2D space.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Define categories for coloring\n",
    "    categories = {\n",
    "        'animals': ['cat', 'dog', 'kitten', 'puppy', 'pet'],\n",
    "        'vehicles': ['car', 'truck', 'vehicle', 'bus'],\n",
    "        'emotions': ['happy', 'joyful', 'sad', 'angry']\n",
    "    }\n",
    "    \n",
    "    colors = {\n",
    "        'animals': COLOR_CONTEXT,\n",
    "        'vehicles': COLOR_CURRENT,\n",
    "        'emotions': COLOR_PREDICT\n",
    "    }\n",
    "    \n",
    "    # Plot words by category\n",
    "    for category, words in categories.items():\n",
    "        for word in words:\n",
    "            if word in embeddings:\n",
    "                x, y = embeddings[word]\n",
    "                plt.scatter(x, y, s=200, c=colors[category], \n",
    "                          alpha=0.7, edgecolors='black', linewidths=2)\n",
    "                plt.annotate(word, (x, y), fontsize=10, fontweight='bold',\n",
    "                           ha='center', va='center')\n",
    "    \n",
    "    # Highlight specific pairs if requested\n",
    "    if highlight_pairs:\n",
    "        for w1, w2 in highlight_pairs:\n",
    "            if w1 in embeddings and w2 in embeddings:\n",
    "                x1, y1 = embeddings[w1]\n",
    "                x2, y2 = embeddings[w2]\n",
    "                plt.plot([x1, x2], [y1, y2], 'k--', alpha=0.3, linewidth=2)\n",
    "                \n",
    "                # Add distance label\n",
    "                mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "                dist = euclidean_distance(embeddings[w1], embeddings[w2])\n",
    "                plt.text(mid_x, mid_y, f'{dist:.1f}', fontsize=8,\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', \n",
    "                                 facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    plt.xlabel('Dimension 1', fontsize=12)\n",
    "    plt.ylabel('Dimension 2', fontsize=12)\n",
    "    plt.title('Word Embeddings in 2D Space - Similar Words Cluster Together',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=colors[cat], label=cat.capitalize())\n",
    "                      for cat in categories.keys()]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the word space\n",
    "plot_word_space(word_embeddings, highlight_pairs=[('cat', 'kitten'), ('cat', 'car')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4: Word Arithmetic\n",
    "\n",
    "One of the most fascinating properties of word embeddings is that they can capture analogies through vector arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified embeddings that demonstrate relationships\n",
    "analogy_embeddings = {\n",
    "    # Gender relationship\n",
    "    'king': [5.0, 3.0],\n",
    "    'queen': [5.0, 2.0],\n",
    "    'man': [4.0, 3.0],\n",
    "    'woman': [4.0, 2.0],\n",
    "    \n",
    "    # Country-Capital relationship\n",
    "    'Paris': [2.0, 5.0],\n",
    "    'France': [3.0, 6.0],\n",
    "    'Berlin': [6.0, 5.0],\n",
    "    'Germany': [7.0, 6.0],\n",
    "    'London': [10.0, 5.0],\n",
    "    'England': [11.0, 6.0],\n",
    "    \n",
    "    # Size relationship\n",
    "    'small': [1.0, 1.0],\n",
    "    'smaller': [0.5, 0.5],\n",
    "    'big': [3.0, 3.0],\n",
    "    'bigger': [3.5, 3.5]\n",
    "}\n",
    "\n",
    "def word_arithmetic(embeddings: Dict, word1: str, word2: str, word3: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform word arithmetic: word1 - word2 + word3\n",
    "    Example: king - man + woman = queen\n",
    "    \"\"\"\n",
    "    vec1 = np.array(embeddings[word1])\n",
    "    vec2 = np.array(embeddings[word2])\n",
    "    vec3 = np.array(embeddings[word3])\n",
    "    \n",
    "    result = vec1 - vec2 + vec3\n",
    "    return result\n",
    "\n",
    "def find_nearest_word(embeddings: Dict, target_vec: np.ndarray, \n",
    "                     exclude_words: List[str] = None) -> Tuple[str, float]:\n",
    "    \"\"\"Find the word whose embedding is closest to the target vector.\"\"\"\n",
    "    if exclude_words is None:\n",
    "        exclude_words = []\n",
    "    \n",
    "    best_word = None\n",
    "    best_distance = float('inf')\n",
    "    \n",
    "    for word, vec in embeddings.items():\n",
    "        if word not in exclude_words:\n",
    "            dist = euclidean_distance(vec, target_vec.tolist())\n",
    "            if dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_word = word\n",
    "    \n",
    "    return best_word, best_distance\n",
    "\n",
    "# Test word arithmetic\n",
    "print(\"Word Arithmetic Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: king - man + woman = ?\n",
    "result = word_arithmetic(analogy_embeddings, 'king', 'man', 'woman')\n",
    "nearest, dist = find_nearest_word(analogy_embeddings, result, \n",
    "                                 exclude_words=['king', 'man', 'woman'])\n",
    "print(f\"king - man + woman = {result}\")\n",
    "print(f\"Nearest word: {nearest} (distance: {dist:.2f})\")\n",
    "print(f\"Expected: queen\")\n",
    "print()\n",
    "\n",
    "# Example 2: Paris - France + Germany = ?\n",
    "result = word_arithmetic(analogy_embeddings, 'Paris', 'France', 'Germany')\n",
    "nearest, dist = find_nearest_word(analogy_embeddings, result,\n",
    "                                 exclude_words=['Paris', 'France', 'Germany'])\n",
    "print(f\"Paris - France + Germany = {result}\")\n",
    "print(f\"Nearest word: {nearest} (distance: {dist:.2f})\")\n",
    "print(f\"Expected: Berlin\")\n",
    "print()\n",
    "\n",
    "# Example 3: small - big + bigger = ?\n",
    "result = word_arithmetic(analogy_embeddings, 'small', 'big', 'bigger')\n",
    "nearest, dist = find_nearest_word(analogy_embeddings, result,\n",
    "                                 exclude_words=['small', 'big', 'bigger'])\n",
    "print(f\"small - big + bigger = {result}\")\n",
    "print(f\"Nearest word: {nearest} (distance: {dist:.2f})\")\n",
    "print(f\"Expected: smaller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_word_arithmetic(embeddings: Dict, w1: str, w2: str, w3: str):\n",
    "    \"\"\"Visualize word arithmetic as vectors.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Get vectors\n",
    "    vec1 = np.array(embeddings[w1])\n",
    "    vec2 = np.array(embeddings[w2])\n",
    "    vec3 = np.array(embeddings[w3])\n",
    "    result = vec1 - vec2 + vec3\n",
    "    \n",
    "    # Find nearest word\n",
    "    nearest, _ = find_nearest_word(embeddings, result, exclude_words=[w1, w2, w3])\n",
    "    vec_nearest = np.array(embeddings[nearest])\n",
    "    \n",
    "    # Plot points\n",
    "    plt.scatter(*vec1, s=300, c=COLOR_CURRENT, marker='*', \n",
    "               edgecolors='black', linewidths=2, zorder=5)\n",
    "    plt.scatter(*vec2, s=200, c=COLOR_CONTEXT, marker='o',\n",
    "               edgecolors='black', linewidths=2, zorder=5)\n",
    "    plt.scatter(*vec3, s=200, c=COLOR_CONTEXT, marker='o',\n",
    "               edgecolors='black', linewidths=2, zorder=5)\n",
    "    plt.scatter(*result, s=300, c='yellow', marker='D',\n",
    "               edgecolors='black', linewidths=2, zorder=5)\n",
    "    plt.scatter(*vec_nearest, s=300, c=COLOR_PREDICT, marker='*',\n",
    "               edgecolors='black', linewidths=2, zorder=5)\n",
    "    \n",
    "    # Labels\n",
    "    plt.annotate(w1, vec1, xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    plt.annotate(w2, vec2, xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    plt.annotate(w3, vec3, xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    plt.annotate('result', result, xytext=(5, -15), textcoords='offset points',\n",
    "                fontsize=11, style='italic', color='orange')\n",
    "    plt.annotate(nearest, vec_nearest, xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold', color=COLOR_PREDICT)\n",
    "    \n",
    "    # Draw vectors\n",
    "    # vec1 - vec2\n",
    "    plt.arrow(vec1[0], vec1[1], vec2[0]-vec1[0], vec2[1]-vec1[1],\n",
    "             head_width=0.1, head_length=0.1, fc='red', ec='red',\n",
    "             linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # + vec3\n",
    "    plt.arrow(vec2[0], vec2[1], vec3[0]-vec2[0], vec3[1]-vec2[1],\n",
    "             head_width=0.1, head_length=0.1, fc='blue', ec='blue',\n",
    "             linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Result\n",
    "    plt.arrow(vec3[0], vec3[1], result[0]-vec3[0], result[1]-vec3[1],\n",
    "             head_width=0.15, head_length=0.15, fc='orange', ec='orange',\n",
    "             linewidth=2)\n",
    "    \n",
    "    plt.title(f'Word Arithmetic: {w1} - {w2} + {w3} = {nearest}',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Dimension 1', fontsize=12)\n",
    "    plt.ylabel('Dimension 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the king-queen analogy\n",
    "visualize_word_arithmetic(analogy_embeddings, 'king', 'man', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 5: Context Matters - Static vs Contextual Embeddings\n",
    "\n",
    "Let's explore why modern NLP uses contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate contextual embeddings\n",
    "class ContextualEmbedding:\n",
    "    \"\"\"Simulate how contextual embeddings work.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Base embeddings (static)\n",
    "        self.static_embeddings = {\n",
    "            'bank': [5.0, 5.0],  # Ambiguous position\n",
    "            'money': [8.0, 8.0],\n",
    "            'river': [2.0, 2.0],\n",
    "            'water': [1.0, 3.0],\n",
    "            'account': [9.0, 7.0],\n",
    "            'fish': [2.0, 1.0]\n",
    "        }\n",
    "        \n",
    "    def get_contextual_embedding(self, word: str, context: List[str]) -> List[float]:\n",
    "        \"\"\"Get embedding based on context.\"\"\"\n",
    "        if word != 'bank':\n",
    "            return self.static_embeddings.get(word, [0, 0])\n",
    "        \n",
    "        # Adjust 'bank' embedding based on context\n",
    "        financial_words = {'money', 'account', 'deposit', 'loan', 'savings'}\n",
    "        nature_words = {'river', 'water', 'fish', 'boat', 'shore'}\n",
    "        \n",
    "        financial_score = sum(1 for w in context if w in financial_words)\n",
    "        nature_score = sum(1 for w in context if w in nature_words)\n",
    "        \n",
    "        if financial_score > nature_score:\n",
    "            return [8.0, 7.5]  # Near financial terms\n",
    "        elif nature_score > financial_score:\n",
    "            return [2.0, 2.5]  # Near nature terms\n",
    "        else:\n",
    "            return [5.0, 5.0]  # Default ambiguous position\n",
    "\n",
    "# Test contextual embeddings\n",
    "embedder = ContextualEmbedding()\n",
    "\n",
    "# Different contexts for 'bank'\n",
    "contexts = [\n",
    "    ['I', 'deposited', 'money', 'in', 'the', 'bank'],\n",
    "    ['We', 'sat', 'by', 'the', 'river', 'bank'],\n",
    "    ['The', 'bank', 'has', 'many', 'branches'],  # Ambiguous!\n",
    "]\n",
    "\n",
    "print(\"Contextual Embeddings for 'bank':\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, context in enumerate(contexts, 1):\n",
    "    embedding = embedder.get_contextual_embedding('bank', context)\n",
    "    print(f\"Context {i}: {' '.join(context)}\")\n",
    "    print(f\"  Embedding: {embedding}\")\n",
    "    \n",
    "    # Find nearest words\n",
    "    distances = {}\n",
    "    for word, vec in embedder.static_embeddings.items():\n",
    "        if word != 'bank':\n",
    "            dist = euclidean_distance(embedding, vec)\n",
    "            distances[word] = dist\n",
    "    \n",
    "    nearest = min(distances, key=distances.get)\n",
    "    print(f\"  Nearest word: {nearest} (distance: {distances[nearest]:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_contextual_embeddings():\n",
    "    \"\"\"Visualize how context changes word embeddings.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    embedder = ContextualEmbedding()\n",
    "    \n",
    "    # Static embedding problem\n",
    "    ax1.set_title('Problem: Static Embedding for \"bank\"', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot static embeddings\n",
    "    for word, pos in embedder.static_embeddings.items():\n",
    "        if word == 'bank':\n",
    "            ax1.scatter(*pos, s=400, c=COLOR_CURRENT, marker='*',\n",
    "                      edgecolors='black', linewidths=2, zorder=5)\n",
    "        elif word in ['money', 'account']:\n",
    "            ax1.scatter(*pos, s=200, c='gold', marker='o',\n",
    "                      edgecolors='black', linewidths=2, zorder=5)\n",
    "        else:\n",
    "            ax1.scatter(*pos, s=200, c='lightblue', marker='o',\n",
    "                      edgecolors='black', linewidths=2, zorder=5)\n",
    "        ax1.annotate(word, pos, xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=10)\n",
    "    \n",
    "    ax1.text(5, 6.5, '?', fontsize=30, fontweight='bold', color='red')\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlabel('Dimension 1')\n",
    "    ax1.set_ylabel('Dimension 2')\n",
    "    \n",
    "    # Contextual embedding solution\n",
    "    ax2.set_title('Solution: Contextual Embeddings', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot base words\n",
    "    for word, pos in embedder.static_embeddings.items():\n",
    "        if word != 'bank':\n",
    "            if word in ['money', 'account']:\n",
    "                ax2.scatter(*pos, s=200, c='gold', marker='o',\n",
    "                          edgecolors='black', linewidths=2, zorder=5)\n",
    "            else:\n",
    "                ax2.scatter(*pos, s=200, c='lightblue', marker='o',\n",
    "                          edgecolors='black', linewidths=2, zorder=5)\n",
    "            ax2.annotate(word, pos, xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=10)\n",
    "    \n",
    "    # Plot contextual bank embeddings\n",
    "    bank_financial = embedder.get_contextual_embedding('bank', ['money', 'account'])\n",
    "    bank_river = embedder.get_contextual_embedding('bank', ['river', 'water'])\n",
    "    \n",
    "    ax2.scatter(*bank_financial, s=400, c=COLOR_PREDICT, marker='*',\n",
    "              edgecolors='black', linewidths=2, zorder=5)\n",
    "    ax2.annotate('bank\\n(financial)', bank_financial, \n",
    "                xytext=(5, -15), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold', ha='center')\n",
    "    \n",
    "    ax2.scatter(*bank_river, s=400, c=COLOR_PREDICT, marker='*',\n",
    "              edgecolors='black', linewidths=2, zorder=5)\n",
    "    ax2.annotate('bank\\n(river)', bank_river,\n",
    "                xytext=(5, -15), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold', ha='center')\n",
    "    \n",
    "    # Draw connections\n",
    "    ax2.plot([bank_financial[0], embedder.static_embeddings['money'][0]],\n",
    "            [bank_financial[1], embedder.static_embeddings['money'][1]],\n",
    "            'g--', alpha=0.5, linewidth=2)\n",
    "    ax2.plot([bank_river[0], embedder.static_embeddings['river'][0]],\n",
    "            [bank_river[1], embedder.static_embeddings['river'][1]],\n",
    "            'g--', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    ax2.set_xlim(0, 10)\n",
    "    ax2.set_ylim(0, 10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlabel('Dimension 1')\n",
    "    ax2.set_ylabel('Dimension 2')\n",
    "    \n",
    "    plt.suptitle('Context Matters: Same Word, Different Meanings',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_contextual_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What You've Discovered\n",
    "\n",
    "1. **Character similarity ≠ Semantic similarity**: Words that are spelled similarly may have completely different meanings.\n",
    "\n",
    "2. **One-hot encoding limitations**:\n",
    "   - All words are equidistant (orthogonal)\n",
    "   - Extremely sparse representations\n",
    "   - No semantic relationships captured\n",
    "\n",
    "3. **Dense embeddings advantages**:\n",
    "   - Similar words cluster together\n",
    "   - Compact representations (e.g., 100-300 dimensions vs 50,000+)\n",
    "   - Can measure meaningful similarities\n",
    "\n",
    "4. **Word arithmetic magic**:\n",
    "   - Embeddings capture analogies\n",
    "   - Relationships are encoded as vector differences\n",
    "   - king - man + woman ≈ queen\n",
    "\n",
    "5. **Context matters**:\n",
    "   - Static embeddings can't handle polysemy (multiple meanings)\n",
    "   - Contextual embeddings (BERT, GPT) solve this by creating different vectors based on context\n",
    "\n",
    "### Questions for Class Discussion\n",
    "\n",
    "1. How do you think computers actually *learn* these embeddings from text?\n",
    "2. What's the optimal number of dimensions for embeddings? (Hint: it's usually 100-300, but why?)\n",
    "3. Can we use embeddings for things other than words? (sentences? documents? images?)\n",
    "4. What are the ethical implications of word embeddings learning from biased data?\n",
    "\n",
    "### Try This Before Class\n",
    "\n",
    "Think of other word relationships that might work with arithmetic:\n",
    "- puppy - dog + cat = ?\n",
    "- Tokyo - Japan + France = ?\n",
    "- walked - walk + run = ?\n",
    "\n",
    "### Resources for Further Exploration\n",
    "\n",
    "- [Word2Vec Explained](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}