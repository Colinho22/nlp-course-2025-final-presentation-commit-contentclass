{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 12 Lab: Ethics & Fairness in NLP\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand and measure bias in NLP models\n",
        "- Implement fairness metrics\n",
        "- Explore toxicity detection\n",
        "- Practice responsible AI deployment\n",
        "\n",
        "## Prerequisites\n",
        "```bash\n",
        "pip install transformers torch numpy matplotlib\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "from collections import defaultdict\n",
        "\n",
        "# Setup\n",
        "print('Week 12: Ethics & Fairness in NLP')\n",
        "print('=' * 50)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Bias in Language Models\n",
        "\n",
        "Language models can encode societal biases present in their training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a masked language model\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Create a fill-mask pipeline\n",
        "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=5)\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test for gender bias in occupations\n",
        "def test_occupation_bias(occupation):\n",
        "    \"\"\"Test gender association with an occupation\"\"\"\n",
        "    templates = [\n",
        "        f\"The {occupation} said [MASK] would finish the work.\",\n",
        "        f\"The {occupation} finished [MASK] work.\",\n",
        "    ]\n",
        "    \n",
        "    results = {'male': 0, 'female': 0}\n",
        "    male_words = {'he', 'his', 'him'}\n",
        "    female_words = {'she', 'her', 'hers'}\n",
        "    \n",
        "    for template in templates:\n",
        "        predictions = fill_mask(template)\n",
        "        for pred in predictions:\n",
        "            token = pred['token_str'].lower()\n",
        "            if token in male_words:\n",
        "                results['male'] += pred['score']\n",
        "            elif token in female_words:\n",
        "                results['female'] += pred['score']\n",
        "    \n",
        "    total = results['male'] + results['female']\n",
        "    if total > 0:\n",
        "        results['male'] /= total\n",
        "        results['female'] /= total\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test various occupations\n",
        "occupations = ['doctor', 'nurse', 'engineer', 'teacher', 'CEO', 'secretary', 'scientist', 'receptionist']\n",
        "\n",
        "print(\"Gender Bias in Occupation Associations:\")\n",
        "print(\"-\" * 50)\n",
        "bias_results = {}\n",
        "for occ in occupations:\n",
        "    result = test_occupation_bias(occ)\n",
        "    bias_results[occ] = result\n",
        "    print(f\"{occ:15} Male: {result['male']:.1%}  Female: {result['female']:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize occupation bias\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "x = np.arange(len(occupations))\n",
        "male_scores = [bias_results[occ]['male'] for occ in occupations]\n",
        "female_scores = [bias_results[occ]['female'] for occ in occupations]\n",
        "\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, male_scores, width, label='Male', color='#3333B2')\n",
        "bars2 = ax.bar(x + width/2, female_scores, width, label='Female', color='#FF7F0E')\n",
        "\n",
        "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='Equal (50%)')\n",
        "ax.set_xlabel('Occupation', fontsize=12)\n",
        "ax.set_ylabel('Gender Association Score', fontsize=12)\n",
        "ax.set_title('Gender Bias in BERT Occupation Associations', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(occupations, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Measuring Bias with WEAT\n",
        "\n",
        "Word Embedding Association Test (WEAT) quantifies bias in word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_word_embedding(word, tokenizer, model):\n",
        "    \"\"\"Get the embedding for a word from BERT\"\"\"\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model.bert(**inputs)\n",
        "    # Use [CLS] token embedding\n",
        "    return outputs.last_hidden_state[0, 0].numpy()\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def weat_score(target1, target2, attribute1, attribute2, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Compute WEAT score (simplified version).\n",
        "    \n",
        "    Higher positive score = target1 more associated with attribute1\n",
        "    Higher negative score = target1 more associated with attribute2\n",
        "    \"\"\"\n",
        "    # Get embeddings\n",
        "    t1_emb = [get_word_embedding(w, tokenizer, model) for w in target1]\n",
        "    t2_emb = [get_word_embedding(w, tokenizer, model) for w in target2]\n",
        "    a1_emb = [get_word_embedding(w, tokenizer, model) for w in attribute1]\n",
        "    a2_emb = [get_word_embedding(w, tokenizer, model) for w in attribute2]\n",
        "    \n",
        "    def association(word_emb, attr1_embs, attr2_embs):\n",
        "        \"\"\"Mean similarity to attr1 minus mean similarity to attr2\"\"\"\n",
        "        s1 = np.mean([cosine_similarity(word_emb, a) for a in attr1_embs])\n",
        "        s2 = np.mean([cosine_similarity(word_emb, a) for a in attr2_embs])\n",
        "        return s1 - s2\n",
        "    \n",
        "    # Compute associations for target sets\n",
        "    s_t1 = np.mean([association(t, a1_emb, a2_emb) for t in t1_emb])\n",
        "    s_t2 = np.mean([association(t, a1_emb, a2_emb) for t in t2_emb])\n",
        "    \n",
        "    return s_t1 - s_t2\n",
        "\n",
        "# Define word sets for gender bias test\n",
        "male_names = ['john', 'paul', 'mike', 'kevin', 'steve']\n",
        "female_names = ['mary', 'susan', 'lisa', 'sarah', 'emma']\n",
        "career_words = ['career', 'professional', 'office', 'business', 'salary']\n",
        "family_words = ['family', 'home', 'children', 'parents', 'marriage']\n",
        "\n",
        "score = weat_score(male_names, female_names, career_words, family_words, tokenizer, model)\n",
        "print(f\"WEAT Score (Gender-Career): {score:.4f}\")\n",
        "print(f\"Interpretation: {'Male names more associated with career' if score > 0 else 'Female names more associated with career'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Fairness Metrics\n",
        "\n",
        "Let's implement common fairness metrics for classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demographic_parity(predictions, sensitive_attribute):\n",
        "    \"\"\"\n",
        "    Measure demographic parity (equal positive rate across groups).\n",
        "    \n",
        "    Perfect fairness = ratio of 1.0\n",
        "    \"\"\"\n",
        "    groups = np.unique(sensitive_attribute)\n",
        "    positive_rates = {}\n",
        "    \n",
        "    for group in groups:\n",
        "        mask = sensitive_attribute == group\n",
        "        positive_rates[group] = np.mean(predictions[mask])\n",
        "    \n",
        "    # Return ratio of min to max positive rate\n",
        "    rates = list(positive_rates.values())\n",
        "    return min(rates) / max(rates) if max(rates) > 0 else 1.0, positive_rates\n",
        "\n",
        "def equalized_odds(predictions, labels, sensitive_attribute):\n",
        "    \"\"\"\n",
        "    Measure equalized odds (equal TPR and FPR across groups).\n",
        "    \"\"\"\n",
        "    groups = np.unique(sensitive_attribute)\n",
        "    tpr = {}\n",
        "    fpr = {}\n",
        "    \n",
        "    for group in groups:\n",
        "        mask = sensitive_attribute == group\n",
        "        group_pred = predictions[mask]\n",
        "        group_label = labels[mask]\n",
        "        \n",
        "        # True Positive Rate\n",
        "        pos_mask = group_label == 1\n",
        "        if pos_mask.sum() > 0:\n",
        "            tpr[group] = np.mean(group_pred[pos_mask])\n",
        "        else:\n",
        "            tpr[group] = 0\n",
        "        \n",
        "        # False Positive Rate\n",
        "        neg_mask = group_label == 0\n",
        "        if neg_mask.sum() > 0:\n",
        "            fpr[group] = np.mean(group_pred[neg_mask])\n",
        "        else:\n",
        "            fpr[group] = 0\n",
        "    \n",
        "    return tpr, fpr\n",
        "\n",
        "# Simulate predictions\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate sensitive attribute (e.g., gender)\n",
        "sensitive = np.random.choice(['A', 'B'], n_samples)\n",
        "\n",
        "# Generate true labels\n",
        "labels = np.random.randint(0, 2, n_samples)\n",
        "\n",
        "# Generate biased predictions (group A gets higher positive rate)\n",
        "predictions = np.zeros(n_samples)\n",
        "predictions[sensitive == 'A'] = np.random.binomial(1, 0.7, (sensitive == 'A').sum())\n",
        "predictions[sensitive == 'B'] = np.random.binomial(1, 0.4, (sensitive == 'B').sum())\n",
        "\n",
        "# Calculate fairness metrics\n",
        "dp_ratio, positive_rates = demographic_parity(predictions, sensitive)\n",
        "tpr, fpr = equalized_odds(predictions, labels, sensitive)\n",
        "\n",
        "print(\"Fairness Metrics:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"\\nDemographic Parity:\")\n",
        "print(f\"  Positive rate (Group A): {positive_rates['A']:.1%}\")\n",
        "print(f\"  Positive rate (Group B): {positive_rates['B']:.1%}\")\n",
        "print(f\"  DP Ratio: {dp_ratio:.3f} (1.0 = perfect parity)\")\n",
        "\n",
        "print(f\"\\nEqualized Odds:\")\n",
        "print(f\"  TPR (Group A): {tpr['A']:.1%}, TPR (Group B): {tpr['B']:.1%}\")\n",
        "print(f\"  FPR (Group A): {fpr['A']:.1%}, FPR (Group B): {fpr['B']:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize fairness metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Demographic Parity\n",
        "ax = axes[0]\n",
        "bars = ax.bar(['Group A', 'Group B'], [positive_rates['A'], positive_rates['B']], \n",
        "              color=['#3333B2', '#FF7F0E'])\n",
        "ax.axhline(np.mean(predictions), color='gray', linestyle='--', label='Average')\n",
        "ax.set_ylabel('Positive Prediction Rate')\n",
        "ax.set_title('Demographic Parity', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.legend()\n",
        "\n",
        "# TPR Comparison\n",
        "ax = axes[1]\n",
        "ax.bar(['Group A', 'Group B'], [tpr['A'], tpr['B']], color=['#3333B2', '#FF7F0E'])\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('TPR by Group', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "# FPR Comparison\n",
        "ax = axes[2]\n",
        "ax.bar(['Group A', 'Group B'], [fpr['A'], fpr['B']], color=['#3333B2', '#FF7F0E'])\n",
        "ax.set_ylabel('False Positive Rate')\n",
        "ax.set_title('FPR by Group', fontsize=12, fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.suptitle('Fairness Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Toxicity Detection\n",
        "\n",
        "Let's explore how to detect and measure toxic content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple toxicity word list approach (for demonstration)\n",
        "# In practice, use trained models like Perspective API or Detoxify\n",
        "\n",
        "toxic_keywords = {\n",
        "    'hate': 0.8,\n",
        "    'stupid': 0.5,\n",
        "    'idiot': 0.7,\n",
        "    'terrible': 0.3,\n",
        "    'awful': 0.3,\n",
        "    'worst': 0.4,\n",
        "    'horrible': 0.4,\n",
        "}\n",
        "\n",
        "def simple_toxicity_score(text):\n",
        "    \"\"\"Calculate a simple toxicity score based on keyword matching.\"\"\"\n",
        "    words = text.lower().split()\n",
        "    scores = [toxic_keywords.get(word, 0) for word in words]\n",
        "    return max(scores) if scores else 0\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"This is a great product!\",\n",
        "    \"I hate this terrible service.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"You are such an idiot.\",\n",
        "    \"This movie was the worst ever.\",\n",
        "]\n",
        "\n",
        "print(\"Toxicity Detection Results:\")\n",
        "print(\"-\" * 60)\n",
        "for sentence in test_sentences:\n",
        "    score = simple_toxicity_score(sentence)\n",
        "    level = 'HIGH' if score > 0.5 else 'MEDIUM' if score > 0.3 else 'LOW'\n",
        "    print(f\"[{level:6}] {score:.2f} | {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze toxicity distribution in a corpus\n",
        "sample_corpus = [\n",
        "    \"Great job on the project!\",\n",
        "    \"This is absolutely terrible work.\",\n",
        "    \"I really appreciate your help.\",\n",
        "    \"What a stupid idea.\",\n",
        "    \"The presentation was excellent.\",\n",
        "    \"I hate having to deal with this.\",\n",
        "    \"Thank you for your patience.\",\n",
        "    \"This is the worst experience ever.\",\n",
        "    \"Looking forward to our next meeting.\",\n",
        "    \"How can you be such an idiot?\",\n",
        "] * 10  # Replicate for visualization\n",
        "\n",
        "# Calculate toxicity scores\n",
        "toxicity_scores = [simple_toxicity_score(text) for text in sample_corpus]\n",
        "\n",
        "# Visualize distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Histogram\n",
        "ax1.hist(toxicity_scores, bins=20, color='#3333B2', alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(np.mean(toxicity_scores), color='red', linestyle='--', label=f'Mean: {np.mean(toxicity_scores):.2f}')\n",
        "ax1.set_xlabel('Toxicity Score')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Toxicity Score Distribution', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# Category breakdown\n",
        "categories = ['Low (<0.3)', 'Medium (0.3-0.5)', 'High (>0.5)']\n",
        "counts = [\n",
        "    sum(1 for s in toxicity_scores if s < 0.3),\n",
        "    sum(1 for s in toxicity_scores if 0.3 <= s <= 0.5),\n",
        "    sum(1 for s in toxicity_scores if s > 0.5)\n",
        "]\n",
        "colors = ['#2CA02C', '#FF7F0E', '#D62728']\n",
        "ax2.pie(counts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title('Toxicity Categories', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Bias Mitigation Strategies\n",
        "\n",
        "Let's explore techniques to reduce bias in models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def counterfactual_augmentation(text, word_pairs):\n",
        "    \"\"\"\n",
        "    Create counterfactual examples by swapping identity terms.\n",
        "    \n",
        "    This helps balance training data.\n",
        "    \"\"\"\n",
        "    augmented = [text]\n",
        "    \n",
        "    for word1, word2 in word_pairs:\n",
        "        if word1 in text.lower():\n",
        "            # Replace preserving case\n",
        "            new_text = text.replace(word1, word2).replace(word1.capitalize(), word2.capitalize())\n",
        "            augmented.append(new_text)\n",
        "        if word2 in text.lower():\n",
        "            new_text = text.replace(word2, word1).replace(word2.capitalize(), word1.capitalize())\n",
        "            augmented.append(new_text)\n",
        "    \n",
        "    return list(set(augmented))\n",
        "\n",
        "# Gender swap pairs\n",
        "gender_pairs = [\n",
        "    ('he', 'she'),\n",
        "    ('him', 'her'),\n",
        "    ('his', 'her'),\n",
        "    ('man', 'woman'),\n",
        "    ('boy', 'girl'),\n",
        "    ('father', 'mother'),\n",
        "]\n",
        "\n",
        "# Test counterfactual augmentation\n",
        "test_text = \"The man said he would finish his project.\"\n",
        "augmented = counterfactual_augmentation(test_text, gender_pairs)\n",
        "\n",
        "print(\"Counterfactual Augmentation:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Original: {test_text}\")\n",
        "print(f\"Augmented versions:\")\n",
        "for text in augmented:\n",
        "    if text != test_text:\n",
        "        print(f\"  -> {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def threshold_calibration(predictions, sensitive_attribute, target_rate=None):\n",
        "    \"\"\"\n",
        "    Calibrate decision thresholds for each group to achieve demographic parity.\n",
        "    \"\"\"\n",
        "    if target_rate is None:\n",
        "        target_rate = np.mean(predictions)\n",
        "    \n",
        "    groups = np.unique(sensitive_attribute)\n",
        "    thresholds = {}\n",
        "    \n",
        "    for group in groups:\n",
        "        group_preds = predictions[sensitive_attribute == group]\n",
        "        # Find threshold that gives target positive rate\n",
        "        thresholds[group] = np.percentile(group_preds, 100 * (1 - target_rate))\n",
        "    \n",
        "    # Apply group-specific thresholds\n",
        "    calibrated = np.zeros_like(predictions)\n",
        "    for group in groups:\n",
        "        mask = sensitive_attribute == group\n",
        "        calibrated[mask] = (predictions[mask] > thresholds[group]).astype(int)\n",
        "    \n",
        "    return calibrated, thresholds\n",
        "\n",
        "# Generate continuous predictions\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "sensitive = np.random.choice(['A', 'B'], n_samples)\n",
        "\n",
        "# Biased continuous predictions\n",
        "predictions_cont = np.zeros(n_samples)\n",
        "predictions_cont[sensitive == 'A'] = np.random.beta(3, 2, (sensitive == 'A').sum())\n",
        "predictions_cont[sensitive == 'B'] = np.random.beta(2, 3, (sensitive == 'B').sum())\n",
        "\n",
        "# Original binary predictions (threshold = 0.5)\n",
        "original_binary = (predictions_cont > 0.5).astype(int)\n",
        "\n",
        "# Calibrated predictions\n",
        "calibrated_binary, thresholds = threshold_calibration(predictions_cont, sensitive)\n",
        "\n",
        "# Compare results\n",
        "print(\"Threshold Calibration Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"\\nOriginal (threshold=0.5 for all):\")\n",
        "for group in ['A', 'B']:\n",
        "    rate = np.mean(original_binary[sensitive == group])\n",
        "    print(f\"  Group {group}: {rate:.1%} positive\")\n",
        "\n",
        "print(f\"\\nCalibrated (group-specific thresholds):\")\n",
        "for group in ['A', 'B']:\n",
        "    rate = np.mean(calibrated_binary[sensitive == group])\n",
        "    print(f\"  Group {group}: {rate:.1%} positive (threshold={thresholds[group]:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Bias Detection**: Test BERT for racial or religious bias using appropriate word sets\n",
        "2. **Fairness**: Implement additional fairness metrics (predictive parity, calibration)\n",
        "3. **Mitigation**: Apply debiasing techniques to a sentiment classifier\n",
        "4. **Audit**: Create a model audit report for a text classification system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise starter: Model audit template\n",
        "def model_audit_report(model_name, predictions, labels, sensitive_attr, groups):\n",
        "    \"\"\"\n",
        "    Generate a fairness audit report for a model.\n",
        "    \"\"\"\n",
        "    report = {\n",
        "        'model': model_name,\n",
        "        'total_samples': len(predictions),\n",
        "        'overall_accuracy': np.mean(predictions == labels),\n",
        "        'group_metrics': {}\n",
        "    }\n",
        "    \n",
        "    for group in groups:\n",
        "        mask = sensitive_attr == group\n",
        "        group_preds = predictions[mask]\n",
        "        group_labels = labels[mask]\n",
        "        \n",
        "        report['group_metrics'][group] = {\n",
        "            'size': mask.sum(),\n",
        "            'accuracy': np.mean(group_preds == group_labels),\n",
        "            'positive_rate': np.mean(group_preds),\n",
        "            'base_rate': np.mean(group_labels),\n",
        "        }\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Generate sample data\n",
        "labels = np.random.randint(0, 2, n_samples)\n",
        "report = model_audit_report('SampleModel', calibrated_binary, labels, sensitive, ['A', 'B'])\n",
        "\n",
        "print(\"Model Audit Report\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {report['model']}\")\n",
        "print(f\"Total Samples: {report['total_samples']}\")\n",
        "print(f\"Overall Accuracy: {report['overall_accuracy']:.1%}\")\n",
        "print(f\"\\nGroup Metrics:\")\n",
        "for group, metrics in report['group_metrics'].items():\n",
        "    print(f\"  Group {group}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    {metric}: {value:.1%}\")\n",
        "        else:\n",
        "            print(f\"    {metric}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, we explored:\n",
        "\n",
        "1. **Bias detection**: Using MLM probing and WEAT to identify biases\n",
        "2. **Fairness metrics**: Demographic parity and equalized odds\n",
        "3. **Toxicity detection**: Simple keyword-based approaches\n",
        "4. **Bias mitigation**: Counterfactual augmentation and threshold calibration\n",
        "5. **Model auditing**: Creating fairness reports\n",
        "\n",
        "**Key Takeaways**:\n",
        "- Language models encode societal biases from training data\n",
        "- Fairness requires careful metric selection based on context\n",
        "- Multiple mitigation strategies exist, each with tradeoffs\n",
        "- Regular auditing is essential for responsible AI deployment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
