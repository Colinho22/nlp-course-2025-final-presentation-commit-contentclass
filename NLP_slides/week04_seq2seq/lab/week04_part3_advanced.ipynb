{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Beam Search and Modern Translation\n",
    "\n",
    "## Welcome to Part 3!\n",
    "\n",
    "You've learned seq2seq (Part 1) and attention (Part 2). Now let's explore how to find BETTER translations!\n",
    "\n",
    "**What you'll learn:**\n",
    "- Why greedy decoding fails\n",
    "- How beam search works\n",
    "- Quality vs speed tradeoffs\n",
    "- Modern translation systems\n",
    "\n",
    "**Time: 15-20 minutes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Ready to explore search strategies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem with Greedy Decoding\n",
    "\n",
    "### What is Greedy?\n",
    "\n",
    "In Parts 1 and 2, we used **greedy decoding**:\n",
    "```python\n",
    "word = argmax(probabilities)  # Pick highest probability\n",
    "```\n",
    "\n",
    "Pick the best word NOW, never look back.\n",
    "\n",
    "### Why Greedy Fails\n",
    "\n",
    "Sometimes the best word NOW leads to a BAD translation LATER!\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Greedy Approach:\n",
    "Step 1: \"Le\" (90%) vs \"La\" (10%) → Pick \"Le\"\n",
    "Step 2: \"chat\" (70%)  [total: 90% × 70% = 63%]\n",
    "Step 3: \"assis\" (40%) [total: 63% × 40% = 25%]\n",
    "Result: \"Le chat assis\" (25% overall)\n",
    "\n",
    "Better Path:\n",
    "Step 1: \"La\" (10%)    → Pick \"La\"\n",
    "Step 2: \"petite\" (80%) [total: 10% × 80% = 8%]\n",
    "Step 3: \"chatte\" (90%) [total: 8% × 90% = 7.2%]\n",
    "Result: \"La petite chatte\" (7.2% overall but more natural!)\n",
    "```\n",
    "\n",
    "**Key Insight:** Lower probability first step can lead to better overall sentence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Beam Search: Keeping Multiple Options\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Instead of keeping ONLY the best path (greedy), keep the **top k best paths**!\n",
    "\n",
    "**Beam width k = 2:**\n",
    "```\n",
    "Step 1: Generate all first words\n",
    "  \"Le\" (90%), \"La\" (10%), \"Un\" (5%)\n",
    "  Keep top 2: \"Le\" (90%), \"La\" (10%)\n",
    "\n",
    "Step 2: For each, generate all second words\n",
    "  \"Le\" + \"chat\" (63%), \"Le\" + \"chien\" (18%)\n",
    "  \"La\" + \"petite\" (8%), \"La\" + \"grande\" (6%)\n",
    "  Keep top 2: \"Le chat\" (63%), \"Le chien\" (18%)\n",
    "\n",
    "Step 3: Continue...\n",
    "```\n",
    "\n",
    "### Let's Implement It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(beam_width=3, max_length=4):\n",
    "    \"\"\"\n",
    "    Simple beam search implementation\n",
    "    \"\"\"\n",
    "    vocab = ['le', 'la', 'chat', 'chien', 'assis', 'court', '<END>']\n",
    "    \n",
    "    # Start with empty sequence\n",
    "    beams = [{'words': [], 'score': 1.0}]\n",
    "    \n",
    "    print(f\"Beam Search (width={beam_width})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        print(f\"\\nStep {step+1}:\")\n",
    "        print(f\"Current beams: {len(beams)}\")\n",
    "        \n",
    "        # Show current beams\n",
    "        for i, beam in enumerate(beams):\n",
    "            words_str = ' '.join(beam['words']) if beam['words'] else '[empty]'\n",
    "            print(f\"  Beam {i+1}: '{words_str}' (score: {beam['score']:.3f})\")\n",
    "        \n",
    "        # Expand all beams\n",
    "        candidates = []\n",
    "        for beam in beams:\n",
    "            # Generate next word probabilities (simplified)\n",
    "            probs = np.random.dirichlet(np.ones(len(vocab))) * 0.5 + 0.1\n",
    "            probs = probs / probs.sum()\n",
    "            \n",
    "            # Create candidates\n",
    "            for word, prob in zip(vocab, probs):\n",
    "                new_beam = {\n",
    "                    'words': beam['words'] + [word],\n",
    "                    'score': beam['score'] * prob\n",
    "                }\n",
    "                candidates.append(new_beam)\n",
    "        \n",
    "        print(f\"  Generated {len(candidates)} candidates\")\n",
    "        \n",
    "        # Keep top k\n",
    "        candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "        beams = candidates[:beam_width]\n",
    "        \n",
    "        # Check if done\n",
    "        if all(b['words'] and b['words'][-1] == '<END>' for b in beams):\n",
    "            print(\"  All beams reached <END>\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS:\")\n",
    "    for i, beam in enumerate(beams):\n",
    "        words = [w for w in beam['words'] if w != '<END>']\n",
    "        print(f\"  {i+1}. '{' '.join(words)}' (score: {beam['score']:.4f})\")\n",
    "    \n",
    "    return beams[0]['words']\n",
    "\n",
    "# Try different beam widths\n",
    "result = beam_search(beam_width=3, max_length=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Beam Search Visualization\n",
    "\n",
    "Let's visualize how beam search explores the search space:\n",
    "\n",
    "```\n",
    "              START\n",
    "             /  |  \\\n",
    "           /    |    \\\n",
    "         Le    La    Un\n",
    "        (90%)  (10%) (5%) ← PRUNE Un!\n",
    "         |      |\n",
    "      -------  ------\n",
    "     /       \\/      \\\n",
    "  chat  chien petite grande\n",
    "  (63%) (18%)  (8%)  (6%)  ← Keep top 2\n",
    "    |      |\n",
    "   BEST  runner-up\n",
    "```\n",
    "\n",
    "**Text representation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_beam_text():\n",
    "    \"\"\"\n",
    "    Show beam search as a text tree\n",
    "    \"\"\"\n",
    "    print(\"\\nBeam Search Tree (k=2):\")\n",
    "    print(\"\\n\" + \"  \"*0 + \"START\")\n",
    "    print(\"  \"*0 + \"│\")\n",
    "    print(\"  \"*0 + \"├─ Le (0.90) ✓ KEEP\")\n",
    "    print(\"  \"*0 + \"├─ La (0.10) ✓ KEEP\")\n",
    "    print(\"  \"*0 + \"└─ Un (0.05) ✗ PRUNE\")\n",
    "    print()\n",
    "    print(\"  \"*1 + \"Le:\")\n",
    "    print(\"  \"*1 + \"├─ chat (0.63) ✓ KEEP\")\n",
    "    print(\"  \"*1 + \"└─ chien (0.18) ✓ KEEP\")\n",
    "    print()\n",
    "    print(\"  \"*1 + \"La:\")\n",
    "    print(\"  \"*1 + \"├─ petite (0.08) ✗ PRUNE\")\n",
    "    print(\"  \"*1 + \"└─ grande (0.06) ✗ PRUNE\")\n",
    "    print()\n",
    "    print(\"  \"*2 + \"Best: Le chat (0.63)\")\n",
    "    print()\n",
    "    print(\"Key:\")\n",
    "    print(\"  ✓ = Kept in beam\")\n",
    "    print(\"  ✗ = Pruned (score too low)\")\n",
    "\n",
    "visualize_beam_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Search Strategies\n",
    "\n",
    "Let's compare different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quality and speed\n",
    "strategies = ['Greedy\\n(k=1)', 'Beam\\n(k=3)', 'Beam\\n(k=5)', 'Beam\\n(k=10)']\n",
    "quality = [65, 82, 88, 91]\n",
    "speed = [100, 75, 55, 30]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Quality\n",
    "colors = ['#FF6B6B', '#FFD93D', '#6BCB77', '#4D96FF']\n",
    "ax1.bar(range(len(strategies)), quality, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Translation Quality (%)', fontweight='bold')\n",
    "ax1.set_title('Quality Comparison', fontweight='bold')\n",
    "ax1.set_xticks(range(len(strategies)))\n",
    "ax1.set_xticklabels(strategies)\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for i, (bar, val) in enumerate(zip(ax1.patches, quality)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 2, f'{val}%',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Speed\n",
    "ax2.bar(range(len(strategies)), speed, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Relative Speed', fontweight='bold')\n",
    "ax2.set_title('Speed Comparison', fontweight='bold')\n",
    "ax2.set_xticks(range(len(strategies)))\n",
    "ax2.set_xticklabels(strategies)\n",
    "ax2.set_ylim(0, 110)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for i, (bar, val) in enumerate(zip(ax2.patches, speed)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val + 2, f'{val}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTradeoffs:\")\n",
    "print(\"- Greedy (k=1): Fastest but lowest quality\")\n",
    "print(\"- Beam (k=3-5): Best balance (MOST COMMON)\")\n",
    "print(\"- Beam (k=10): Higher quality but much slower\")\n",
    "print(\"\\nGoogle Translate uses k≈5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modern Translation Systems\n",
    "\n",
    "### Evolution Timeline\n",
    "\n",
    "```\n",
    "2000s: Rule-Based\n",
    "  - Hand-written grammar rules\n",
    "  - Dictionary lookups\n",
    "  - Quality: 30%\n",
    "\n",
    "2010-2016: Statistical (SMT)\n",
    "  - Learn patterns from data\n",
    "  - Phrase-based\n",
    "  - Quality: 45-55%\n",
    "\n",
    "2016-2017: Neural + Attention (NMT)\n",
    "  - What you learned in this course!\n",
    "  - Quality jump: 75%\n",
    "  - Google Translate switched in 2016\n",
    "\n",
    "2017-now: Transformers\n",
    "  - Self-attention everywhere\n",
    "  - Parallel processing\n",
    "  - Quality: 85-95%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evolution\n",
    "years = [2005, 2010, 2015, 2016, 2018, 2020, 2024]\n",
    "quality = [30, 45, 55, 75, 85, 92, 95]\n",
    "labels = ['Rule\\nBased', 'SMT', 'SMT', 'NMT\\n+Attn', 'Trans\\nformer', 'GPT-3', 'GPT-4']\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "plt.plot(years, quality, 'o-', linewidth=3, markersize=12, color='#4ECDC4')\n",
    "\n",
    "# Annotations\n",
    "for i, (year, qual, label) in enumerate(zip(years, quality, labels)):\n",
    "    if i in [0, 3, 6]:  # Key milestones\n",
    "        plt.annotate(label, xy=(year, qual), \n",
    "                   xytext=(year, qual + 8 if i % 2 == 0 else qual - 12),\n",
    "                   fontsize=10, fontweight='bold', ha='center',\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Highlight breakthrough\n",
    "plt.axvspan(2015.5, 2017, alpha=0.2, color='green')\n",
    "plt.text(2016.2, 65, 'Attention\\nRevolution', ha='center', \n",
    "        fontsize=11, fontweight='bold', color='darkgreen')\n",
    "\n",
    "plt.xlabel('Year', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Translation Quality (%)', fontweight='bold', fontsize=12)\n",
    "plt.title('Evolution of Machine Translation', fontweight='bold', fontsize=14)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Milestones:\")\n",
    "print(\"- 2016: Google switches to NMT (20% quality jump!)\")\n",
    "print(\"- 2017: Transformers paper ('Attention Is All You Need')\")\n",
    "print(\"- 2020+: GPT models achieve near-human quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connection to ChatGPT\n",
    "\n",
    "### The Surprising Truth\n",
    "\n",
    "ChatGPT uses the SAME core ideas you learned:\n",
    "\n",
    "```\n",
    "Translation (2016):          ChatGPT (2024):\n",
    "- Encoder-Decoder            - Decoder-only\n",
    "- Attention in decoder       - Self-attention everywhere\n",
    "- Beam search (k=5)          - Sampling + temperature\n",
    "- 200M parameters            - 175B+ parameters\n",
    "- Trained on sentences       - Trained on internet\n",
    "```\n",
    "\n",
    "**Same principles, bigger scale!**\n",
    "\n",
    "Your knowledge powers:\n",
    "- Google Translate\n",
    "- ChatGPT / GPT-4\n",
    "- BERT, T5, Claude\n",
    "- Image captioning\n",
    "- Speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary: Your Learning Journey\n",
    "\n",
    "### Part 1: Basic Seq2Seq\n",
    "- Encoder compresses input → context vector\n",
    "- Decoder expands context → output\n",
    "- Problem: **Bottleneck**\n",
    "\n",
    "### Part 2: Attention\n",
    "- Decoder looks at ALL encoder states\n",
    "- Custom context for each output word\n",
    "- Solution to bottleneck!\n",
    "\n",
    "### Part 3: Beam Search\n",
    "- Keep multiple paths (not just best)\n",
    "- Quality vs speed tradeoff\n",
    "- Used in real systems\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "You now understand **the foundation of modern NLP**:\n",
    "\n",
    "```\n",
    "Week 1-3: Foundations (N-grams, RNNs)\n",
    "    ↓\n",
    "Week 4: Seq2Seq + Attention ← YOU ARE HERE!\n",
    "    ↓\n",
    "Week 5: Transformers (next week!)\n",
    "    ↓\n",
    "Week 6+: Pre-trained models (BERT, GPT)\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Week 5: Transformers**\n",
    "- Self-attention (attention on steroids!)\n",
    "- Parallel processing\n",
    "- The architecture behind GPT and BERT\n",
    "\n",
    "---\n",
    "\n",
    "## Try It Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Try different beam widths\n",
    "print(\"Comparing different beam widths:\\n\")\n",
    "\n",
    "for k in [1, 2, 5]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Beam width k={k}:\")\n",
    "    result = beam_search(beam_width=k, max_length=3)\n",
    "    print(f\"Best: {' '.join([w for w in result if w != '<END>'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- k=1 (greedy): Fastest, might miss better paths\")\n",
    "print(\"- k=2: Some exploration\")\n",
    "print(\"- k=5: More exploration, better quality\")\n",
    "print(\"\\nQuestion: What happens with k=10? Try it above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the Week 4 lab series!\n",
    "\n",
    "### You Now Understand:\n",
    "- How neural translation works\n",
    "- Why attention is crucial\n",
    "- How to find better translations\n",
    "- The foundation of modern AI\n",
    "\n",
    "### Keep Learning!\n",
    "\n",
    "Next week: **Transformers** - the architecture that changed everything."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
