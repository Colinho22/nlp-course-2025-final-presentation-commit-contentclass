% Neural Networks Primer: Teaching Machines to See Patterns
% A Problem-Solving Journey from 1950s to Today
% Enhanced version with additional historical and modern content
\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{seahorse}

% Color definitions
\definecolor{mainGray}{RGB}{64,64,64}
\definecolor{accentGray}{RGB}{180,180,180}
\definecolor{lightGray}{RGB}{240,240,240}

\setbeamercolor{structure}{fg=mainGray}
\setbeamercolor{normal text}{fg=mainGray}
\setbeamertemplate{navigation symbols}{}

% Commands
\newcommand{\highlight}[1]{{\color{red}#1}}
\newcommand{\secondary}[1]{{\color{accentGray}#1}}

\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}

\title{Teaching Machines to See Patterns}
\subtitle{A Neural Networks Primer: Why We Needed Each Piece of the Puzzle}
\author{NLP Course 2025}
\date{}

\begin{document}

% Title Slide
\begin{frame}
\titlepage
\vfill
\begin{center}
\secondary{\small From the 1950s mail sorting crisis to ChatGPT: How humanity taught machines to think}
\end{center}
\end{frame}

% Act I: The Problem That Started Everything
\begin{frame}{Act I: The Problem That Started Everything}
\begin{center}
{\Large \textbf{1950s: The Mail Sorting Crisis}}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Challenge:}
\begin{itemize}
\item 150 million letters per day
\item Hand-written addresses
\item Human sorters: slow, expensive, error-prone
\item Traditional programming: useless
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Traditional Code Failed:}
\begin{itemize}
\item Can't write rules for every handwriting style
\item Too many variations of each letter
\item Context matters: "I" vs "l" vs "1"
\item This wasn't computation—it was \highlight{pattern recognition}
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This problem would take 40 years to solve properly}
\end{frame}

% Why Rules Don't Work
\begin{frame}[fragile]{Why Can't We Just Write Rules?}
\begin{center}
\textbf{Problem: Recognize the Letter "A"}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Traditional Approach (Failed):}
\begin{lstlisting}[basicstyle=\tiny]
if (has_triangle_top AND
    has_horizontal_bar AND
    two_diagonal_lines) {
  return "A"
}
\end{lstlisting}
\secondary{\small But what about...}
\begin{itemize}
\item Handwritten A's?
\item Different fonts?
\item Rotated A's?
\item Partial A's?
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/various_a_styles.pdf}
\end{center}
\secondary{\small Just for the letter "A", we'd need thousands of rules!}
\end{columns}
\vfill
\secondary{\footnotesize The breakthrough: What if machines could learn patterns like children do?}
\end{frame}

% NEW: McCulloch-Pitts Historical Context
\begin{frame}{1943: The First Spark - McCulloch \& Pitts}
\begin{center}
\textbf{The Birth of Computational Neuroscience}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Revolutionary Paper:}
\begin{itemize}
\item "A Logical Calculus of Ideas Immanent in Nervous Activity"
\item First mathematical model of neurons
\item Proved: Networks can compute ANY logical function
\item Inspired von Neumann's computer architecture
\end{itemize}

\textbf{Key Insight:}
\begin{itemize}
\item Neurons = Logic gates
\item Brain = Computing machine
\item Thinking = Computation
\end{itemize}

\column{0.48\textwidth}
\textbf{The Model:}
\begin{itemize}
\item Binary neurons (0 or 1)
\item Threshold activation
\item Fixed connections
\item No learning yet!
\end{itemize}

\textbf{Historical Impact:}
\begin{itemize}
\item Founded field of neural networks
\item Influenced cybernetics movement
\item Set stage for AI research
\item "The brain is a computer" metaphor
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize 14 years later, Rosenblatt would add the missing piece: learning}
\end{frame}

% 1957: The First Attempt
\begin{frame}{1957: The First Learning Machine - The Perceptron}
\begin{center}
\textbf{Frank Rosenblatt's Radical Idea: Neurons That Learn}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Beyond McCulloch-Pitts:}
\begin{itemize}
\item Adjustable weights (not fixed!)
\item Learning from mistakes
\item Physical machine built (Mark I)
\item Could recognize simple patterns
\end{itemize}

\textbf{The Hardware:}
\begin{itemize}
\item 400 photocells (20×20 "retina")
\item 512 motor-driven potentiometers
\item Weights adjusted by electric motors
\item Took 5 minutes to learn patterns
\end{itemize}

\column{0.48\textwidth}
\textbf{Mathematical Model:}
\begin{itemize}
\item Inputs: $x_1, x_2, ..., x_n$
\item Weights: $w_1, w_2, ..., w_n$
\item Sum: $z = \sum_{i=1}^{n} w_i x_i + b$
\item Output: $y = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$
\end{itemize}

\textbf{Learning Rule:}
If wrong: $w_i = w_i + \eta \cdot error \cdot x_i$
\end{columns}
\vfill
\secondary{\footnotesize The New York Times, 1958: "The Navy revealed the embryo of an electronic computer that will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."}
\end{frame}

% The Math Behind It (Simple)
\begin{frame}{Making It Concrete: Teaching OR Logic}
\begin{center}
\textbf{Problem: Learn OR function (output 1 if ANY input is 1)}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Training Data:}
\begin{center}
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & Output \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{tabular}
\end{center}

\textbf{The Perceptron:}
\begin{align*}
z &= w_1 \cdot x_1 + w_2 \cdot x_2 + b \\
\text{output} &= \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}
\end{align*}

\column{0.5\textwidth}
\textbf{Learning Process:}
\begin{enumerate}
\item Start with random weights
\item For each example:
   \begin{itemize}
   \item Calculate output
   \item If wrong: adjust weights
   \item If correct: keep weights
   \end{itemize}
\item Repeat until all correct
\end{enumerate}

\textbf{Final Solution:}
$w_1 = 1$, $w_2 = 1$, $b = -0.5$
\end{columns}
\vfill
\secondary{\footnotesize Success! But this was just the beginning...}
\end{frame}

% Notation Explained
\begin{frame}{Understanding the Notation}
\begin{center}
\textbf{Breaking Down the Math Symbols}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Inputs and Weights:}
\begin{itemize}
\item $x_i$ = input value (what we see)
\item $w_i$ = weight (importance/strength)
\item $b$ = bias (threshold adjuster)
\end{itemize}

\textbf{The Computation:}
$$z = \sum_{i=1}^{n} w_i x_i + b$$

This means:
\begin{itemize}
\item Multiply each input by its weight
\item Add them all up
\item Add the bias
\end{itemize}

\column{0.48\textwidth}
\textbf{Real Example:}
\begin{center}
Should I go outside? \\[3mm]
\begin{tabular}{lcc}
Factor & Value & Weight \\
\hline
Sunny? & 1 & +2 \\
Raining? & 0 & -3 \\
Weekend? & 1 & +1 \\
\hline
\end{tabular}
\end{center}
$$z = (1 \times 2) + (0 \times -3) + (1 \times 1) = 3$$
$$\text{Decision: } z > 0 \text{, so YES!}$$
\end{columns}
\vfill
\secondary{\footnotesize This simple math would evolve into deep learning}
\end{frame}

% 1969: The Crisis
\begin{frame}{1969: The Crisis - XOR Problem}
\begin{center}
\textbf{Minsky \& Papert's Devastating Discovery}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{XOR (Exclusive OR):}
\begin{center}
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & Output \\
\hline
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\end{tabular}
\end{center}

\textbf{The Problem:}
\begin{itemize}
\item Can't draw a single line to separate
\item Perceptron only learns linear boundaries
\item Real-world problems are non-linear!
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/xor_visualization.pdf}
\end{center}
\textbf{Impact:}
\begin{itemize}
\item Funding dried up
\item "AI Winter" begins
\item Neural networks abandoned
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize The field would be dormant for over a decade...}
\end{frame}

% Act II: The Journey
\begin{frame}{Act II: The Journey Back}
\begin{center}
{\Large \textbf{1980s: The Hidden Layer Revolution}}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Insight:}
\begin{itemize}
\item Stack multiple layers!
\item First layer: detect simple features
\item Hidden layer: combine features
\item Output layer: final decision
\end{itemize}

\textbf{Solving XOR:}
\begin{itemize}
\item Hidden neuron 1: Is it (0,1)?
\item Hidden neuron 2: Is it (1,0)?
\item Output: OR of hidden neurons
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/multilayer_network.pdf}
\end{center}
\textbf{New Architecture:}
\begin{itemize}
\item Input layer: raw data
\item Hidden layer(s): feature extraction
\item Output layer: final classification
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize But how do we train multiple layers?}
\end{frame}

% Backpropagation
\begin{frame}{1986: Backpropagation - Teaching Networks to Learn}
\begin{center}
\textbf{The Credit Assignment Problem: Who's to Blame?}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Challenge:}
\begin{itemize}
\item Network makes error at output
\item Many neurons contributed
\item Which weights should change?
\item By how much?
\end{itemize}

\textbf{The Solution: Chain Rule}
\begin{itemize}
\item Calculate error at output
\item Propagate error backwards
\item Each layer gets its "share of blame"
\item Adjust weights proportionally
\end{itemize}

\column{0.48\textwidth}
\textbf{Mathematical Insight:}
$$\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial out_j} \cdot \frac{\partial out_j}{\partial net_j} \cdot \frac{\partial net_j}{\partial w_{ij}}$$

\textbf{In Simple Terms:}
\begin{enumerate}
\item How wrong were we? (Error)
\item How sensitive is error to this weight?
\item Adjust weight in opposite direction
\item Repeat for all weights, back to front
\end{enumerate}
\end{columns}
\vfill
\secondary{\footnotesize This algorithm is still the foundation of all deep learning today}
\end{frame}

% NEW: NetTalk Success Story
\begin{frame}{1987: NetTalk - Networks Learn to Speak}
\begin{center}
\textbf{Sejnowski \& Rosenberg's Speaking Network}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Challenge:}
\begin{itemize}
\item English pronunciation is irregular
\item "though" vs "through" vs "tough"
\item Rule-based systems failed
\item Can a network learn from examples?
\end{itemize}

\textbf{The Architecture:}
\begin{itemize}
\item Input: 7 letters (context window)
\item Hidden: 80 neurons
\item Output: 26 phonemes
\item 18,000 total weights
\end{itemize}

\column{0.48\textwidth}
\textbf{The Results:}
\begin{itemize}
\item Started: Random babbling
\item After 10 epochs: Consonants/vowels
\item After 30 epochs: Simple words
\item After 50 epochs: 95\% correct!
\end{itemize}

\textbf{Why It Mattered:}
\begin{itemize}
\item Proved backprop works on real problems
\item Learned complex, irregular mappings
\item No rules programmed!
\item Sounded like a child learning to read
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize The network literally learned English pronunciation overnight}
\end{frame}

% NEW: Universal Approximation Theorem
\begin{frame}{1989: Universal Approximation - The Mathematical Foundation}
\begin{center}
\textbf{Cybenko's Theorem: Networks Can Learn ANY Function}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Theorem:}
"A feedforward network with:
\begin{itemize}
\item One hidden layer
\item Finite neurons
\item Sigmoid activation
\end{itemize}
can approximate ANY continuous function to arbitrary accuracy"

\textbf{What This Means:}
\begin{itemize}
\item Neural networks are universal
\item Can solve any pattern recognition
\item Just need enough neurons
\item Mathematics guarantees it!
\end{itemize}

\column{0.48\textwidth}
\textbf{The Catch:}
\begin{itemize}
\item Doesn't say HOW MANY neurons
\item Doesn't say HOW to find weights
\item Might need exponentially many
\item Training might take forever
\end{itemize}

\textbf{Historical Impact:}
\begin{itemize}
\item Ended theoretical doubts
\item Justified deep learning research
\item Shifted focus to practical training
\item "We know it's possible, now make it work"
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This theorem convinced skeptics that neural networks were worth pursuing}
\end{frame}

% Activation Functions
\begin{frame}{Why Linear Doesn't Work: Activation Functions}
\begin{center}
\textbf{The Need for Non-Linearity}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Problem with Linear:}
\begin{itemize}
\item Stack of linear layers = still linear!
\item $f(g(x)) = (wx + b_1)w' + b_2 = w'wx + ...$
\item Can't learn complex patterns
\end{itemize}

\textbf{Solution: Activation Functions}
\begin{itemize}
\item Add non-linearity after each layer
\item Allows learning complex boundaries
\item Different functions for different needs
\end{itemize}

\column{0.48\textwidth}
\textbf{Common Activation Functions:}
\begin{itemize}
\item \textbf{Sigmoid:} $\sigma(x) = \frac{1}{1 + e^{-x}}$
   \begin{itemize}
   \item Smooth, outputs 0-1
   \item Good for probabilities
   \end{itemize}
\item \textbf{ReLU:} $f(x) = \max(0, x)$
   \begin{itemize}
   \item Simple, fast
   \item Solves vanishing gradient
   \end{itemize}
\item \textbf{Tanh:} $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
   \begin{itemize}
   \item Outputs -1 to 1
   \item Zero-centered
   \end{itemize}
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize ReLU's simplicity revolutionized deep learning in 2011}
\end{frame}

% Simple 2D Example
\begin{frame}{Visualizing Learning: 2D Classification}
\begin{center}
\textbf{Teaching a Network to Separate Red from Blue Points}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Setup:}
\begin{itemize}
\item Input: (x, y) coordinates
\item Output: Red or Blue class
\item Network: 2 → 4 → 2 neurons
\end{itemize}

\textbf{Training Process:}
\begin{enumerate}
\item Epoch 1: Random boundary
\item Epoch 10: Rough separation
\item Epoch 50: Good boundary
\item Epoch 100: Perfect fit
\end{enumerate}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/2d_classification_evolution.pdf}
\end{center}
\textbf{What Each Layer Learns:}
\begin{itemize}
\item Layer 1: Simple boundaries
\item Hidden: Combine boundaries
\item Output: Final decision
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This same principle scales to millions of parameters}
\end{frame}

% Act III: The Breakthrough
\begin{frame}{Act III: The Breakthrough Years}
\begin{center}
{\Large \textbf{1998-2012: From Digits to ImageNet}}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{1998 - LeNet: First Success}
\begin{itemize}
\item Yann LeCun's CNN for digits
\item 32×32 pixels → 10 classes
\item 60,000 parameters
\item Banks adopt for check reading
\end{itemize}

\textbf{Key Innovation: Convolutions}
\begin{itemize}
\item Share weights across image
\item Detect features anywhere
\item Build complexity layer by layer
\end{itemize}

\column{0.48\textwidth}
\textbf{2012 - AlexNet: The Revolution}
\begin{itemize}
\item 1000 ImageNet classes
\item 60 million parameters
\item GPUs enable training
\item Error rate: 26\% → 16\%
\end{itemize}

\textbf{What Changed:}
\begin{itemize}
\item Big Data (millions of images)
\item GPU computing (100x faster)
\item ReLU activation
\item Dropout regularization
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This victory ended the second AI winter permanently}
\end{frame}

% Understanding Convolutions
\begin{frame}{The Convolution Innovation: See Like Humans Do}
\begin{center}
\textbf{How We Actually Recognize Objects}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Human Vision Process:}
\begin{enumerate}
\item Detect edges
\item Find shapes
\item Identify parts
\item Recognize object
\end{enumerate}

\textbf{CNN Mimics This:}
\begin{itemize}
\item Layer 1: Edge detectors
\item Layer 2: Corner/curve detectors
\item Layer 3: Part detectors
\item Layer 4: Object detectors
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/cnn_feature_hierarchy.pdf}
\end{center}
\textbf{Key Insight:}
\begin{itemize}
\item A "wheel detector" works anywhere in image
\item Share the same detector across positions
\item Reduces parameters dramatically
\item Makes network translation-invariant
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This is why CNNs dominate computer vision}
\end{frame}

% The Mathematics of Learning
\begin{frame}{The Mathematics of Learning: Gradient Descent}
\begin{center}
\textbf{Finding the Best Weights: Like Hiking Down a Mountain}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Optimization Problem:}
\begin{itemize}
\item Millions of weights to adjust
\item Each affects the error
\item Need to find best combination
\end{itemize}

\textbf{Gradient Descent:}
\begin{enumerate}
\item Calculate error (loss)
\item Find slope (gradient) for each weight
\item Step downhill: $w = w - \alpha \cdot \nabla L$
\item Repeat until bottom
\end{enumerate}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/gradient_descent.pdf}
\end{center}
\textbf{Learning Rate ($\alpha$):}
\begin{itemize}
\item Too small: takes forever
\item Too large: overshoot minimum
\item Just right: smooth convergence
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize Modern optimizers like Adam adapt the learning rate automatically}
\end{frame}

% Types of Learning
\begin{frame}{Types of Learning: Different Problems, Different Approaches}
\begin{columns}
\column{0.48\textwidth}
\textbf{Supervised Learning:}
\begin{itemize}
\item Have input-output pairs
\item Learn mapping function
\item Examples: Classification, Regression
\end{itemize}

\textbf{Unsupervised Learning:}
\begin{itemize}
\item Only have inputs
\item Find patterns/structure
\item Examples: Clustering, Compression
\end{itemize}

\column{0.48\textwidth}
\textbf{Reinforcement Learning:}
\begin{itemize}
\item Learn through trial/error
\item Maximize reward signal
\item Examples: Games, Robotics
\end{itemize}

\textbf{Self-Supervised (Modern):}
\begin{itemize}
\item Create labels from data itself
\item Predict next word, masked words
\item Examples: GPT, BERT
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize Self-supervised learning powers all modern language models}
\end{frame}

% Overfitting Problem
\begin{frame}{The Overfitting Problem: When Learning Goes Too Far}
\begin{center}
\textbf{Memorization vs. Understanding}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Problem:}
\begin{itemize}
\item Network memorizes training data
\item Fails on new, unseen data
\item Like student memorizing answers
\end{itemize}

\textbf{Signs of Overfitting:}
\begin{itemize}
\item Training accuracy: 99\%
\item Test accuracy: 60\%
\item Complex decision boundaries
\item High variance
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/overfitting_visualization.pdf}
\end{center}
\textbf{Solutions:}
\begin{itemize}
\item \textbf{More data:} Can't memorize everything
\item \textbf{Dropout:} Randomly disable neurons
\item \textbf{Regularization:} Penalize complexity
\item \textbf{Early stopping:} Stop before overfitting
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize "With four parameters I can fit an elephant, with five I can make him wiggle his trunk" - von Neumann}
\end{frame}

% Act IV: The Revolution
\begin{frame}{Act IV: The Deep Learning Revolution}
\begin{center}
{\Large \textbf{2014-Present: Networks That Changed the World}}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Depth Revolution:}
\begin{itemize}
\item 2014 - VGGNet: 19 layers
\item 2015 - ResNet: 152 layers
\item 2017 - Transformers: Attention
\item 2020 - GPT-3: 175B parameters
\end{itemize}

\textbf{Why Depth Matters:}
\begin{itemize}
\item Each layer = abstraction level
\item Deep = complex reasoning
\item Hierarchical feature learning
\end{itemize}

\column{0.48\textwidth}
\textbf{Real-World Impact:}
\begin{itemize}
\item \textbf{Vision:} Self-driving cars
\item \textbf{Language:} Google Translate
\item \textbf{Speech:} Siri, Alexa
\item \textbf{Medicine:} Disease diagnosis
\item \textbf{Science:} Protein folding
\end{itemize}

\textbf{The Scale:}
\begin{itemize}
\item Billions of parameters
\item Trained on internet-scale data
\item Months of GPU time
\item Emergent abilities appear
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize We went from recognizing digits to passing the bar exam in 25 years}
\end{frame}

% ResNet Innovation
\begin{frame}{2015: ResNet - The Skip Connection Revolution}
\begin{center}
\textbf{Problem: Networks Couldn't Get Deeper}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Vanishing Gradient:}
\begin{itemize}
\item Gradients multiply through layers
\item Become exponentially small
\item Deep layers stop learning
\item 20 layers was the limit
\end{itemize}

\textbf{The Breakthrough: Skip Connections}
\begin{itemize}
\item Add input directly to output
\item $F(x) + x$ instead of just $F(x)$
\item Gradients flow directly backward
\item Can train 1000+ layers!
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/resnet_skip_connection.pdf}
\end{center}
\textbf{Why It Works:}
\begin{itemize}
\item Learn residual (difference) only
\item Identity mapping is easy default
\item Gradients have direct path
\item Each layer refines previous result
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This simple trick enabled the deep learning revolution}
\end{frame}

% Batch Normalization
\begin{frame}{Batch Normalization: Keeping Networks Stable}
\begin{center}
\textbf{The Internal Covariate Shift Problem}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Issue:}
\begin{itemize}
\item Each layer's input distribution changes
\item As previous layers update
\item Makes learning unstable
\item Requires tiny learning rates
\end{itemize}

\textbf{The Solution:}
\begin{itemize}
\item Normalize inputs to each layer
\item Mean = 0, Variance = 1
\item Learn scale and shift parameters
\item Apply during training and testing
\end{itemize}

\column{0.48\textwidth}
\textbf{BatchNorm Algorithm:}
\begin{align*}
\mu_B &= \frac{1}{m} \sum_{i=1}^{m} x_i \\
\sigma_B^2 &= \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2 \\
\hat{x}_i &= \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{align*}

\textbf{Benefits:}
\begin{itemize}
\item 10x faster training
\item Higher learning rates OK
\item Less sensitive to initialization
\item Acts as regularization
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize Now standard in every deep network}
\end{frame}

% NEW: The Lottery Ticket Hypothesis
\begin{frame}{2019: The Lottery Ticket Hypothesis}
\begin{center}
\textbf{Most Network Weights Don't Matter!}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Discovery:}
\begin{itemize}
\item Networks contain "winning tickets"
\item Subnetworks that train well alone
\item 90-95\% of weights can be removed
\item Performance stays the same!
\end{itemize}

\textbf{The Hypothesis:}
"Dense networks succeed because they contain sparse subnetworks that are capable of training effectively"

\column{0.48\textwidth}
\textbf{Implications:}
\begin{itemize}
\item We massively overparameterize
\item Training finds the needle in haystack
\item Future: Train small from start?
\item Mobile deployment possible
\end{itemize}

\textbf{Why It Matters:}
\begin{itemize}
\item Explains why big networks train better
\item Pruning after training works
\item Efficiency revolution starting
\item Changes how we think about learning
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize A 1 billion parameter model might only need 50 million}
\end{frame}

% NEW: Inductive Biases
\begin{frame}{Inductive Biases: Building in Assumptions}
\begin{center}
\textbf{The Right Architecture for the Right Problem}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{What Are Inductive Biases?}
\begin{itemize}
\item Assumptions built into architecture
\item Guide learning toward solutions
\item Trade flexibility for efficiency
\item "Priors" about the problem
\end{itemize}

\textbf{Examples:}
\begin{itemize}
\item \textbf{CNN:} Spatial locality matters
\item \textbf{RNN:} Order/time matters
\item \textbf{GNN:} Graph structure matters
\item \textbf{Transformer:} All positions can interact
\end{itemize}

\column{0.48\textwidth}
\textbf{Why They Matter:}
\begin{itemize}
\item Reduce search space
\item Faster convergence
\item Better generalization
\item Less data needed
\end{itemize}

\textbf{The Tradeoff:}
\begin{itemize}
\item Right bias = 10x better
\item Wrong bias = 10x worse
\item General architectures = safe but slow
\item Specialized = fast but limited
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize Choosing the right inductive bias is still an art}
\end{frame}

% NEW: Emergent Abilities
\begin{frame}{Emergent Abilities: When Scale Creates Intelligence}
\begin{center}
\textbf{Capabilities That Appear Suddenly with Scale}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Phenomenon:}
\begin{itemize}
\item Small models: Can't do task at all
\item Medium models: Still can't
\item Large models: Suddenly can!
\item No gradual improvement
\end{itemize}

\textbf{Examples:}
\begin{itemize}
\item 3-digit arithmetic (>10B params)
\item Chain-of-thought reasoning (>50B)
\item Code generation (>20B)
\item Multilingual translation (>100B)
\end{itemize}

\column{0.48\textwidth}
\textbf{Why It Happens:}
\begin{itemize}
\item Complex patterns need capacity
\item Phase transitions in learning
\item Composition of simpler abilities
\item "Grokking" - sudden understanding
\end{itemize}

\textbf{Implications:}
\begin{itemize}
\item We can't predict what's next
\item Scaling might unlock AGI
\item Or hit fundamental limits
\item Active area of research
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize GPT-3 showed abilities nobody expected or programmed}
\end{frame}

% Different Architectures
\begin{frame}{Neural Network Architectures: Right Tool for Right Job}
\begin{columns}
\column{0.48\textwidth}
\textbf{Feedforward Networks:}
\begin{itemize}
\item Information flows forward only
\item Fixed-size input and output
\item Good for: Classification, regression
\end{itemize}

\textbf{Convolutional (CNN):}
\begin{itemize}
\item Spatial feature detection
\item Translation invariance
\item Good for: Images, video
\end{itemize}

\column{0.48\textwidth}
\textbf{Recurrent (RNN):}
\begin{itemize}
\item Process sequences
\item Maintain memory/state
\item Good for: Text, time-series
\end{itemize}

\textbf{Transformer:}
\begin{itemize}
\item Attention mechanism
\item Parallel processing
\item Good for: Language, everything else
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize Each architecture encodes different assumptions about the data}
\end{frame}

% Modern Training Techniques
\begin{frame}{Modern Training: Standing on Shoulders of Giants}
\begin{columns}
\column{0.48\textwidth}
\textbf{Transfer Learning:}
\begin{itemize}
\item Start with pre-trained network
\item Fine-tune on your task
\item 100x less data needed
\item Days → Hours training
\end{itemize}

\textbf{Data Augmentation:}
\begin{itemize}
\item Create variations of training data
\item Rotations, crops, color shifts
\item Prevents overfitting
\item Free performance boost
\end{itemize}

\column{0.48\textwidth}
\textbf{Advanced Optimizers:}
\begin{itemize}
\item \textbf{SGD:} Basic gradient descent
\item \textbf{Momentum:} Remember past gradients
\item \textbf{Adam:} Adaptive learning rates
\item \textbf{AdamW:} With weight decay
\end{itemize}

\textbf{Mixed Precision:}
\begin{itemize}
\item Use 16-bit floats where possible
\item Keep 32-bit for critical ops
\item 2-3x speedup
\item Same accuracy
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize These techniques make deep learning practical for everyone}
\end{frame}

% Why Now?
\begin{frame}{Why Deep Learning Exploded Now: The Perfect Storm}
\begin{columns}
\column{0.48\textwidth}
\textbf{1. Data Explosion:}
\begin{itemize}
\item Internet = infinite training data
\item ImageNet: 14M labeled images
\item Common Crawl: 300TB of text
\item YouTube: 500 hours/minute
\end{itemize}

\textbf{2. Hardware Revolution:}
\begin{itemize}
\item GPUs: 100x faster than CPUs
\item TPUs: Built for neural nets
\item Cloud computing: Rent supercomputers
\item Mobile chips with NPUs
\end{itemize}

\column{0.48\textwidth}
\textbf{3. Algorithm Breakthroughs:}
\begin{itemize}
\item ReLU activation (2011)
\item Batch normalization (2015)
\item Skip connections (2015)
\item Attention mechanism (2017)
\end{itemize}

\textbf{4. Open Source Culture:}
\begin{itemize}
\item TensorFlow, PyTorch free
\item Pre-trained models shared
\item Papers with code
\item Collaborative research
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize The same ideas from 1980s finally had the resources to work}
\end{frame}

% Understanding Scale
\begin{frame}{Understanding Scale: From Perceptron to GPT-4}
\begin{center}
\textbf{The Exponential Growth of Neural Networks}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Parameter Growth:}
\begin{itemize}
\item 1957 Perceptron: 20 weights
\item 1987 NetTalk: 18,000
\item 1998 LeNet: 60,000
\item 2012 AlexNet: 60 million
\item 2018 BERT: 340 million
\item 2020 GPT-3: 175 billion
\item 2023 GPT-4: ~1.8 trillion
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/scale_growth_chart.pdf}
\end{center}
\textbf{What Scale Brings:}
\begin{itemize}
\item Emergent abilities
\item Zero-shot learning
\item Multi-task capability
\item Common sense reasoning
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize Each 10x increase unlocks new capabilities}
\end{frame}

% Practical Implementation
\begin{frame}[fragile]{From Theory to Practice: Your First Network}
\begin{center}
\textbf{Building a Digit Classifier in 10 Lines}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.58\textwidth}
\textbf{PyTorch Implementation:}
\begin{lstlisting}[language=Python,basicstyle=\tiny]
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# Train
model = SimpleNet()
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()
\end{lstlisting}

\column{0.38\textwidth}
\textbf{What This Does:}
\begin{itemize}
\item Input: 28×28 pixel image
\item Hidden: 128 neurons
\item Output: 10 digit classes
\item Activation: ReLU
\item Training: Adam optimizer
\end{itemize}

\textbf{Training Loop:}
\begin{itemize}
\item Forward pass
\item Calculate loss
\item Backward pass
\item Update weights
\item Repeat
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize This simple network achieves 97\% accuracy on MNIST}
\end{frame}

% NEW: Advanced Debugging Techniques
\begin{frame}{Debugging Neural Networks: Detective Work}
\begin{center}
\textbf{When Things Go Wrong (They Always Do)}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Gradient Issues:}
\begin{itemize}
\item \textbf{Exploding:} Gradients → infinity
   \begin{itemize}
   \item Solution: Gradient clipping
   \end{itemize}
\item \textbf{Vanishing:} Gradients → 0
   \begin{itemize}
   \item Solution: Better initialization, ReLU
   \end{itemize}
\item \textbf{Dead ReLU:} Neurons never activate
   \begin{itemize}
   \item Solution: LeakyReLU, smaller learning rate
   \end{itemize}
\end{itemize}

\textbf{Debugging Tools:}
\begin{itemize}
\item TensorBoard: Visualize training
\item Gradient histograms
\item Activation distributions
\item Weight evolution plots
\end{itemize}

\column{0.48\textwidth}
\textbf{Common Failure Modes:}
\begin{itemize}
\item Loss not decreasing: Learning rate
\item Loss NaN: Numerical instability
\item Oscillating loss: LR too high
\item Plateau: Local minimum or LR too small
\end{itemize}

\textbf{Sanity Checks:}
\begin{enumerate}
\item Overfit single batch first
\item Check gradient flow
\item Visualize first layer filters
\item Plot loss curves
\item Test on toy problem
\end{enumerate}
\end{columns}
\vfill
\secondary{\footnotesize "If it's not working, it's always the learning rate" - Andrej Karpathy}
\end{frame}

% Common Pitfalls
\begin{frame}{Common Pitfalls: Learn from Others' Mistakes}
\begin{columns}
\column{0.48\textwidth}
\textbf{Data Problems:}
\begin{itemize}
\item Not enough data
\item Unbalanced classes
\item Data leakage
\item No validation set
\end{itemize}

\textbf{Architecture Issues:}
\begin{itemize}
\item Too deep without skip connections
\item Wrong activation functions
\item Incorrect output layer
\item Bad initialization
\end{itemize}

\column{0.48\textwidth}
\textbf{Training Mistakes:}
\begin{itemize}
\item Learning rate too high/low
\item No normalization
\item Overfitting ignored
\item Wrong loss function
\end{itemize}

\textbf{Debugging Tips:}
\begin{itemize}
\item Start simple, add complexity
\item Overfit single batch first
\item Monitor gradients
\item Visualize predictions
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize "It's not working" usually means one of these issues}
\end{frame}

% The Future
\begin{frame}{The Future: What's Next?}
\begin{columns}
\column{0.48\textwidth}
\textbf{Current Frontiers:}
\begin{itemize}
\item Multimodal models (text+image+audio)
\item Efficient models for phones
\item Neuromorphic hardware
\item Quantum neural networks
\end{itemize}

\textbf{Unsolved Problems:}
\begin{itemize}
\item True reasoning ability
\item Learning from few examples
\item Explaining decisions
\item Energy efficiency
\end{itemize}

\column{0.48\textwidth}
\textbf{Next Breakthroughs?}
\begin{itemize}
\item Models that update continuously
\item Networks that program themselves
\item Biological-digital hybrids
\item AGI (Artificial General Intelligence)?
\end{itemize}

\textbf{Your Role:}
\begin{itemize}
\item This field is 70 years young
\item Major breakthroughs every 2-3 years
\item Anyone can contribute
\item The best is yet to come
\end{itemize}
\end{columns}
\vfill
\secondary{\footnotesize "We're still in the steam engine era of AI" - Geoffrey Hinton}
\end{frame}

% Key Takeaways
\begin{frame}{Key Takeaways: From Mail Sorting to ChatGPT}
\begin{center}
{\Large \textbf{The Journey So Far}}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{Core Concepts:}
\begin{enumerate}
\item \textbf{Neurons:} $y = f(\sum w_i x_i + b)$
\item \textbf{Learning:} Adjust weights to minimize error
\item \textbf{Depth:} Each layer adds abstraction
\item \textbf{Backpropagation:} Distribute error backwards
\item \textbf{Non-linearity:} Enables complex functions
\end{enumerate}

\column{0.48\textwidth}
\textbf{Historical Lessons:}
\begin{enumerate}
\item Every limitation spawned innovation
\item Simple ideas + scale = revolution
\item Biology inspires but doesn't limit
\item Persistence pays (40-year problem!)
\item We're just getting started
\end{enumerate}
\end{columns}
\vspace{5mm}
\begin{center}
\textbf{Remember: Neural networks are just functions that learn from examples}
\end{center}
\vfill
\secondary{\footnotesize Next: RNNs - Teaching networks to remember}
\end{frame}

% Final Slide
\begin{frame}{Your Neural Network Journey Begins}
\begin{center}
{\Large \textbf{From Here to RNNs and Beyond}}
\end{center}
\vspace{10mm}

\textbf{What You Now Understand:}
\begin{itemize}
\item Why traditional programming failed for pattern recognition
\item How neurons compute: inputs → weights → sum → activation → output
\item Why we need multiple layers and non-linearity
\item How networks learn through backpropagation
\item The historical journey from McCulloch-Pitts to GPT-4
\end{itemize}

\vspace{5mm}

\textbf{Next Steps:}
\begin{itemize}
\item \textbf{Week 3:} RNNs - Adding memory for sequences
\item \textbf{Week 4:} Seq2Seq - Teaching translation
\item \textbf{Week 5:} Transformers - The attention revolution
\item \textbf{Week 6:} Pre-trained models - Standing on giants
\end{itemize}

\vfill
\begin{center}
\secondary{\small "The question is not whether machines can think, but whether humans do" - B.F. Skinner}
\end{center}
\end{frame}

\end{document}