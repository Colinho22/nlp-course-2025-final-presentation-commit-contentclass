% Act IV: The Deep Learning Revolution
\begin{frame}{Act IV: The Deep Learning Revolution}
\begin{center}
{\Large \textbf{2014-Present: Networks That Changed the World}}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Depth Revolution:}
\begin{itemize}
\item 2014 - VGGNet: 19 layers
\item 2015 - ResNet: 152 layers
\item 2017 - Transformers: Attention
\item 2020 - GPT-3: 175B parameters
\end{itemize}

\textbf{Why Depth Matters:}
\begin{itemize}
\item Each layer = abstraction level
\item Deep = complex reasoning
\item Hierarchical feature learning
\end{itemize}

\column{0.48\textwidth}
\textbf{Real-World Impact:}
\begin{itemize}
\item \textbf{Vision:} Self-driving cars
\item \textbf{Language:} Google Translate
\item \textbf{Speech:} Siri, Alexa
\item \textbf{Medicine:} Disease diagnosis
\item \textbf{Science:} Protein folding
\end{itemize}

\textbf{The Scale:}
\begin{itemize}
\item Billions of parameters
\item Trained on internet-scale data
\item Months of GPU time
\item Emergent abilities appear
\end{itemize}
\end{columns}
\bottomnote{ We went from recognizing digits to passing the bar exam in 25 years}
\end{frame}

% ResNet Innovation
\begin{frame}{2015: ResNet - The Skip Connection Revolution}
\begin{center}
\textbf{Problem: Networks Couldn't Get Deeper}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Vanishing Gradient:}
\begin{itemize}
\item Gradients multiply through layers
\item Become exponentially small
\item Deep layers stop learning
\item 20 layers was the limit
\end{itemize}

\textbf{The Breakthrough: Skip Connections}
\begin{itemize}
\item Add input directly to output
\item $F(x) + x$ instead of just $F(x)$
\item Gradients flow directly backward
\item Can train 1000+ layers!
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{../figures/resnet_skip_connection.pdf}
\end{center}
\textbf{Why It Works:}
\begin{itemize}
\item Learn residual (difference) only
\item Identity mapping is easy default
\item Gradients have direct path
\item Each layer refines previous result
\end{itemize}
\end{columns}
\bottomnote{ This simple trick enabled the deep learning revolution}
\end{frame}

% Batch Normalization
\begin{frame}{Batch Normalization: Keeping Networks Stable}
\begin{center}
\textbf{The Internal Covariate Shift Problem}
\end{center}
\vspace{2mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Issue:}
\begin{itemize}
\item Each layer's input distribution changes
\item As previous layers update
\item Makes learning unstable
\item Requires tiny learning rates
\end{itemize}

\textbf{The Solution:}
\begin{itemize}
\item Normalize inputs to each layer
\item Mean = 0, Variance = 1
\item Learn scale and shift parameters
\item Apply during training and testing
\end{itemize}

\column{0.48\textwidth}
\textbf{BatchNorm Algorithm:}
\begin{align*}
\mu_B &= \frac{1}{m} \sum_{i=1}^{m} x_i \\
\sigma_B^2 &= \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2 \\
\hat{x}_i &= \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{align*}
\plainmath{1) Find average, 2) Find spread, 3) Normalize to standard range, 4) Scale and shift as needed}

\textbf{Benefits:}
\begin{itemize}
\item 10x faster training
\item Higher learning rates OK
\item Less sensitive to initialization
\item Acts as regularization
\end{itemize}
\end{columns}
\bottomnote{ Now standard in every deep network}
\end{frame}

% NEW: The Lottery Ticket Hypothesis
\begin{frame}{2019: The Lottery Ticket Hypothesis}
\begin{center}
\textbf{Most Network Weights Don't Matter!}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Discovery:}
\begin{itemize}
\item Networks contain "winning tickets"
\item Subnetworks that train well alone
\item 90-95\% of weights can be removed
\item Performance stays the same!
\end{itemize}

\textbf{The Hypothesis:}
"Dense networks succeed because they contain sparse subnetworks that are capable of training effectively"

\column{0.48\textwidth}
\textbf{Implications:}
\begin{itemize}
\item We massively overparameterize
\item Training finds the needle in haystack
\item Future: Train small from start?
\item Mobile deployment possible
\end{itemize}

\textbf{Why It Matters:}
\begin{itemize}
\item Explains why big networks train better
\item Pruning after training works
\item Efficiency revolution starting
\item Changes how we think about learning
\end{itemize}
\end{columns}
\bottomnote{ A 1 billion parameter model might only need 50 million}
\end{frame}

% NEW: Inductive Biases
\begin{frame}{Inductive Biases: Building in Assumptions}
\begin{center}
\textbf{The Right Architecture for the Right Problem}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{What Are Inductive Biases?}
\begin{itemize}
\item Assumptions built into architecture
\item Guide learning toward solutions
\item Trade flexibility for efficiency
\item "Priors" about the problem
\end{itemize}

\textbf{Examples:}
\begin{itemize}
\item \textbf{CNN:} Spatial locality matters
\item \textbf{RNN:} Order/time matters
\item \textbf{GNN:} Graph structure matters
\item \textbf{Transformer:} All positions can interact
\end{itemize}

\column{0.48\textwidth}
\textbf{Why They Matter:}
\begin{itemize}
\item Reduce search space
\item Faster convergence
\item Better generalization
\item Less data needed
\end{itemize}

\textbf{The Tradeoff:}
\begin{itemize}
\item Right bias = 10x better
\item Wrong bias = 10x worse
\item General architectures = safe but slow
\item Specialized = fast but limited
\end{itemize}
\end{columns}
\bottomnote{ Choosing the right inductive bias is still an art}
\end{frame}

% NEW: Emergent Abilities
\begin{frame}{Emergent Abilities: When Scale Creates Intelligence}
\begin{center}
\textbf{Capabilities That Appear Suddenly with Scale}
\end{center}
\vspace{5mm}
\begin{columns}
\column{0.48\textwidth}
\textbf{The Phenomenon:}
\begin{itemize}
\item Small models: Can't do task at all
\item Medium models: Still can't
\item Large models: Suddenly can!
\item No gradual improvement
\end{itemize}

\textbf{Examples:}
\begin{itemize}
\item 3-digit arithmetic (>10B params)
\item Chain-of-thought reasoning (>50B)
\item Code generation (>20B)
\item Multilingual translation (>100B)
\end{itemize}

\column{0.48\textwidth}
\textbf{Why It Happens:}
\begin{itemize}
\item Complex patterns need capacity
\item Phase transitions in learning
\item Composition of simpler abilities
\item "Grokking" - sudden understanding
\end{itemize}

\textbf{Implications:}
\begin{itemize}
\item We can't predict what's next
\item Scaling might unlock AGI
\item Or hit fundamental limits
\item Active area of research
\end{itemize}
\end{columns}
\bottomnote{ GPT-3 showed abilities nobody expected or programmed}
\end{frame}

% NEW: Depth vs Width Comparison
\begin{frame}{Architecture Choices: Deep vs Wide Networks}
\begin{center}
\textbf{The Fundamental Tradeoff in Neural Architecture}
\end{center}
\vspace{2mm}
\begin{columns}
\column{0.55\textwidth}
\includegraphics[width=0.9\textwidth]{../figures/depth_width_comparison.pdf}

\textbf{Deep Networks (Many Layers):}
\begin{itemize}
\item Complex hierarchical features
\item Exponential expressiveness growth
\item Harder to train (vanishing gradients)
\item Better for vision, NLP
\end{itemize}

\textbf{Wide Networks (Many Neurons):}
\begin{itemize}
\item More parallel processing
\item Easier optimization landscape
\item Linear expressiveness growth
\item Better for tabular data
\end{itemize}

\column{0.42\textwidth}
\textbf{The Sweet Spot:}
\begin{itemize}
\item Vision: Deep (100+ layers)
\item Language: Very deep (24-96 layers)
\item Tabular: Wide and shallow (2-4 layers)
\item Time series: Moderate (5-10 layers)
\end{itemize}

\textbf{Modern Insights:}
\begin{itemize}
\item Depth beats width for same parameters
\item Skip connections enable extreme depth
\item Width helps with memorization
\item Depth helps with generalization
\end{itemize}

\textbf{Scaling Laws:}
\begin{itemize}
\item Performance $\propto$ depth$^{0.8}$
\item Performance $\propto$ width$^{0.5}$
\end{itemize}
\end{columns}
\bottomnote{ The depth revolution (2012-2016) showed that deep beats wide for most problems}
\end{frame}

% NEW: Data Scaling Laws
\begin{frame}{Scaling Laws: How Performance Grows with Data}
\begin{center}
\textbf{The Predictable Relationship Between Data, Model Size, and Performance}
\end{center}
\vspace{2mm}
\begin{columns}
\column{0.55\textwidth}
\includegraphics[width=0.82\textwidth]{../figures/data_scaling_laws.pdf}

\textbf{The Chinchilla Law (2022):}
\begin{itemize}
\item Optimal ratio: 20 tokens per parameter
\item 10B model needs 200B tokens
\item Most models are undertrained
\item Data quality matters more than quantity
\end{itemize}

\textbf{Power Law Scaling:}
$$\text{Loss} = A \cdot N^{-\alpha} + B \cdot D^{-\beta} + C$$
\begin{itemize}
\item $N$ = model parameters
\item $D$ = dataset size
\item $\alpha \approx 0.07$, $\beta \approx 0.10$
\end{itemize}

\column{0.42\textwidth}
\textbf{Practical Implications:}
\begin{itemize}
\item 10x data $\rightarrow$ 2x performance
\item 10x parameters $\rightarrow$ 1.7x performance
\item 10x compute $\rightarrow$ 3x performance
\item Diminishing returns always
\end{itemize}

\textbf{Data Efficiency Tricks:}
\begin{itemize}
\item Data augmentation
\item Synthetic data generation
\item Active learning
\item Curriculum learning
\item Multi-task training
\end{itemize}

\whymatters{These laws predict costs before training}

\textbf{Current Limits:}
\begin{itemize}
\item Internet has ~10T tokens
\item Already using most of it
\item Quality > Quantity now
\end{itemize}
\end{columns}
\bottomnote{ These laws let us predict performance before training - saving millions in compute}
\end{frame}

% NEW: Optimizer Comparison
\begin{frame}{Optimization Algorithms: How Networks Learn}
\begin{center}
\textbf{The Evolution of Gradient Descent}
\end{center}
\vspace{2mm}
\begin{columns}
\column{0.55\textwidth}
\includegraphics[width=0.82\textwidth]{../figures/optimizer_comparison.pdf}

\textbf{SGD (1951):}
\begin{itemize}
\item Basic gradient descent
\item Learning rate: Fixed
\item Slow but reliable
\item Still used for fine-tuning
\end{itemize}

\textbf{Momentum (1964):}
\begin{itemize}
\item Remember past gradients
\item Accelerate in consistent directions
\item Escape shallow local minima
\item $v_t = \beta v_{t-1} + (1-\beta) g_t$
\end{itemize}

\column{0.42\textwidth}
\textbf{Adam (2014):}
\begin{itemize}
\item Adaptive learning rates per parameter
\item Combines momentum + RMSprop
\item De facto standard
\item Works out-of-the-box
\end{itemize}

\textbf{Modern Variants:}
\begin{itemize}
\item \textbf{AdamW:} Decoupled weight decay
\item \textbf{RAdam:} Rectified Adam
\item \textbf{LAMB:} Large batch training
\item \textbf{Sophia:} 2nd-order approximation
\end{itemize}

\textbf{Choosing an Optimizer:}
\begin{itemize}
\item Start with Adam (lr=3e-4)
\item Large batch: LAMB
\item Fine-tuning: SGD with momentum
\item Transformers: AdamW
\end{itemize}
\end{columns}
\bottomnote{ Adam works 90\% of the time - but that last 10\% matters for SOTA results}
\end{frame}

% NEW: Architecture Decision Tree
\begin{frame}{Quick Guide: Choosing Your Architecture}
\begin{center}
\textbf{\checkpoint{Which Network Should You Use?}}
\end{center}
\vspace{2mm}
\begin{columns}
\column{0.55\textwidth}
\includegraphics[width=0.9\textwidth]{../figures/architecture_decision_tree.pdf}

\tryit{You have 10,000 customer reviews to classify as positive/negative. Which architecture? Why?}

\column{0.42\textwidth}
\textbf{Decision Questions:}
\begin{enumerate}
\item Is your data sequential?
\item Does position matter?
\item Is it images/spatial?
\item Fixed or variable size?
\end{enumerate}

\textbf{Quick Rules:}
\begin{itemize}
\item Images $\rightarrow$ CNN
\item Text $\rightarrow$ Transformer/RNN
\item Tabular $\rightarrow$ Feedforward
\item Audio $\rightarrow$ CNN or RNN
\item Video $\rightarrow$ CNN + RNN
\end{itemize}

\confusion{Transformers now dominate most tasks, but specialized architectures still win for specific problems!}
\end{columns}
\bottomnote{ Answer: Transformer or RNN - text is sequential and context matters}
\end{frame}