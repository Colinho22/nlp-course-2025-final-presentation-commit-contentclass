{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Lab: Efficiency & Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand model quantization techniques\n",
    "- Implement knowledge distillation\n",
    "- Measure inference latency and throughput\n",
    "- Apply pruning strategies\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install transformers torch numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Setup\n",
    "print('Week 11: Efficiency & Optimization')\n",
    "print('=' * 50)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Size Analysis\n",
    "\n",
    "Let's understand what makes models large and how size affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    return (param_size + buffer_size) / 1024 / 1024\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "# Load a model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "model_size = get_model_size(model)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "print(f\"Bytes per parameter: {model_size * 1024 * 1024 / total_params:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model size by component\n",
    "component_sizes = {}\n",
    "for name, module in model.named_modules():\n",
    "    if len(list(module.children())) == 0:  # Leaf modules only\n",
    "        size = sum(p.numel() * p.element_size() for p in module.parameters())\n",
    "        component_type = type(module).__name__\n",
    "        if component_type not in component_sizes:\n",
    "            component_sizes[component_type] = 0\n",
    "        component_sizes[component_type] += size\n",
    "\n",
    "# Convert to MB and sort\n",
    "component_sizes = {k: v / 1024 / 1024 for k, v in component_sizes.items()}\n",
    "component_sizes = dict(sorted(component_sizes.items(), key=lambda x: x[1], reverse=True)[:8])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.barh(list(component_sizes.keys()), list(component_sizes.values()), color='#3333B2')\n",
    "ax.set_xlabel('Size (MB)', fontsize=12)\n",
    "ax.set_title('Model Size by Component Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, component_sizes.values()):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.1f} MB', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Quantization\n",
    "\n",
    "Quantization reduces model size by using lower-precision numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quantization concepts\n",
    "def simulate_quantization(tensor, bits=8):\n",
    "    \"\"\"Simulate quantization to lower precision\"\"\"\n",
    "    # Get range\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    \n",
    "    # Calculate scale and zero point\n",
    "    qmin, qmax = 0, 2**bits - 1\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "    \n",
    "    # Dequantize\n",
    "    dequantized = (quantized - zero_point) * scale\n",
    "    \n",
    "    return quantized.to(torch.int8 if bits == 8 else torch.int16), dequantized, scale\n",
    "\n",
    "# Test quantization\n",
    "original = torch.randn(1000)\n",
    "\n",
    "print(\"Quantization Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for bits in [8, 4, 2]:\n",
    "    quantized, dequantized, scale = simulate_quantization(original, bits)\n",
    "    error = torch.mean((original - dequantized) ** 2).item()\n",
    "    compression = 32 / bits\n",
    "    print(f\"{bits}-bit: MSE={error:.6f}, Compression={compression}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization error\n",
    "original = torch.randn(10000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original distribution\n",
    "axes[0, 0].hist(original.numpy(), bins=50, color='#3333B2', alpha=0.7)\n",
    "axes[0, 0].set_title('Original (FP32)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "\n",
    "# Different quantization levels\n",
    "for ax, bits in zip([axes[0, 1], axes[1, 0], axes[1, 1]], [8, 4, 2]):\n",
    "    _, dequantized, _ = simulate_quantization(original, bits)\n",
    "    ax.hist(dequantized.numpy(), bins=50, color='#FF7F0E', alpha=0.7)\n",
    "    error = torch.mean((original - dequantized) ** 2).item()\n",
    "    ax.set_title(f'{bits}-bit Quantized (MSE: {error:.4f})', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "plt.suptitle('Effect of Quantization on Value Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Knowledge Distillation\n",
    "\n",
    "Knowledge distillation transfers knowledge from a large model to a smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Large teacher model\"\"\"\n",
    "    def __init__(self, input_size=768, hidden_size=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Small student model\"\"\"\n",
    "    def __init__(self, input_size=768, hidden_size=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Create models\n",
    "teacher = TeacherModel()\n",
    "student = StudentModel()\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"  Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"  Student parameters: {student_params:,}\")\n",
    "print(f\"  Compression ratio: {teacher_params/student_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):\n",
    "    \"\"\"Combined distillation and classification loss\"\"\"\n",
    "    # Soft targets from teacher\n",
    "    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "    soft_student = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "    \n",
    "    # KL divergence loss (soft targets)\n",
    "    distill_loss = nn.functional.kl_div(soft_student, soft_targets, reduction='batchmean') * (temperature ** 2)\n",
    "    \n",
    "    # Standard cross-entropy loss (hard targets)\n",
    "    ce_loss = nn.functional.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    return alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "# Demonstrate distillation\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, 768)\n",
    "labels = torch.randint(0, 10, (batch_size,))\n",
    "\n",
    "# Get teacher predictions\n",
    "teacher.eval()\n",
    "with torch.no_grad():\n",
    "    teacher_logits = teacher(x)\n",
    "\n",
    "# Student forward pass\n",
    "student.train()\n",
    "student_logits = student(x)\n",
    "\n",
    "# Calculate losses\n",
    "loss = distillation_loss(student_logits, teacher_logits, labels)\n",
    "print(f\"Distillation loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of temperature on soft targets\n",
    "logits = torch.tensor([2.0, 1.0, 0.1, -0.5, -1.0])\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    probs = torch.softmax(logits / temp, dim=0).numpy()\n",
    "    ax.bar(range(5), probs, color='#3333B2')\n",
    "    ax.set_title(f'T = {temp}', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Class')\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel('Probability')\n",
    "\n",
    "plt.suptitle('Effect of Temperature on Softmax Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Pruning\n",
    "\n",
    "Pruning removes unnecessary weights to reduce model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_prune(weight, sparsity=0.5):\n",
    "    \"\"\"Prune weights by magnitude\"\"\"\n",
    "    threshold = torch.quantile(torch.abs(weight.flatten()), sparsity)\n",
    "    mask = torch.abs(weight) > threshold\n",
    "    return weight * mask, mask\n",
    "\n",
    "# Create a weight matrix\n",
    "weight = torch.randn(256, 256)\n",
    "\n",
    "print(\"Pruning Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sparsities = [0.3, 0.5, 0.7, 0.9]\n",
    "for sparsity in sparsities:\n",
    "    pruned, mask = magnitude_prune(weight, sparsity)\n",
    "    actual_sparsity = 1 - mask.float().mean().item()\n",
    "    frobenius_ratio = torch.norm(pruned) / torch.norm(weight)\n",
    "    print(f\"Target sparsity {sparsity*100:.0f}%: \"\n",
    "          f\"Actual {actual_sparsity*100:.1f}%, \"\n",
    "          f\"Frobenius norm ratio: {frobenius_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distribution and pruning\n",
    "weight = torch.randn(1000, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original weights\n",
    "axes[0, 0].hist(weight.flatten().numpy(), bins=50, color='#3333B2', alpha=0.7)\n",
    "axes[0, 0].set_title('Original Weight Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Different pruning levels\n",
    "for ax, sparsity in zip([axes[0, 1], axes[1, 0], axes[1, 1]], [0.5, 0.7, 0.9]):\n",
    "    pruned, _ = magnitude_prune(weight, sparsity)\n",
    "    non_zero = pruned[pruned != 0].flatten().numpy()\n",
    "    ax.hist(non_zero, bins=50, color='#2CA02C', alpha=0.7)\n",
    "    ax.set_title(f'{sparsity*100:.0f}% Pruned ({len(non_zero):,} remaining)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Effect of Pruning on Weight Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Inference Speed Benchmarking\n",
    "\n",
    "Let's measure and compare inference speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_inference(model, input_data, num_runs=10, warmup=3):\n    \"\"\"Benchmark model inference time\"\"\"\n    model.eval()\n\n    # Warmup\n    with torch.no_grad():\n        for _ in range(warmup):\n            _ = model(input_data)\n\n    # Benchmark\n    times = []\n    with torch.no_grad():\n        for _ in range(num_runs):\n            start = time.perf_counter()\n            _ = model(input_data)\n            end = time.perf_counter()\n            times.append((end - start) * 1000)  # Convert to ms\n\n    return {\n        'mean': np.mean(times),\n        'std': np.std(times),\n        'min': np.min(times),\n        'max': np.max(times)\n    }\n\n# Benchmark different batch sizes\nbatch_sizes = [1, 4, 8, 16]\nseq_length = 64\n\nresults = []\nprint(\"Inference Benchmark Results:\")\nprint(\"-\" * 60)\n\nfor batch_size in batch_sizes:\n    # Create dummy input\n    dummy_input = tokenizer([\"Hello world\"] * batch_size,\n                           padding='max_length',\n                           max_length=seq_length,\n                           return_tensors='pt')\n\n    stats = benchmark_inference(model, dummy_input['input_ids'], num_runs=10)\n    results.append({'batch_size': batch_size, **stats})\n\n    throughput = batch_size / (stats['mean'] / 1000)\n    print(f\"Batch {batch_size:2d}: {stats['mean']:.2f} +/- {stats['std']:.2f} ms, \"\n          f\"Throughput: {throughput:.1f} samples/sec\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "batch_sizes = [r['batch_size'] for r in results]\n",
    "latencies = [r['mean'] for r in results]\n",
    "throughputs = [b / (l / 1000) for b, l in zip(batch_sizes, latencies)]\n",
    "\n",
    "# Latency\n",
    "ax1.plot(batch_sizes, latencies, 'b-o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "ax1.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax1.set_title('Inference Latency vs Batch Size', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput\n",
    "ax2.bar(range(len(batch_sizes)), throughputs, color='#2CA02C')\n",
    "ax2.set_xticks(range(len(batch_sizes)))\n",
    "ax2.set_xticklabels(batch_sizes)\n",
    "ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "ax2.set_ylabel('Throughput (samples/sec)', fontsize=12)\n",
    "ax2.set_title('Inference Throughput vs Batch Size', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Quantization**: Implement a more sophisticated quantization scheme with per-channel scaling\n",
    "2. **Distillation**: Train a student model to match the teacher on a classification task\n",
    "3. **Pruning**: Implement structured pruning (remove entire neurons/heads)\n",
    "4. **Benchmarking**: Compare inference speed of different optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter: Structured pruning\n",
    "def structured_prune_neurons(layer, sparsity=0.5):\n",
    "    \"\"\"\n",
    "    Prune entire neurons based on their weight magnitude.\n",
    "    \n",
    "    Args:\n",
    "        layer: Linear layer to prune\n",
    "        sparsity: Fraction of neurons to remove\n",
    "    \n",
    "    Returns:\n",
    "        Indices of kept neurons\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Calculate importance of each neuron (L2 norm of weights)\n",
    "        importance = torch.norm(layer.weight, dim=1)\n",
    "        \n",
    "        # Find threshold\n",
    "        k = int(len(importance) * (1 - sparsity))\n",
    "        threshold = torch.topk(importance, k).values[-1]\n",
    "        \n",
    "        # Get indices of neurons to keep\n",
    "        keep_indices = importance >= threshold\n",
    "        \n",
    "        return keep_indices\n",
    "\n",
    "# Test\n",
    "test_layer = nn.Linear(100, 50)\n",
    "kept = structured_prune_neurons(test_layer, sparsity=0.5)\n",
    "print(f\"Original neurons: 50\")\n",
    "print(f\"Kept neurons: {kept.sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we explored:\n",
    "\n",
    "1. **Model size analysis**: Understanding what makes models large\n",
    "2. **Quantization**: Reducing precision to compress models\n",
    "3. **Knowledge distillation**: Training smaller models from larger ones\n",
    "4. **Pruning**: Removing unnecessary weights\n",
    "5. **Benchmarking**: Measuring inference performance\n",
    "\n",
    "**Key Takeaways**:\n",
    "- 8-bit quantization typically has minimal accuracy loss\n",
    "- Knowledge distillation can achieve 3-10x compression\n",
    "- Pruning can remove 50-90% of weights with careful tuning\n",
    "- Batch size significantly affects throughput"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}