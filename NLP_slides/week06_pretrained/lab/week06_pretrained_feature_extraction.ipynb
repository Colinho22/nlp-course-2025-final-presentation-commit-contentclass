{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Lab: Pre-trained Models - Feature Extraction\n",
    "\n",
    "## Learning Objectives\n",
    "- Load and use pre-trained BERT and GPT models\n",
    "- Extract contextual embeddings from both architectures\n",
    "- Compare BERT vs GPT representations\n",
    "- Visualize attention patterns\n",
    "- Use pre-trained features for classification\n",
    "- Understand when to use each architecture\n",
    "\n",
    "**Note**: This lab focuses on feature extraction (not fine-tuning) to work with limited compute resources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run once)\n",
    "# !pip install transformers torch numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Pre-trained Models\n",
    "\n",
    "We'll load BERT-base and GPT-2 (small) from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT\n",
    "print(\"Loading BERT-base-uncased...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "print(f\"BERT loaded:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in bert_model.parameters())/1e6:.1f}M\")\n",
    "print(f\"  Layers: {bert_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {bert_model.config.hidden_size}\")\n",
    "print(f\"  Attention heads: {bert_model.config.num_attention_heads}\")\n",
    "print(f\"  Vocabulary: {bert_model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "print(\"Loading GPT-2 (small)...\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')\n",
    "gpt_model.eval()\n",
    "\n",
    "# GPT-2 needs padding token\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "print(f\"GPT-2 loaded:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in gpt_model.parameters())/1e6:.1f}M\")\n",
    "print(f\"  Layers: {gpt_model.config.n_layer}\")\n",
    "print(f\"  Hidden size: {gpt_model.config.n_embd}\")\n",
    "print(f\"  Attention heads: {gpt_model.config.n_head}\")\n",
    "print(f\"  Vocabulary: {gpt_model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Extract BERT Embeddings\n",
    "\n",
    "BERT provides contextual embeddings for each token. The [CLS] token is commonly used as a sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text, model, tokenizer, layer=-1):\n",
    "    \"\"\"\n",
    "    Extract BERT embeddings for text\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "        model: BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        layer: Which layer to extract from (-1 = last layer)\n",
    "    \n",
    "    Returns:\n",
    "        cls_embedding: [CLS] token embedding (sentence representation)\n",
    "        all_embeddings: All token embeddings\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract from specified layer\n",
    "    hidden_states = outputs.hidden_states[layer]  # (batch, seq_len, hidden_size)\n",
    "    \n",
    "    # [CLS] token is first token\n",
    "    cls_embedding = hidden_states[0, 0, :].numpy()  # (hidden_size,)\n",
    "    all_embeddings = hidden_states[0, :, :].numpy()  # (seq_len, hidden_size)\n",
    "    \n",
    "    return cls_embedding, all_embeddings\n",
    "\n",
    "# Test with sample sentences\n",
    "sample_sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"BERT provides bidirectional context.\",\n",
    "    \"Pre-training enables transfer learning.\"\n",
    "]\n",
    "\n",
    "print(\"Extracting BERT embeddings...\\n\")\n",
    "for sent in sample_sentences:\n",
    "    cls_emb, all_emb = get_bert_embeddings(sent, bert_model, bert_tokenizer)\n",
    "    tokens = bert_tokenizer.tokenize(sent)\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  [CLS] embedding shape: {cls_emb.shape}\")\n",
    "    print(f\"  All embeddings shape: {all_emb.shape}\")\n",
    "    print(f\"  [CLS] L2 norm: {np.linalg.norm(cls_emb):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BERT embeddings with PCA\n",
    "sentences_for_viz = [\n",
    "    \"The cat sleeps.\",\n",
    "    \"The dog runs.\",\n",
    "    \"She loves reading books.\",\n",
    "    \"He enjoys playing games.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Natural language processing is powerful.\",\n",
    "    \"Python is a programming language.\",\n",
    "    \"Java is also a programming language.\"\n",
    "]\n",
    "\n",
    "# Extract [CLS] embeddings\n",
    "bert_cls_embeddings = []\n",
    "for sent in sentences_for_viz:\n",
    "    cls_emb, _ = get_bert_embeddings(sent, bert_model, bert_tokenizer)\n",
    "    bert_cls_embeddings.append(cls_emb)\n",
    "\n",
    "bert_cls_embeddings = np.array(bert_cls_embeddings)  # (n_sentences, 768)\n",
    "\n",
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "bert_pca = pca.fit_transform(bert_cls_embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for i, (sent, coords) in enumerate(zip(sentences_for_viz, bert_pca)):\n",
    "    ax.scatter(coords[0], coords[1], s=200, alpha=0.6)\n",
    "    ax.annotate(sent, xy=coords, xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=9, bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel('PCA Component 1', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('PCA Component 2', fontsize=12, fontweight='bold')\n",
    "ax.set_title('BERT [CLS] Embeddings in 2D Space (PCA)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extract GPT Embeddings\n",
    "\n",
    "GPT-2 uses final hidden states (no [CLS] token). We'll use the last token's embedding as sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_embeddings(text, model, tokenizer, layer=-1):\n",
    "    \"\"\"\n",
    "    Extract GPT-2 embeddings\n",
    "    \n",
    "    Returns:\n",
    "        last_token_embedding: Last token's hidden state (sentence representation)\n",
    "        all_embeddings: All token embeddings\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract from specified layer\n",
    "    hidden_states = outputs.hidden_states[layer]\n",
    "    \n",
    "    # Last token as sentence embedding\n",
    "    last_token_embedding = hidden_states[0, -1, :].numpy()\n",
    "    all_embeddings = hidden_states[0, :, :].numpy()\n",
    "    \n",
    "    return last_token_embedding, all_embeddings\n",
    "\n",
    "# Extract GPT embeddings for same sentences\n",
    "print(\"Extracting GPT-2 embeddings...\\n\")\n",
    "gpt_sentence_embeddings = []\n",
    "\n",
    "for sent in sample_sentences:\n",
    "    last_emb, all_emb = get_gpt_embeddings(sent, gpt_model, gpt_tokenizer)\n",
    "    gpt_sentence_embeddings.append(last_emb)\n",
    "    tokens = gpt_tokenizer.tokenize(sent)\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Last token embedding shape: {last_emb.shape}\")\n",
    "    print(f\"  All embeddings shape: {all_emb.shape}\")\n",
    "    print(f\"  Last token L2 norm: {np.linalg.norm(last_emb):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare BERT vs GPT Embeddings\n",
    "\n",
    "Let's compare how BERT and GPT represent the same sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract both BERT and GPT embeddings for comparison\n",
    "comparison_sentences = [\n",
    "    \"The bank is by the river.\",\n",
    "    \"I deposited money at the bank.\",\n",
    "    \"The model performs well.\",\n",
    "    \"She is a fashion model.\",\n",
    "]\n",
    "\n",
    "bert_embs = []\n",
    "gpt_embs = []\n",
    "\n",
    "for sent in comparison_sentences:\n",
    "    bert_cls, _ = get_bert_embeddings(sent, bert_model, bert_tokenizer)\n",
    "    gpt_last, _ = get_gpt_embeddings(sent, gpt_model, gpt_tokenizer)\n",
    "    \n",
    "    bert_embs.append(bert_cls)\n",
    "    gpt_embs.append(gpt_last)\n",
    "\n",
    "bert_embs = np.array(bert_embs)\n",
    "gpt_embs = np.array(gpt_embs)\n",
    "\n",
    "print(f\"BERT embeddings shape: {bert_embs.shape}\")\n",
    "print(f\"GPT embeddings shape: {gpt_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both in same 2D space\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# BERT visualization\n",
    "pca_bert = PCA(n_components=2)\n",
    "bert_2d = pca_bert.fit_transform(bert_embs)\n",
    "\n",
    "for i, sent in enumerate(comparison_sentences):\n",
    "    color = 'red' if 'bank' in sent else 'blue' if 'model' in sent else 'gray'\n",
    "    ax1.scatter(bert_2d[i, 0], bert_2d[i, 1], s=200, c=color, alpha=0.6)\n",
    "    ax1.annotate(sent, xy=bert_2d[i], xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=8, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax1.set_title('BERT Embeddings (Bidirectional)', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('PCA 1', fontsize=11)\n",
    "ax1.set_ylabel('PCA 2', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GPT visualization\n",
    "pca_gpt = PCA(n_components=2)\n",
    "gpt_2d = pca_gpt.fit_transform(gpt_embs)\n",
    "\n",
    "for i, sent in enumerate(comparison_sentences):\n",
    "    color = 'red' if 'bank' in sent else 'blue' if 'model' in sent else 'gray'\n",
    "    ax2.scatter(gpt_2d[i, 0], gpt_2d[i, 1], s=200, c=color, alpha=0.6)\n",
    "    ax2.annotate(sent, xy=gpt_2d[i], xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=8, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "ax2.set_title('GPT-2 Embeddings (Autoregressive)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('PCA 1', fontsize=11)\n",
    "ax2.set_ylabel('PCA 2', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how similar sentences cluster together in both representations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "bert_sim = cosine_similarity(bert_embs)\n",
    "gpt_sim = cosine_similarity(gpt_embs)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BERT similarities\n",
    "im1 = ax1.imshow(bert_sim, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_title('BERT Cosine Similarities', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(range(len(comparison_sentences)))\n",
    "ax1.set_yticks(range(len(comparison_sentences)))\n",
    "ax1.set_xticklabels([f'S{i+1}' for i in range(len(comparison_sentences))], fontsize=9)\n",
    "ax1.set_yticklabels([f'S{i+1}' for i in range(len(comparison_sentences))], fontsize=9)\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# GPT similarities\n",
    "im2 = ax2.imshow(gpt_sim, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_title('GPT-2 Cosine Similarities', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(range(len(comparison_sentences)))\n",
    "ax2.set_yticks(range(len(comparison_sentences)))\n",
    "ax2.set_xticklabels([f'S{i+1}' for i in range(len(comparison_sentences))], fontsize=9)\n",
    "ax2.set_yticklabels([f'S{i+1}' for i in range(len(comparison_sentences))], fontsize=9)\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSentence pairs with 'bank' (S1, S2) and 'model' (S3, S4) should be similar.\")\n",
    "print(f\"BERT - bank pair similarity: {bert_sim[0, 1]:.3f}\")\n",
    "print(f\"GPT - bank pair similarity: {gpt_sim[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualize Attention Patterns\n",
    "\n",
    "Let's examine what BERT and GPT attend to in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bert_attention(text, model, tokenizer, layer=11, head=0):\n",
    "    \"\"\"\n",
    "    Visualize BERT attention patterns\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Extract attention from specified layer and head\n",
    "    attention = outputs.attentions[layer][0, head, :, :].numpy()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "    \n",
    "    im = ax.imshow(attention, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=10)\n",
    "    ax.set_yticklabels(tokens, fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel('Attend TO (keys)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Attend FROM (queries)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'BERT Attention Pattern (Layer {layer+1}, Head {head+1})',\n",
    "                fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention, tokens\n",
    "\n",
    "# Visualize attention for a sample sentence\n",
    "test_sentence = \"The cat sat on the mat because it was tired.\"\n",
    "attention, tokens = visualize_bert_attention(test_sentence, bert_model, bert_tokenizer, layer=11, head=0)\n",
    "\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Notice: BERT can attend to ALL tokens (full matrix, no triangular restriction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gpt_attention(text, model, tokenizer, layer=11, head=0):\n",
    "    \"\"\"\n",
    "    Visualize GPT-2 attention patterns (should show causal/triangular)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    attention = outputs.attentions[layer][0, head, :, :].numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "    \n",
    "    im = ax.imshow(attention, cmap='Oranges', aspect='auto')\n",
    "    \n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Attend TO (keys)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Attend FROM (queries)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'GPT-2 Attention Pattern (Layer {layer+1}, Head {head+1}) - Notice Triangular!',\n",
    "                fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention, tokens\n",
    "\n",
    "# Visualize GPT attention (should be lower-triangular due to causal masking)\n",
    "gpt_attention, gpt_tokens = visualize_gpt_attention(test_sentence, gpt_model, gpt_tokenizer, layer=11, head=0)\n",
    "\n",
    "print(f\"\\nTokens: {gpt_tokens}\")\n",
    "print(f\"Notice: GPT-2 shows CAUSAL (lower-triangular) pattern - can't attend to future tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Use for Classification Task\n",
    "\n",
    "Use BERT embeddings as features for a simple sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple sentiment dataset\n",
    "train_texts = [\n",
    "    # Positive\n",
    "    \"This movie was excellent!\",\n",
    "    \"I absolutely loved it.\",\n",
    "    \"Best film I've seen this year.\",\n",
    "    \"Highly recommend this.\",\n",
    "    \"Fantastic performance by the actors.\",\n",
    "    \"Brilliant storytelling.\",\n",
    "    # Negative\n",
    "    \"Terrible waste of time.\",\n",
    "    \"I hated every minute.\",\n",
    "    \"Worst movie ever made.\",\n",
    "    \"Do not watch this.\",\n",
    "    \"Boring and predictable.\",\n",
    "    \"Completely disappointing.\"\n",
    "]\n",
    "\n",
    "train_labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  # 1=positive, 0=negative\n",
    "\n",
    "test_texts = [\n",
    "    \"Amazing experience!\",\n",
    "    \"Not worth your time.\",\n",
    "    \"Truly inspiring film.\",\n",
    "    \"Awful and boring.\"\n",
    "]\n",
    "\n",
    "test_labels = [1, 0, 1, 0]\n",
    "\n",
    "print(f\"Training set: {len(train_texts)} examples\")\n",
    "print(f\"Test set: {len(test_texts)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BERT features\n",
    "print(\"Extracting BERT features for training...\")\n",
    "train_features_bert = []\n",
    "for text in train_texts:\n",
    "    cls_emb, _ = get_bert_embeddings(text, bert_model, bert_tokenizer)\n",
    "    train_features_bert.append(cls_emb)\n",
    "\n",
    "train_features_bert = np.array(train_features_bert)\n",
    "\n",
    "print(\"Extracting BERT features for test...\")\n",
    "test_features_bert = []\n",
    "for text in test_texts:\n",
    "    cls_emb, _ = get_bert_embeddings(text, bert_model, bert_tokenizer)\n",
    "    test_features_bert.append(cls_emb)\n",
    "\n",
    "test_features_bert = np.array(test_features_bert)\n",
    "\n",
    "print(f\"\\nTrain features shape: {train_features_bert.shape}\")\n",
    "print(f\"Test features shape: {test_features_bert.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression classifier on BERT features\n",
    "clf_bert = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_bert.fit(train_features_bert, train_labels)\n",
    "\n",
    "# Predict on test set\n",
    "pred_bert = clf_bert.predict(test_features_bert)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_bert = accuracy_score(test_labels, pred_bert)\n",
    "\n",
    "print(\"=== BERT Feature-Based Classification ===\")\n",
    "print(f\"\\nTest Accuracy: {accuracy_bert:.3f}\")\n",
    "print(f\"\\nPredictions vs True:\")\n",
    "for text, true_label, pred_label in zip(test_texts, test_labels, pred_bert):\n",
    "    label_map = {0: 'Negative', 1: 'Positive'}\n",
    "    match = \"âœ“\" if true_label == pred_label else \"X\"\n",
    "    print(f\"  {match} '{text}'\")\n",
    "    print(f\"     True: {label_map[true_label]}, Predicted: {label_map[pred_label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Compare BERT vs GPT for Classification\n",
    "\n",
    "Now extract GPT-2 features and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GPT features\n",
    "print(\"Extracting GPT-2 features for training...\")\n",
    "train_features_gpt = []\n",
    "for text in train_texts:\n",
    "    last_emb, _ = get_gpt_embeddings(text, gpt_model, gpt_tokenizer)\n",
    "    train_features_gpt.append(last_emb)\n",
    "\n",
    "train_features_gpt = np.array(train_features_gpt)\n",
    "\n",
    "print(\"Extracting GPT-2 features for test...\")\n",
    "test_features_gpt = []\n",
    "for text in test_texts:\n",
    "    last_emb, _ = get_gpt_embeddings(text, gpt_model, gpt_tokenizer)\n",
    "    test_features_gpt.append(last_emb)\n",
    "\n",
    "test_features_gpt = np.array(test_features_gpt)\n",
    "\n",
    "# Train classifier on GPT features\n",
    "clf_gpt = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_gpt.fit(train_features_gpt, train_labels)\n",
    "\n",
    "# Predict\n",
    "pred_gpt = clf_gpt.predict(test_features_gpt)\n",
    "accuracy_gpt = accuracy_score(test_labels, pred_gpt)\n",
    "\n",
    "print(\"=== GPT-2 Feature-Based Classification ===\")\n",
    "print(f\"\\nTest Accuracy: {accuracy_gpt:.3f}\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"BERT accuracy: {accuracy_bert:.3f}\")\n",
    "print(f\"GPT-2 accuracy: {accuracy_gpt:.3f}\")\n",
    "\n",
    "if accuracy_bert > accuracy_gpt:\n",
    "    print(\"\\nBERT wins! (Expected - bidirectional better for classification)\")\n",
    "else:\n",
    "    print(\"\\nGPT-2 competitive! (Impressive for autoregressive model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['BERT\\n(Bidirectional)', 'GPT-2\\n(Autoregressive)']\n",
    "accuracies = [accuracy_bert, accuracy_gpt]\n",
    "colors = ['#E74C3C', '#3498DB']\n",
    "\n",
    "bars = ax.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Classification Performance: BERT vs GPT-2 (Feature Extraction)', fontsize=14,\n",
    "            fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
    "    ax.text(i, acc + 0.03, f'{acc:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "ax.grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: BERT's bidirectional context gives it an edge for classification tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Loading Pre-trained Models**:\n",
    "   - HuggingFace makes it easy to load BERT and GPT\n",
    "   - Models are large (110M-1.5B parameters) but manageable\n",
    "   \n",
    "2. **Feature Extraction**:\n",
    "   - BERT: Use [CLS] token for sentence embedding\n",
    "   - GPT: Use last token for sentence embedding\n",
    "   - Both provide rich 768-dim contextual representations\n",
    "   \n",
    "3. **BERT vs GPT**:\n",
    "   - BERT: Bidirectional, better for classification/understanding\n",
    "   - GPT: Causal, better for generation tasks\n",
    "   - Both learn useful representations\n",
    "   \n",
    "4. **Attention Patterns**:\n",
    "   - BERT: Full attention matrix (can see all tokens)\n",
    "   - GPT: Lower-triangular (causal mask prevents future peeking)\n",
    "   \n",
    "5. **Practical Use**:\n",
    "   - Pre-trained features work well for downstream tasks\n",
    "   - Simple classifier on top achieves good results\n",
    "   - No fine-tuning needed for quick prototyping\n",
    "\n",
    "### Next Steps:\n",
    "- Week 7: Advanced architectures (T5, GPT-3, scaling laws)\n",
    "- Week 10: Fine-tuning and prompt engineering\n",
    "- Experiment: Try on your own classification/generation tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Different Layers**: Extract embeddings from different BERT layers (0, 6, 12). How do they differ?\n",
    "\n",
    "2. **Pooling Strategies**: Compare [CLS] vs mean pooling vs max pooling for sentence embeddings.\n",
    "\n",
    "3. **Multi-class Classification**: Extend to 3+ classes (positive/neutral/negative).\n",
    "\n",
    "4. **Attention Analysis**: Find which words BERT/GPT attend to most for specific tasks.\n",
    "\n",
    "5. **Domain Adaptation**: Try on technical text or social media. Does performance degrade?\n",
    "\n",
    "6. **Embedding Similarity**: Build a semantic search using cosine similarity of BERT embeddings.\n",
    "\n",
    "7. **Generation with GPT**: Use GPT-2 to generate text (extend beyond feature extraction)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
