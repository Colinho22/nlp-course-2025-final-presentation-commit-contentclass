{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based Summarization Lab\n",
    "\n",
    "**Objective**: Explore LLM summarization using local open-source models (LLaMA/Mistral)\n",
    "\n",
    "**Topics Covered**:\n",
    "1. Loading local LLMs via Hugging Face\n",
    "2. Prompt engineering (zero-shot vs few-shot)\n",
    "3. Decoding parameter experiments (temperature, top-p, repetition penalty)\n",
    "4. Handling long documents (chunking strategy)\n",
    "\n",
    "**Duration**: 60-90 minutes\n",
    "\n",
    "**Prerequisites**: Hugging Face Transformers, PyTorch, 8GB+ GPU (or CPU with patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Model Loading\n",
    "\n",
    "We'll use a smaller open-source model that can run locally:\n",
    "- **Model**: FLAN-T5-small or distilgpt2 (for low-resource environments)\n",
    "- **Alternative**: mistral-7b-instruct (if you have 16GB+ GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\osterriederjro\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\OsterriederJRO\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\OsterriederJRO\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\OsterriederJRO\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if needed)\n",
    "!pip install transformers torch sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/flan-t5-small...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764b432493e4a2684f1ed442f307998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165716b8fa204e5186dcea4ca1194686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ca8db34df443069cfb4ddbe720621c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b172255be004a8593ce817dbc95155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffce35f3caf841bdb72c65262d5946af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f104493414461fb01fcb19fffad64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8550f59cfc4c88a0f63879cc78ef7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Load FLAN-T5-small (good for summarization, runs on CPU)\n",
    "model_name = \"google/flan-t5-small\"  # 80M parameters, fast\n",
    "# Alternative: \"google/flan-t5-base\" (250M parameters, better quality)\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Prompt Engineering Experiments\n",
    "\n",
    "Compare **zero-shot** vs **few-shot** prompting on the same article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article length: 1054 characters, 160 words\n"
     ]
    }
   ],
   "source": [
    "# Sample article to summarize\n",
    "article = \"\"\"The Federal Reserve raised interest rates by 0.25 percentage points on Wednesday, \n",
    "marking the tenth consecutive increase in borrowing costs as the central bank continues its \n",
    "battle against persistent inflation. The decision brings the benchmark federal funds rate to \n",
    "a range of 5.00% to 5.25%, the highest level in 16 years. Fed Chair Jerome Powell indicated \n",
    "that officials would carefully monitor economic data before deciding on future rate changes, \n",
    "suggesting a possible pause in rate hikes. Inflation has shown signs of cooling, with the \n",
    "consumer price index rising 4.9% year-over-year in April, down from 5.0% in March. However, \n",
    "core inflation, which excludes volatile food and energy prices, remains elevated at 5.5%. \n",
    "The Fed's preferred measure, the personal consumption expenditures price index, rose 4.2% \n",
    "in March. Financial markets reacted positively to the Fed's statement, with major stock \n",
    "indexes gaining ground. Economists are divided on whether the Fed will pause or implement \n",
    "one more rate increase at its next meeting in June.\"\"\"\n",
    "\n",
    "print(f\"Article length: {len(article)} characters, {len(article.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, prompt_prefix=\"\", max_length=100, temperature=1.0, top_p=1.0, repetition_penalty=1.0):\n",
    "    \"\"\"Generate summary with specified parameters\"\"\"\n",
    "    # For FLAN-T5, format as instruction\n",
    "    full_input = f\"{prompt_prefix}\\n\\n{text}\" if prompt_prefix else f\"Summarize: {text}\"\n",
    "    \n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT PROMPT ===\n",
      "Prompt: Summarize this article in 3 sentences\n",
      "\n",
      "Summary: Federal Reserve Chair Jerome Powell says the financial markets remained happy on Tuesday, predicting the next week that the rate would lower for a year.\n",
      "\n",
      "Length: 25 words\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Zero-shot (simple instruction)\n",
    "print(\"=== ZERO-SHOT PROMPT ===\")\n",
    "zero_shot_prompt = \"Summarize this article in 3 sentences\"\n",
    "print(f\"Prompt: {zero_shot_prompt}\\n\")\n",
    "\n",
    "summary_zero = summarize(article, zero_shot_prompt)\n",
    "print(f\"Summary: {summary_zero}\\n\")\n",
    "print(f\"Length: {len(summary_zero.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEW-SHOT PROMPT ===\n",
      "Prompt (truncated): You are a financial news summarizer. Here's an example:\n",
      "\n",
      "Article: The stock market rose 2% today as ...\n",
      "\n",
      "Summary: Federal Reserve raises interest rates by 0.25 percentage points on Wednesday, a pause that continues to be a trend between the central bank and the central bank.\n",
      "\n",
      "Length: 27 words\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Few-shot (with example)\n",
    "print(\"=== FEW-SHOT PROMPT ===\")\n",
    "\n",
    "few_shot_prompt = \"\"\"You are a financial news summarizer. Here's an example:\n",
    "\n",
    "Article: The stock market rose 2% today as tech companies reported strong earnings...\n",
    "Summary: Markets gained on tech earnings. Major indexes up 2%. Investor confidence increased.\n",
    "\n",
    "Now summarize this article in the same style (3 short sentences, focus on facts):\"\"\"\n",
    "\n",
    "print(f\"Prompt (truncated): {few_shot_prompt[:100]}...\\n\")\n",
    "\n",
    "summary_few = summarize(article, few_shot_prompt)\n",
    "print(f\"Summary: {summary_few}\\n\")\n",
    "print(f\"Length: {len(summary_few.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON ===\n",
      "Zero-shot: Federal Reserve Chair Jerome Powell says the financial markets remained happy on Tuesday, predicting the next week that the rate would lower for a year.\n",
      "\n",
      "Few-shot: Federal Reserve raises interest rates by 0.25 percentage points on Wednesday, a pause that continues to be a trend between the central bank and the central bank.\n",
      "\n",
      "Observation: Which is more consistent with desired format?\n"
     ]
    }
   ],
   "source": [
    "# Comparison\n",
    "print(\"=== COMPARISON ===\")\n",
    "print(f\"Zero-shot: {summary_zero}\")\n",
    "print(f\"\\nFew-shot: {summary_few}\")\n",
    "print(f\"\\nObservation: Which is more consistent with desired format?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Decoding Parameter Experiments\n",
    "\n",
    "Test how **temperature**, **top-p**, and **repetition penalty** affect output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPERATURE EXPERIMENTS ===\n",
      "\n",
      "--- Temperature = 0.3 ---\n",
      "Federal Reserve chiefs have raised interest rates to a range of 5.00% to 5.25%, the highest level in 16 years.\n",
      "\n",
      "--- Temperature = 0.7 ---\n",
      "Federal Reserve President Mark Zuckerberg told the Wall Street Journal the Federal Reserve remained calm in the wake of the flurry of interest rates.\n",
      "\n",
      "--- Temperature = 1.0 ---\n",
      "Federal Reserve chair Jerome Powell said the US rate had been lowered, a move which highlights ongoing uncertainty as the central bank faces interest rates.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3A: Temperature variations\n",
    "print(\"=== TEMPERATURE EXPERIMENTS ===\")\n",
    "base_prompt = \"Summarize this financial news in 2 sentences\"\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n--- Temperature = {temp} ---\")\n",
    "    summary = summarize(article, base_prompt, temperature=temp, max_length=60)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP-P (NUCLEUS) EXPERIMENTS ===\n",
      "\n",
      "--- Top-p = 0.8 ---\n",
      "Federal Reserve officials have raised interest rates by 0.25 percentage points in a bid to cut interest rates, despite a decline in inflation.\n",
      "\n",
      "--- Top-p = 0.9 ---\n",
      "Federal Reserve officials say they will monitor data on a possible rate hike to keep inflation lower.\n",
      "\n",
      "--- Top-p = 0.95 ---\n",
      "Federal Reserve Chairman Jerome Powell said he would monitor the current rate growth rate and make changes to interest rates in a way that would ensure a pause in rate hikes.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3B: Top-p (nucleus) variations\n",
    "print(\"=== TOP-P (NUCLEUS) EXPERIMENTS ===\")\n",
    "\n",
    "top_ps = [0.8, 0.9, 0.95]\n",
    "for p in top_ps:\n",
    "    print(f\"\\n--- Top-p = {p} ---\")\n",
    "    summary = summarize(article, base_prompt, temperature=0.7, top_p=p, max_length=60)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REPETITION PENALTY EXPERIMENTS ===\n",
      "\n",
      "--- Repetition Penalty = 1.0 ---\n",
      "Federal Reserve Chairman Jerome Powell has warned that inflation may impede growth, but does not raise interest rates.\n",
      "\n",
      "--- Repetition Penalty = 1.2 ---\n",
      "Federal Reserve Chairman Jerome Powell has said he expects to raise interest rates by 0.25 per cent to 1.5%.\n",
      "\n",
      "--- Repetition Penalty = 1.5 ---\n",
      "Federal Reserve chiefs have announced a drop in interest rates.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3C: Repetition penalty\n",
    "print(\"=== REPETITION PENALTY EXPERIMENTS ===\")\n",
    "\n",
    "penalties = [1.0, 1.2, 1.5]\n",
    "for penalty in penalties:\n",
    "    print(f\"\\n--- Repetition Penalty = {penalty} ---\")\n",
    "    summary = summarize(article, base_prompt, temperature=0.7, repetition_penalty=penalty, max_length=60)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Long Document Handling (Chunking)\n",
    "\n",
    "For documents exceeding model context limits, we need to chunk and merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate long document (concatenate article multiple times)\n",
    "long_document = (article + \"\\n\\n\") * 5  # ~5x original length\n",
    "print(f\"Long document: {len(long_document)} characters, {len(long_document.split())} words\")\n",
    "print(f\"\\nThis exceeds typical context windows for smaller models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks by words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        if chunk:  # Only add non-empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk the long document\n",
    "chunks = chunk_text(long_document, chunk_size=300, overlap=50)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:3]):  # Show first 3\n",
    "    print(f\"\\nChunk {i+1}: {len(chunk.split())} words\")\n",
    "    print(chunk[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize each chunk\n",
    "print(\"=== SUMMARIZING EACH CHUNK ===\")\n",
    "chunk_summaries = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nProcessing chunk {i+1}/{len(chunks)}...\")\n",
    "    summary = summarize(chunk, \"Summarize briefly\", max_length=50, temperature=0.3)\n",
    "    chunk_summaries.append(summary)\n",
    "    print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge chunk summaries into final summary\n",
    "print(\"\\n=== MERGING CHUNK SUMMARIES ===\")\n",
    "combined_summaries = \"\\n\\n\".join(chunk_summaries)\n",
    "print(f\"Combined summaries ({len(combined_summaries.split())} words):\\n\")\n",
    "print(combined_summaries)\n",
    "\n",
    "print(\"\\n=== FINAL SUMMARY (summarizing the summaries) ===\")\n",
    "final_summary = summarize(combined_summaries, \"Combine these summaries into one coherent summary\", \n",
    "                          max_length=80, temperature=0.3)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "In this lab, you explored:\n",
    "\n",
    "1. **Model Loading**: Used FLAN-T5-small, a local open-source model\n",
    "2. **Prompt Engineering**: \n",
    "   - Zero-shot: Simple instructions work but vary\n",
    "   - Few-shot: Examples improve consistency\n",
    "3. **Decoding Parameters**:\n",
    "   - Temperature: Low (0.3) = factual, High (1.0) = creative\n",
    "   - Top-p: Filters unlikely words, typically 0.9\n",
    "   - Repetition penalty: Reduces redundancy (1.2 recommended)\n",
    "4. **Long Documents**: Chunking + merging strategy handles context limits\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try larger models (FLAN-T5-base, mistral-7b) if you have GPU\n",
    "- Experiment with different prompt templates\n",
    "- Test on real documents from different domains (news, scientific, legal)\n",
    "- Implement map-reduce or hierarchical summarization strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
