{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 Lab: N-grams and Statistical Language Models\n",
        "\n",
        "## Learning Objectives\n",
        "- Build n-gram language models from scratch\n",
        "- Calculate perplexity to evaluate model quality\n",
        "- Generate text using different n-gram models\n",
        "- Understand smoothing techniques\n",
        "- Compare unigram, bigram, and trigram models\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configure visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text corpus (Alice in Wonderland excerpt)\n",
        "sample_text = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do. Once or twice she had peeped into the book her sister was reading,\n",
        "but it had no pictures or conversations in it. And what is the use of a book, thought Alice,\n",
        "without pictures or conversations?\n",
        "\n",
        "So she was considering in her own mind, as well as she could, for the hot day made her feel\n",
        "very sleepy and stupid, whether the pleasure of making a daisy chain would be worth the trouble\n",
        "of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
        "\n",
        "There was nothing so very remarkable in that. Alice did not even think it so very much out of\n",
        "the way to hear the Rabbit say to itself, Oh dear! Oh dear! I shall be late! But when the\n",
        "Rabbit actually took a watch out of its waistcoat pocket, and looked at it, and then hurried on,\n",
        "Alice started to her feet, for it flashed across her mind that she had never before seen a\n",
        "rabbit with either a waistcoat pocket, or a watch to take out of it.\"\"\"\n",
        "\n",
        "print(f\"Corpus length: {len(sample_text)} characters\")\n",
        "print(f\"First 200 characters:\\n{sample_text[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text: str, lowercase: bool = True) -> List[str]:\n",
        "    \"\"\"Preprocess text into list of tokens\"\"\"\n",
        "    # Convert to lowercase\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "    \n",
        "    # Replace newlines with spaces\n",
        "    text = text.replace('\\n', ' ')\n",
        "    \n",
        "    # Add spaces around punctuation\n",
        "    text = re.sub(r'([.!?,;])', r' \\1 ', text)\n",
        "    \n",
        "    # Split into tokens\n",
        "    tokens = text.split()\n",
        "    \n",
        "    # Remove empty tokens\n",
        "    tokens = [token for token in tokens if token]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# Preprocess the corpus\n",
        "tokens = preprocess_text(sample_text)\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "print(f\"Unique tokens: {len(set(tokens))}\")\n",
        "print(f\"\\nFirst 20 tokens:\\n{tokens[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token frequencies\n",
        "token_freq = Counter(tokens)\n",
        "print(\"Most common tokens:\")\n",
        "for token, count in token_freq.most_common(15):\n",
        "    print(f\"  '{token}': {count}\")\n",
        "\n",
        "# Plot token frequency distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot top 20 most common tokens\n",
        "top_tokens = token_freq.most_common(20)\n",
        "ax1.bar(range(len(top_tokens)), [count for _, count in top_tokens])\n",
        "ax1.set_xticks(range(len(top_tokens)))\n",
        "ax1.set_xticklabels([token for token, _ in top_tokens], rotation=45, ha='right')\n",
        "ax1.set_xlabel('Token')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Top 20 Most Frequent Tokens')\n",
        "\n",
        "# Plot Zipf's law\n",
        "frequencies = sorted(token_freq.values(), reverse=True)\n",
        "ax2.loglog(range(1, len(frequencies)+1), frequencies, 'b-', alpha=0.6)\n",
        "ax2.set_xlabel('Token Rank (log scale)')\n",
        "ax2.set_ylabel('Frequency (log scale)')\n",
        "ax2.set_title(\"Zipf's Law Distribution\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Building N-gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, n: int, smoothing: str = 'none', alpha: float = 1.0):\n",
        "        \"\"\"\n",
        "        N-gram language model\n",
        "        \n",
        "        Args:\n",
        "            n: Order of n-gram (1=unigram, 2=bigram, 3=trigram, etc.)\n",
        "            smoothing: Smoothing technique ('none', 'laplace', 'add-k')\n",
        "            alpha: Smoothing parameter for add-k smoothing\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.smoothing = smoothing\n",
        "        self.alpha = alpha\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.vocabulary = set()\n",
        "        self.total_tokens = 0\n",
        "        \n",
        "    def train(self, tokens: List[str]):\n",
        "        \"\"\"Train the n-gram model on tokenized text\"\"\"\n",
        "        # Add special tokens for sentence boundaries\n",
        "        tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
        "        \n",
        "        # Build vocabulary\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "        self.total_tokens = len(tokens)\n",
        "        \n",
        "        # Count n-grams\n",
        "        for i in range(len(tokens) - self.n + 1):\n",
        "            ngram = tuple(tokens[i:i+self.n])\n",
        "            context = ngram[:-1]\n",
        "            \n",
        "            self.ngram_counts[ngram] += 1\n",
        "            self.context_counts[context] += 1\n",
        "        \n",
        "        print(f\"Trained {self.n}-gram model:\")\n",
        "        print(f\"  Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"  Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
        "        \n",
        "    def probability(self, ngram: Tuple[str, ...]) -> float:\n",
        "        \"\"\"Calculate probability of an n-gram\"\"\"\n",
        "        context = ngram[:-1]\n",
        "        \n",
        "        if self.n == 1:\n",
        "            # Unigram model\n",
        "            count = self.ngram_counts[ngram]\n",
        "            if self.smoothing == 'laplace' or self.smoothing == 'add-k':\n",
        "                return (count + self.alpha) / (self.total_tokens + self.alpha * self.vocab_size)\n",
        "            else:\n",
        "                return count / self.total_tokens if self.total_tokens > 0 else 0\n",
        "        else:\n",
        "            # N-gram model (n > 1)\n",
        "            ngram_count = self.ngram_counts[ngram]\n",
        "            context_count = self.context_counts[context]\n",
        "            \n",
        "            if self.smoothing == 'laplace' or self.smoothing == 'add-k':\n",
        "                return (ngram_count + self.alpha) / (context_count + self.alpha * self.vocab_size)\n",
        "            else:\n",
        "                return ngram_count / context_count if context_count > 0 else 0\n",
        "    \n",
        "    def generate(self, num_tokens: int = 50, temperature: float = 1.0) -> str:\n",
        "        \"\"\"Generate text using the n-gram model\"\"\"\n",
        "        # Start with initial context\n",
        "        context = ['<START>'] * (self.n - 1)\n",
        "        generated = []\n",
        "        \n",
        "        for _ in range(num_tokens):\n",
        "            # Get possible next tokens\n",
        "            candidates = defaultdict(float)\n",
        "            \n",
        "            for ngram in self.ngram_counts:\n",
        "                if ngram[:-1] == tuple(context):\n",
        "                    next_token = ngram[-1]\n",
        "                    prob = self.probability(ngram)\n",
        "                    candidates[next_token] = prob ** (1/temperature) if prob > 0 else 0\n",
        "            \n",
        "            if not candidates or all(p == 0 for p in candidates.values()):\n",
        "                # Fallback to random token\n",
        "                next_token = random.choice(list(self.vocabulary - {'<START>', '<END>'}))\n",
        "            else:\n",
        "                # Sample from distribution\n",
        "                tokens = list(candidates.keys())\n",
        "                probs = np.array(list(candidates.values()))\n",
        "                probs = probs / probs.sum()\n",
        "                next_token = np.random.choice(tokens, p=probs)\n",
        "            \n",
        "            if next_token == '<END>':\n",
        "                break\n",
        "                \n",
        "            generated.append(next_token)\n",
        "            \n",
        "            # Update context\n",
        "            context = context[1:] + [next_token]\n",
        "        \n",
        "        return ' '.join(generated)\n",
        "\n",
        "# Train different n-gram models\n",
        "unigram = NGramModel(n=1)\n",
        "unigram.train(tokens)\n",
        "\n",
        "bigram = NGramModel(n=2)\n",
        "bigram.train(tokens)\n",
        "\n",
        "trigram = NGramModel(n=3)\n",
        "trigram.train(tokens)\n",
        "\n",
        "# Train with Laplace smoothing\n",
        "bigram_smooth = NGramModel(n=2, smoothing='laplace')\n",
        "bigram_smooth.train(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with different models\n",
        "print(\"=== Text Generation Examples ===\")\n",
        "\n",
        "print(\"\\n1. UNIGRAM Model (random words):\")\n",
        "print(unigram.generate(30, temperature=1.0))\n",
        "\n",
        "print(\"\\n2. BIGRAM Model (pairs of words):\")\n",
        "print(bigram.generate(30, temperature=1.0))\n",
        "\n",
        "print(\"\\n3. TRIGRAM Model (triplets of words):\")\n",
        "print(trigram.generate(30, temperature=1.0))\n",
        "\n",
        "print(\"\\n4. BIGRAM with Smoothing:\")\n",
        "print(bigram_smooth.generate(30, temperature=1.0))\n",
        "\n",
        "print(\"\\n5. TRIGRAM with Low Temperature (more deterministic):\")\n",
        "print(trigram.generate(30, temperature=0.5))\n",
        "\n",
        "print(\"\\n6. TRIGRAM with High Temperature (more random):\")\n",
        "print(trigram.generate(30, temperature=2.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Perplexity Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_perplexity(model: NGramModel, test_tokens: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate perplexity of a model on test data\n",
        "    Lower perplexity = better model\n",
        "    \"\"\"\n",
        "    # Add boundary tokens\n",
        "    test_tokens = ['<START>'] * (model.n - 1) + test_tokens + ['<END>']\n",
        "    \n",
        "    log_prob_sum = 0\n",
        "    num_tokens = 0\n",
        "    \n",
        "    for i in range(len(test_tokens) - model.n + 1):\n",
        "        ngram = tuple(test_tokens[i:i+model.n])\n",
        "        prob = model.probability(ngram)\n",
        "        \n",
        "        if prob > 0:\n",
        "            log_prob_sum += math.log2(prob)\n",
        "        else:\n",
        "            # Handle zero probability (use small value)\n",
        "            log_prob_sum += math.log2(1e-10)\n",
        "        \n",
        "        num_tokens += 1\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    avg_log_prob = log_prob_sum / num_tokens\n",
        "    perplexity = 2 ** (-avg_log_prob)\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Split data for evaluation\n",
        "split_point = int(len(tokens) * 0.8)\n",
        "train_tokens = tokens[:split_point]\n",
        "test_tokens = tokens[split_point:]\n",
        "\n",
        "print(f\"Train set: {len(train_tokens)} tokens\")\n",
        "print(f\"Test set: {len(test_tokens)} tokens\\n\")\n",
        "\n",
        "# Retrain models on train set\n",
        "models = [\n",
        "    ('Unigram', NGramModel(n=1)),\n",
        "    ('Bigram', NGramModel(n=2)),\n",
        "    ('Trigram', NGramModel(n=3)),\n",
        "    ('Bigram + Laplace', NGramModel(n=2, smoothing='laplace')),\n",
        "    ('Trigram + Laplace', NGramModel(n=3, smoothing='laplace'))\n",
        "]\n",
        "\n",
        "perplexities = []\n",
        "for name, model in models:\n",
        "    model.train(train_tokens)\n",
        "    perp = calculate_perplexity(model, test_tokens)\n",
        "    perplexities.append((name, perp))\n",
        "    print(f\"{name:20} Perplexity: {perp:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize perplexity comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "names = [name for name, _ in perplexities]\n",
        "values = [perp for _, perp in perplexities]\n",
        "\n",
        "bars = ax.bar(names, values, color=['#FF6B6B', '#4ECDC4', '#95E77E', '#FFA07A', '#98D8C8'])\n",
        "ax.set_ylabel('Perplexity (lower is better)')\n",
        "ax.set_title('Model Comparison: Perplexity on Test Data')\n",
        "ax.set_ylim(0, max(values) * 1.1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Analyzing Model Behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_next_word_distribution(model: NGramModel, context: List[str], top_k: int = 10):\n",
        "    \"\"\"Get probability distribution for next word given context\"\"\"\n",
        "    # Adjust context length for model\n",
        "    if len(context) >= model.n - 1:\n",
        "        context = context[-(model.n-1):]\n",
        "    else:\n",
        "        context = ['<START>'] * (model.n - 1 - len(context)) + context\n",
        "    \n",
        "    # Get probabilities for all possible next words\n",
        "    next_word_probs = {}\n",
        "    \n",
        "    for ngram in model.ngram_counts:\n",
        "        if ngram[:-1] == tuple(context):\n",
        "            next_word = ngram[-1]\n",
        "            prob = model.probability(ngram)\n",
        "            next_word_probs[next_word] = prob\n",
        "    \n",
        "    # Sort by probability\n",
        "    sorted_probs = sorted(next_word_probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return sorted_probs[:top_k]\n",
        "\n",
        "# Test with different contexts\n",
        "test_contexts = [\n",
        "    ['alice', 'was'],\n",
        "    ['the', 'rabbit'],\n",
        "    ['she', 'had'],\n",
        "    ['white', 'rabbit']\n",
        "]\n",
        "\n",
        "print(\"=== Next Word Predictions ===\")\n",
        "\n",
        "for context in test_contexts:\n",
        "    print(f\"\\nContext: {' '.join(context)}\")\n",
        "    \n",
        "    for model_name, model in [('Bigram', bigram), ('Trigram', trigram)]:\n",
        "        predictions = get_next_word_distribution(model, context, top_k=5)\n",
        "        \n",
        "        if predictions:\n",
        "            print(f\"  {model_name} predictions:\")\n",
        "            for word, prob in predictions:\n",
        "                print(f\"    '{word}': {prob:.3f}\")\n",
        "        else:\n",
        "            print(f\"  {model_name}: No predictions (unseen context)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze most common n-grams\n",
        "def show_top_ngrams(model: NGramModel, k: int = 10):\n",
        "    \"\"\"Display most frequent n-grams\"\"\"\n",
        "    top_ngrams = sorted(model.ngram_counts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    \n",
        "    print(f\"\\nTop {k} {model.n}-grams:\")\n",
        "    for ngram, count in top_ngrams:\n",
        "        ngram_str = ' '.join(ngram)\n",
        "        print(f\"  '{ngram_str}': {count}\")\n",
        "    \n",
        "    return top_ngrams\n",
        "\n",
        "# Show top n-grams for each model\n",
        "for name, model in [('Unigram', unigram), ('Bigram', bigram), ('Trigram', trigram)]:\n",
        "    top = show_top_ngrams(model, k=8)\n",
        "    \n",
        "# Visualize bigram frequencies\n",
        "top_bigrams = show_top_ngrams(bigram, k=15)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "bigram_labels = [' '.join(bg[0]) for bg in top_bigrams]\n",
        "bigram_counts = [bg[1] for bg in top_bigrams]\n",
        "\n",
        "ax.barh(range(len(bigram_labels)), bigram_counts)\n",
        "ax.set_yticks(range(len(bigram_labels)))\n",
        "ax.set_yticklabels(bigram_labels)\n",
        "ax.set_xlabel('Frequency')\n",
        "ax.set_title('Most Common Bigrams')\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Advanced - Interpolation and Backoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InterpolatedNGramModel:\n",
        "    \"\"\"N-gram model with interpolation between different orders\"\"\"\n",
        "    \n",
        "    def __init__(self, max_n: int = 3, lambdas: List[float] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_n: Maximum n-gram order\n",
        "            lambdas: Interpolation weights (must sum to 1)\n",
        "        \"\"\"\n",
        "        self.max_n = max_n\n",
        "        self.models = [NGramModel(n=i, smoothing='laplace') for i in range(1, max_n+1)]\n",
        "        \n",
        "        if lambdas is None:\n",
        "            # Equal weights by default\n",
        "            self.lambdas = [1/max_n] * max_n\n",
        "        else:\n",
        "            assert len(lambdas) == max_n and abs(sum(lambdas) - 1.0) < 1e-6\n",
        "            self.lambdas = lambdas\n",
        "    \n",
        "    def train(self, tokens: List[str]):\n",
        "        \"\"\"Train all component models\"\"\"\n",
        "        for model in self.models:\n",
        "            model.train(tokens)\n",
        "        print(f\"Trained interpolated model with orders 1-{self.max_n}\")\n",
        "        print(f\"Interpolation weights: {self.lambdas}\")\n",
        "    \n",
        "    def probability(self, word: str, context: List[str]) -> float:\n",
        "        \"\"\"Calculate interpolated probability\"\"\"\n",
        "        prob = 0\n",
        "        \n",
        "        for i, model in enumerate(self.models):\n",
        "            n = i + 1\n",
        "            if n == 1:\n",
        "                # Unigram\n",
        "                ngram = (word,)\n",
        "            else:\n",
        "                # Use appropriate context length\n",
        "                context_len = min(n-1, len(context))\n",
        "                if context_len < n-1:\n",
        "                    # Pad with START tokens\n",
        "                    padded_context = ['<START>'] * (n-1-context_len) + context[-context_len:]\n",
        "                else:\n",
        "                    padded_context = context[-(n-1):]\n",
        "                ngram = tuple(padded_context) + (word,)\n",
        "            \n",
        "            prob += self.lambdas[i] * model.probability(ngram)\n",
        "        \n",
        "        return prob\n",
        "\n",
        "# Train interpolated model\n",
        "interpolated = InterpolatedNGramModel(max_n=3, lambdas=[0.1, 0.3, 0.6])\n",
        "interpolated.train(train_tokens)\n",
        "\n",
        "# Compare with individual models\n",
        "print(\"\\n=== Probability Comparison ===\")\n",
        "test_cases = [\n",
        "    (['alice'], 'was'),\n",
        "    (['the', 'white'], 'rabbit'),\n",
        "    (['she', 'had'], 'never')\n",
        "]\n",
        "\n",
        "for context, word in test_cases:\n",
        "    print(f\"\\nP({word} | {' '.join(context)}):\")\n",
        "    \n",
        "    # Individual models\n",
        "    for i, model in enumerate(interpolated.models):\n",
        "        n = i + 1\n",
        "        if n == 1:\n",
        "            ngram = (word,)\n",
        "        else:\n",
        "            context_for_model = context[-(n-1):] if len(context) >= n-1 else ['<START>'] * (n-1-len(context)) + context\n",
        "            ngram = tuple(context_for_model) + (word,)\n",
        "        \n",
        "        prob = model.probability(ngram)\n",
        "        print(f\"  {n}-gram: {prob:.4f}\")\n",
        "    \n",
        "    # Interpolated\n",
        "    interp_prob = interpolated.probability(word, context)\n",
        "    print(f\"  Interpolated: {interp_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Practical Exercise - Build Your Own Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise: Create your own specialized language model\n",
        "print(\"=== Exercise: Domain-Specific Language Model ===\")\n",
        "print(\"\\nTry creating a language model for a specific domain!\")\n",
        "print(\"Ideas:\")\n",
        "print(\"  1. Scientific abstracts\")\n",
        "print(\"  2. Recipe instructions\")\n",
        "print(\"  3. News headlines\")\n",
        "print(\"  4. Poetry\")\n",
        "print(\"  5. Technical documentation\")\n",
        "\n",
        "# Example: Simple recipe corpus\n",
        "recipe_corpus = \"\"\"Preheat oven to 350 degrees. Mix flour and sugar in bowl.\n",
        "Add eggs and milk. Stir until smooth. Pour batter into pan.\n",
        "Bake for 30 minutes. Let cool before serving.\n",
        "Preheat oven to 400 degrees. Season chicken with salt and pepper.\n",
        "Place in baking dish. Add vegetables around chicken.\n",
        "Bake for 45 minutes until golden. Serve hot with rice.\"\"\"\n",
        "\n",
        "# Train a model on recipe text\n",
        "recipe_tokens = preprocess_text(recipe_corpus)\n",
        "recipe_model = NGramModel(n=3, smoothing='laplace')\n",
        "recipe_model.train(recipe_tokens)\n",
        "\n",
        "print(\"\\n=== Recipe Language Model ===\")\n",
        "print(\"Generated recipe instructions:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n{i+1}. {recipe_model.generate(20, temperature=0.8)}\")\n",
        "\n",
        "# Show common patterns\n",
        "print(\"\\nCommon recipe phrases:\")\n",
        "show_top_ngrams(recipe_model, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Visualization Matching Presentation\n",
        "\n",
        "In this section, we'll recreate the key charts from the Week 1 presentation. This helps you understand how the visualizations were created and allows you to experiment with different parameters.\n",
        "\n",
        "### 9.1 N-gram Context Windows\n",
        "\n",
        "Let's visualize how unigram, bigram, and trigram models use different amounts of context."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Before/After Smoothing Comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Sample words following \"the\" \n",
        "test_context = ['the']\n",
        "sample_words = ['cat', 'dog', 'mat', 'rabbit', 'alice', 'book', 'UNSEEN1', 'UNSEEN2', 'UNSEEN3']\n",
        "\n",
        "# Get probabilities without smoothing\n",
        "probs_before = []\n",
        "for word in sample_words:\n",
        "    ngram = ('the', word)\n",
        "    prob = bigram.probability(ngram)  # No smoothing model\n",
        "    probs_before.append(prob)\n",
        "\n",
        "# Get probabilities with smoothing\n",
        "probs_after = []\n",
        "for word in sample_words:\n",
        "    ngram = ('the', word)\n",
        "    prob = bigram_smooth.probability(ngram)  # Laplace smoothing\n",
        "    probs_after.append(prob)\n",
        "\n",
        "# Before smoothing\n",
        "colors_before = ['#95E77E' if p > 0 else '#FF6B6B' for p in probs_before]\n",
        "ax1.bar(range(len(sample_words)), probs_before, color=colors_before, alpha=0.7,\n",
        "        edgecolor='#404040', linewidth=1.5)\n",
        "ax1.set_ylabel('Probability', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Before Smoothing (MLE)', fontsize=12, fontweight='bold')\n",
        "ax1.set_xticks(range(len(sample_words)))\n",
        "ax1.set_xticklabels(sample_words, rotation=45, ha='right')\n",
        "ax1.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "# Mark zeros\n",
        "for i, prob in enumerate(probs_before):\n",
        "    if prob == 0:\n",
        "        ax1.text(i, 0.0005, 'ZERO', ha='center', fontsize=8, color='red', fontweight='bold')\n",
        "\n",
        "# After smoothing  \n",
        "colors_after = ['#4ECDC4' if probs_before[i] > 0 else '#95E77E' for i in range(len(sample_words))]\n",
        "ax2.bar(range(len(sample_words)), probs_after, color=colors_after, alpha=0.7,\n",
        "        edgecolor='#404040', linewidth=1.5)\n",
        "ax2.set_ylabel('Probability', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('After Smoothing (Laplace)', fontsize=12, fontweight='bold')\n",
        "ax2.set_xticks(range(len(sample_words)))\n",
        "ax2.set_xticklabels(sample_words, rotation=45, ha='right')\n",
        "ax2.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "# Mark rescued probabilities\n",
        "for i, (prob_before, prob_after) in enumerate(zip(probs_before, probs_after)):\n",
        "    if prob_before == 0 and prob_after > 0:\n",
        "        ax2.text(i, prob_after, f'{prob_after:.4f}', ha='center', va='bottom',\n",
        "                fontsize=7, color='#95E77E', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(f\"  - Seen bigrams: Probability slightly reduced (mass stolen)\")\n",
        "print(f\"  - Unseen bigrams: Get small non-zero probability (mass received)\")\n",
        "print(f\"  - Total probability still sums to 1.0\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Before/After Smoothing Visualization\n",
        "\n",
        "See exactly how smoothing shifts probability mass."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different k values\n",
        "k_values = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
        "\n",
        "print(\"=== Effect of k Parameter on Perplexity ===\\n\")\n",
        "print(f\"{'k Value':<10} {'Perplexity':<15} {'Effect':<30}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "k_results = []\n",
        "for k in k_values:\n",
        "    model_k = NGramModel(n=2, smoothing='add-k', alpha=k)\n",
        "    model_k.train(train_tokens)\n",
        "    perp_k = calculate_perplexity(model_k, test_tokens)\n",
        "    k_results.append((k, perp_k))\n",
        "    \n",
        "    if k == 0.01:\n",
        "        effect = \"Often optimal for large corpora\"\n",
        "    elif k == 1.0:\n",
        "        effect = \"Laplace (too aggressive)\"\n",
        "    elif k < 0.01:\n",
        "        effect = \"Very light smoothing\"\n",
        "    else:\n",
        "        effect = \"Moderate smoothing\"\n",
        "    \n",
        "    print(f\"{k:<10.3f} {perp_k:<15.2f} {effect:<30}\")\n",
        "\n",
        "# Visualize effect of k\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ks = [k for k, _ in k_results]\n",
        "perps_k = [perp for _, perp in k_results]\n",
        "\n",
        "ax.plot(ks, perps_k, marker='o', markersize=10, linewidth=2.5, color='#3333B2')\n",
        "ax.axvline(x=0.01, color='#95E77E', linestyle='--', linewidth=2, alpha=0.7, label='Often optimal (k=0.01)')\n",
        "\n",
        "ax.set_xlabel('Smoothing Parameter k', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Perplexity', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Effect of Smoothing Parameter on Model Performance', fontsize=14, fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.grid(True, alpha=0.2)\n",
        "ax.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey insight: Smaller k usually better for large corpora!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Comparing k Values\n",
        "\n",
        "Experiment with different smoothing parameters to see the effect."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Probability redistribution visualization\n",
        "\n",
        "# Train a small bigram model\n",
        "small_corpus = \"the cat sat on the mat the dog\".split()\n",
        "test_model = NGramModel(n=2, smoothing='none')\n",
        "test_model.train(small_corpus)\n",
        "\n",
        "test_model_smooth = NGramModel(n=2, smoothing='add-k', alpha=0.1)\n",
        "test_model_smooth.train(small_corpus)\n",
        "\n",
        "# Compare probabilities for \"the\" followed by different words\n",
        "test_words = ['cat', 'dog', 'mat', 'xylophone', 'elephant', 'computer']\n",
        "\n",
        "print(\"=== Probability Comparison: P(word | 'the') ===\\n\")\n",
        "print(f\"{'Word':<15} {'No Smoothing':<18} {'With Smoothing (k=0.1)':<25}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for word in test_words:\n",
        "    prob_no_smooth = test_model.probability(('the', word))\n",
        "    prob_smooth = test_model_smooth.probability(('the', word))\n",
        "    \n",
        "    marker_no = \"*\" if prob_no_smooth == 0 else \" \"\n",
        "    marker_smooth = \"+\" if prob_smooth > 0 and prob_no_smooth == 0 else \" \"\n",
        "    \n",
        "    print(f\"{word:<15} {prob_no_smooth:<18.6f}{marker_no}  {prob_smooth:<25.6f}{marker_smooth}\")\n",
        "\n",
        "print(\"\\n* = Zero probability (unseen bigram)\")\n",
        "print(\"+ = Non-zero after smoothing (rescued from zero!)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 10: Smoothing Experiments\n",
        "\n",
        "Deep dive into smoothing techniques - implement and compare different methods.\n",
        "\n",
        "### 10.1 Probability Mass Redistribution\n",
        "\n",
        "Visualize how smoothing redistributes probability from seen to unseen events."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 4: Text Quality Progression\n",
        "# This recreates the concept from Slide 2\n",
        "\n",
        "print(\"=== Text Quality with Different N-gram Orders ===\\n\")\n",
        "\n",
        "# Generate text with each model\n",
        "models_for_quality = [\n",
        "    (\"1-gram (no context)\", unigram),\n",
        "    (\"2-gram (1 word)\", bigram),\n",
        "    (\"3-gram (2 words)\", trigram)\n",
        "]\n",
        "\n",
        "for i, (name, model) in enumerate(models_for_quality, 1):\n",
        "    print(f\"{i}. {name}:\")\n",
        "    generated = model.generate(20, temperature=0.8)\n",
        "    print(f\"   \\\"{generated}\\\"\")\n",
        "    print()\n",
        "\n",
        "print(\"Notice how text becomes more coherent with higher n!\")\n",
        "print(\"\\n1-gram: Random words, no grammar\")\n",
        "print(\"2-gram: Local coherence, some grammar\")  \n",
        "print(\"3-gram: Better grammar, more natural flow\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4 Text Quality Progression\n",
        "\n",
        "See how generated text improves with higher n-gram order (from Slide 2)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 3: Smoothing Methods Comparison\n",
        "# This recreates the chart from Slide 27\n",
        "\n",
        "# Train models with different smoothing\n",
        "smoothing_models = [\n",
        "    ('No Smoothing', NGramModel(n=2, smoothing='none')),\n",
        "    ('Add-1 (Laplace)', NGramModel(n=2, smoothing='laplace', alpha=1.0)),\n",
        "    ('Add-0.1', NGramModel(n=2, smoothing='add-k', alpha=0.1)),\n",
        "    ('Add-0.01', NGramModel(n=2, smoothing='add-k', alpha=0.01))\n",
        "]\n",
        "\n",
        "smoothing_perps = []\n",
        "for name, model in smoothing_models:\n",
        "    model.train(train_tokens)\n",
        "    perp = calculate_perplexity(model, test_tokens)\n",
        "    # Handle infinity for no smoothing (zero probabilities)\n",
        "    if perp > 10000:\n",
        "        perp = 9999  # Cap for visualization\n",
        "    smoothing_perps.append((name, perp))\n",
        "    print(f\"{name:20} Perplexity: {perp:.1f if perp < 1000 else 'Infinity (zeros)'}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 6))\n",
        "\n",
        "methods = [name for name, _ in smoothing_perps]\n",
        "perps = [perp for _, perp in smoothing_perps]\n",
        "\n",
        "colors = ['#CC0000', '#FF9999', '#4ECDC4', '#95E77E']\n",
        "bars = ax.bar(methods, perps, color=colors, alpha=0.7, edgecolor='#404040', linewidth=2)\n",
        "\n",
        "ax.set_ylabel('Perplexity (lower is better)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Smoothing Methods: Performance Comparison', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add value labels\n",
        "for i, (method, perp) in enumerate(smoothing_perps):\n",
        "    label = '∞' if perp > 1000 else f'{perp:.0f}'\n",
        "    ax.text(i, perp + 100 if perp < 1000 else 300, label, ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.grid(True, alpha=0.2, linestyle='--')\n",
        "plt.xticks(rotation=15, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey takeaway: Smoothing is essential - no smoothing gives infinite perplexity!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Smoothing Methods Comparison\n",
        "\n",
        "Compare different smoothing techniques (from Slide 27)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 2: Perplexity Comparison (from models already trained)\n",
        "# This recreates the chart from Slide 33\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Use the perplexities already calculated on test data\n",
        "n_gram_types = ['Unigram', 'Bigram', 'Trigram']\n",
        "n_gram_perps = [perplexities[0][1], perplexities[1][1], perplexities[2][1]]\n",
        "\n",
        "bars = ax.bar(n_gram_types, n_gram_perps, color='#3333B2', alpha=0.7,\n",
        "              edgecolor='#404040', linewidth=2)\n",
        "\n",
        "# Highlight trigram\n",
        "bars[2].set_color('#95E77E')\n",
        "\n",
        "ax.set_ylabel('Perplexity', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Perplexity vs N-gram Order (with smoothing)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add value labels and improvement percentages\n",
        "for i in range(len(n_gram_perps)):\n",
        "    ax.text(i, n_gram_perps[i] + 5, f'{n_gram_perps[i]:.1f}', ha='center',\n",
        "            fontsize=11, fontweight='bold')\n",
        "    \n",
        "    if i > 0:\n",
        "        improvement = ((n_gram_perps[i-1] - n_gram_perps[i]) / n_gram_perps[i-1]) * 100\n",
        "        ax.text(i, n_gram_perps[i] - 15, f'−{improvement:.0f}%', ha='center',\n",
        "                fontsize=9, color='#95E77E', fontweight='bold')\n",
        "\n",
        "plt.grid(True, alpha=0.2, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Perplexity improves as we increase context!\")\n",
        "print(f\"  Unigram → Bigram: {((n_gram_perps[0]-n_gram_perps[1])/n_gram_perps[0]*100):.1f}% improvement\")\n",
        "print(f\"  Bigram → Trigram: {((n_gram_perps[1]-n_gram_perps[2])/n_gram_perps[1]*100):.1f}% improvement\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Perplexity Comparison Across N-gram Orders\n",
        "\n",
        "Recreate the perplexity comparison chart from Slide 33."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 1: N-gram Context Windows Visualization\n",
        "# This recreates the chart from Slide 12 of the presentation\n",
        "\n",
        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
        "\n",
        "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "COLOR_CURRENT = '#FF6B6B'  # Red for current\n",
        "COLOR_CONTEXT = '#4ECDC4'  # Teal for context\n",
        "COLOR_PREDICT = '#95E77E'  # Green for prediction\n",
        "COLOR_NEUTRAL = '#E0E0E0'  # Gray for neutral\n",
        "\n",
        "# Unigram (n=1)\n",
        "ax = axes[0]\n",
        "ax.set_title(\"Unigram (n=1): No Context\", fontsize=12, fontweight='bold')\n",
        "ax.set_xlim(-0.5, 6.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.axis('off')\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "    color = COLOR_CURRENT if i == 2 else COLOR_NEUTRAL\n",
        "    rect = FancyBboxPatch((i-0.4, 0), 0.8, 0.8, boxstyle=\"round,pad=0.08\",\n",
        "                          facecolor=color, edgecolor='black', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(i, 0.4, word, ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.text(3, -0.3, \"Focus: 'sat' (P(sat) only)\", ha='center', fontsize=9)\n",
        "\n",
        "# Bigram (n=2)\n",
        "ax = axes[1]\n",
        "ax.set_title(\"Bigram (n=2): Previous Word\", fontsize=12, fontweight='bold')\n",
        "ax.set_xlim(-0.5, 6.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.axis('off')\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "    if i == 1:\n",
        "        color = COLOR_CONTEXT\n",
        "    elif i == 2:\n",
        "        color = COLOR_PREDICT\n",
        "    else:\n",
        "        color = COLOR_NEUTRAL\n",
        "    rect = FancyBboxPatch((i-0.4, 0), 0.8, 0.8, boxstyle=\"round,pad=0.08\",\n",
        "                          facecolor=color, edgecolor='black', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(i, 0.4, word, ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "arrow = FancyArrowPatch((1.5, 0.4), (2.5, 0.4), arrowstyle='->', mutation_scale=15,\n",
        "                       linewidth=2, color='black')\n",
        "ax.add_patch(arrow)\n",
        "ax.text(2, -0.3, \"P(sat | cat)\", ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Trigram (n=3)\n",
        "ax = axes[2]\n",
        "ax.set_title(\"Trigram (n=3): Two Previous Words\", fontsize=12, fontweight='bold')\n",
        "ax.set_xlim(-0.5, 6.5)\n",
        "ax.set_ylim(-0.5, 1.5)\n",
        "ax.axis('off')\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "    if i in [0, 1]:\n",
        "        color = COLOR_CONTEXT\n",
        "    elif i == 2:\n",
        "        color = COLOR_PREDICT\n",
        "    else:\n",
        "        color = COLOR_NEUTRAL\n",
        "    rect = FancyBboxPatch((i-0.4, 0), 0.8, 0.8, boxstyle=\"round,pad=0.08\",\n",
        "                          facecolor=color, edgecolor='black', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(i, 0.4, word, ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "for start in [0.5, 1.5]:\n",
        "    arrow = FancyArrowPatch((start, 0.4), (2.5, 0.4), arrowstyle='->',\n",
        "                           mutation_scale=15, linewidth=2, color='black', alpha=0.7)\n",
        "    ax.add_patch(arrow)\n",
        "\n",
        "ax.text(2, -0.3, \"P(sat | The, cat)\", ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"This chart shows how different n-gram models use different amounts of context.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We've Learned:\n",
        "\n",
        "1. **N-gram Models**: Statistical approach to language modeling based on sequences of n tokens\n",
        "\n",
        "2. **Perplexity**: Metric for evaluating language models (lower is better)\n",
        "   - Measures how \"surprised\" the model is by test data\n",
        "   - Related to cross-entropy\n",
        "\n",
        "3. **Trade-offs**:\n",
        "   - **Unigrams**: Simple but ignore context\n",
        "   - **Bigrams**: Capture local dependencies\n",
        "   - **Trigrams**: Better context but sparser data\n",
        "   - **Higher-order**: Diminishing returns, data sparsity\n",
        "\n",
        "4. **Smoothing Techniques**:\n",
        "   - **Laplace (Add-1)**: Simple but can over-smooth\n",
        "   - **Add-k**: Tunable smoothing parameter\n",
        "   - **Interpolation**: Combine different n-gram orders\n",
        "\n",
        "5. **Limitations of N-grams**:\n",
        "   - Fixed context window\n",
        "   - Exponential growth in parameters\n",
        "   - No semantic understanding\n",
        "   - Can't capture long-range dependencies\n",
        "\n",
        "### Next Steps:\n",
        "- Week 2: Word embeddings to capture semantic meaning\n",
        "- Week 3: RNNs for variable-length contexts\n",
        "- Week 4: Attention mechanisms for long-range dependencies\n",
        "- Week 5: Transformers - the modern approach!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Experiment with corpus size**: How does perplexity change with more training data?\n",
        "\n",
        "2. **Compare smoothing techniques**: Implement Good-Turing or Kneser-Ney smoothing\n",
        "\n",
        "3. **Character-level models**: Build n-gram models at the character level\n",
        "\n",
        "4. **Cross-domain evaluation**: Train on one domain, test on another\n",
        "\n",
        "5. **Optimize interpolation weights**: Use held-out data to find optimal lambdas\n",
        "\n",
        "6. **Build a spell checker**: Use n-grams to detect and correct typos\n",
        "\n",
        "7. **Language identification**: Use character n-grams to identify languages"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}