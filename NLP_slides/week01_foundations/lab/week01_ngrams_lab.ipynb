{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Lab: N-grams and Statistical Language Models\n",
    "\n",
    "## Learning Objectives\n",
    "- Build n-gram language models from scratch\n",
    "- Calculate perplexity to evaluate model quality\n",
    "- Generate text using different n-gram models\n",
    "- Understand smoothing techniques\n",
    "- Compare unigram, bigram, and trigram models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus (Alice in Wonderland excerpt)\n",
    "sample_text = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank,\n",
    "and of having nothing to do. Once or twice she had peeped into the book her sister was reading,\n",
    "but it had no pictures or conversations in it. And what is the use of a book, thought Alice,\n",
    "without pictures or conversations?\n",
    "\n",
    "So she was considering in her own mind, as well as she could, for the hot day made her feel\n",
    "very sleepy and stupid, whether the pleasure of making a daisy chain would be worth the trouble\n",
    "of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
    "\n",
    "There was nothing so very remarkable in that. Alice did not even think it so very much out of\n",
    "the way to hear the Rabbit say to itself, Oh dear! Oh dear! I shall be late! But when the\n",
    "Rabbit actually took a watch out of its waistcoat pocket, and looked at it, and then hurried on,\n",
    "Alice started to her feet, for it flashed across her mind that she had never before seen a\n",
    "rabbit with either a waistcoat pocket, or a watch to take out of it.\"\"\"\n",
    "\n",
    "print(f\"Corpus length: {len(sample_text)} characters\")\n",
    "print(f\"First 200 characters:\\n{sample_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    \"\"\"Preprocess text into list of tokens\"\"\"\n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Replace newlines with spaces\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Add spaces around punctuation\n",
    "    text = re.sub(r'([.!?,;])', r' \\1 ', text)\n",
    "    \n",
    "    # Split into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess the corpus\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print(f\"\\nFirst 20 tokens:\\n{tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token frequencies\n",
    "token_freq = Counter(tokens)\n",
    "print(\"Most common tokens:\")\n",
    "for token, count in token_freq.most_common(15):\n",
    "    print(f\"  '{token}': {count}\")\n",
    "\n",
    "# Plot token frequency distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot top 20 most common tokens\n",
    "top_tokens = token_freq.most_common(20)\n",
    "ax1.bar(range(len(top_tokens)), [count for _, count in top_tokens])\n",
    "ax1.set_xticks(range(len(top_tokens)))\n",
    "ax1.set_xticklabels([token for token, _ in top_tokens], rotation=45, ha='right')\n",
    "ax1.set_xlabel('Token')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Top 20 Most Frequent Tokens')\n",
    "\n",
    "# Plot Zipf's law\n",
    "frequencies = sorted(token_freq.values(), reverse=True)\n",
    "ax2.loglog(range(1, len(frequencies)+1), frequencies, 'b-', alpha=0.6)\n",
    "ax2.set_xlabel('Token Rank (log scale)')\n",
    "ax2.set_ylabel('Frequency (log scale)')\n",
    "ax2.set_title(\"Zipf's Law Distribution\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, n: int, smoothing: str = 'none', alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        N-gram language model\n",
    "        \n",
    "        Args:\n",
    "            n: Order of n-gram (1=unigram, 2=bigram, 3=trigram, etc.)\n",
    "            smoothing: Smoothing technique ('none', 'laplace', 'add-k')\n",
    "            alpha: Smoothing parameter for add-k smoothing\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.alpha = alpha\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "    def train(self, tokens: List[str]):\n",
    "        \"\"\"Train the n-gram model on tokenized text\"\"\"\n",
    "        # Add special tokens for sentence boundaries\n",
    "        tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.vocabulary = set(tokens)\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.total_tokens = len(tokens)\n",
    "        \n",
    "        # Count n-grams\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i+self.n])\n",
    "            context = ngram[:-1]\n",
    "            \n",
    "            self.ngram_counts[ngram] += 1\n",
    "            self.context_counts[context] += 1\n",
    "        \n",
    "        print(f\"Trained {self.n}-gram model:\")\n",
    "        print(f\"  Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"  Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "        \n",
    "    def probability(self, ngram: Tuple[str, ...]) -> float:\n",
    "        \"\"\"Calculate probability of an n-gram\"\"\"\n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        if self.n == 1:\n",
    "            # Unigram model\n",
    "            count = self.ngram_counts[ngram]\n",
    "            if self.smoothing == 'laplace' or self.smoothing == 'add-k':\n",
    "                return (count + self.alpha) / (self.total_tokens + self.alpha * self.vocab_size)\n",
    "            else:\n",
    "                return count / self.total_tokens if self.total_tokens > 0 else 0\n",
    "        else:\n",
    "            # N-gram model (n > 1)\n",
    "            ngram_count = self.ngram_counts[ngram]\n",
    "            context_count = self.context_counts[context]\n",
    "            \n",
    "            if self.smoothing == 'laplace' or self.smoothing == 'add-k':\n",
    "                return (ngram_count + self.alpha) / (context_count + self.alpha * self.vocab_size)\n",
    "            else:\n",
    "                return ngram_count / context_count if context_count > 0 else 0\n",
    "    \n",
    "    def generate(self, num_tokens: int = 50, temperature: float = 1.0) -> str:\n",
    "        \"\"\"Generate text using the n-gram model\"\"\"\n",
    "        # Start with initial context\n",
    "        context = ['<START>'] * (self.n - 1)\n",
    "        generated = []\n",
    "        \n",
    "        for _ in range(num_tokens):\n",
    "            # Get possible next tokens\n",
    "            candidates = defaultdict(float)\n",
    "            \n",
    "            for ngram in self.ngram_counts:\n",
    "                if ngram[:-1] == tuple(context):\n",
    "                    next_token = ngram[-1]\n",
    "                    prob = self.probability(ngram)\n",
    "                    candidates[next_token] = prob ** (1/temperature) if prob > 0 else 0\n",
    "            \n",
    "            if not candidates or all(p == 0 for p in candidates.values()):\n",
    "                # Fallback to random token\n",
    "                next_token = random.choice(list(self.vocabulary - {'<START>', '<END>'}))\n",
    "            else:\n",
    "                # Sample from distribution\n",
    "                tokens = list(candidates.keys())\n",
    "                probs = np.array(list(candidates.values()))\n",
    "                probs = probs / probs.sum()\n",
    "                next_token = np.random.choice(tokens, p=probs)\n",
    "            \n",
    "            if next_token == '<END>':\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Update context\n",
    "            context = context[1:] + [next_token]\n",
    "        \n",
    "        return ' '.join(generated)\n",
    "\n",
    "# Train different n-gram models\n",
    "unigram = NGramModel(n=1)\n",
    "unigram.train(tokens)\n",
    "\n",
    "bigram = NGramModel(n=2)\n",
    "bigram.train(tokens)\n",
    "\n",
    "trigram = NGramModel(n=3)\n",
    "trigram.train(tokens)\n",
    "\n",
    "# Train with Laplace smoothing\n",
    "bigram_smooth = NGramModel(n=2, smoothing='laplace')\n",
    "bigram_smooth.train(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different models\n",
    "print(\"=== Text Generation Examples ===\")\n",
    "\n",
    "print(\"\\n1. UNIGRAM Model (random words):\")\n",
    "print(unigram.generate(30, temperature=1.0))\n",
    "\n",
    "print(\"\\n2. BIGRAM Model (pairs of words):\")\n",
    "print(bigram.generate(30, temperature=1.0))\n",
    "\n",
    "print(\"\\n3. TRIGRAM Model (triplets of words):\")\n",
    "print(trigram.generate(30, temperature=1.0))\n",
    "\n",
    "print(\"\\n4. BIGRAM with Smoothing:\")\n",
    "print(bigram_smooth.generate(30, temperature=1.0))\n",
    "\n",
    "print(\"\\n5. TRIGRAM with Low Temperature (more deterministic):\")\n",
    "print(trigram.generate(30, temperature=0.5))\n",
    "\n",
    "print(\"\\n6. TRIGRAM with High Temperature (more random):\")\n",
    "print(trigram.generate(30, temperature=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model: NGramModel, test_tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a model on test data\n",
    "    Lower perplexity = better model\n",
    "    \"\"\"\n",
    "    # Add boundary tokens\n",
    "    test_tokens = ['<START>'] * (model.n - 1) + test_tokens + ['<END>']\n",
    "    \n",
    "    log_prob_sum = 0\n",
    "    num_tokens = 0\n",
    "    \n",
    "    for i in range(len(test_tokens) - model.n + 1):\n",
    "        ngram = tuple(test_tokens[i:i+model.n])\n",
    "        prob = model.probability(ngram)\n",
    "        \n",
    "        if prob > 0:\n",
    "            log_prob_sum += math.log2(prob)\n",
    "        else:\n",
    "            # Handle zero probability (use small value)\n",
    "            log_prob_sum += math.log2(1e-10)\n",
    "        \n",
    "        num_tokens += 1\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_log_prob = log_prob_sum / num_tokens\n",
    "    perplexity = 2 ** (-avg_log_prob)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Split data for evaluation\n",
    "split_point = int(len(tokens) * 0.8)\n",
    "train_tokens = tokens[:split_point]\n",
    "test_tokens = tokens[split_point:]\n",
    "\n",
    "print(f\"Train set: {len(train_tokens)} tokens\")\n",
    "print(f\"Test set: {len(test_tokens)} tokens\\n\")\n",
    "\n",
    "# Retrain models on train set\n",
    "models = [\n",
    "    ('Unigram', NGramModel(n=1)),\n",
    "    ('Bigram', NGramModel(n=2)),\n",
    "    ('Trigram', NGramModel(n=3)),\n",
    "    ('Bigram + Laplace', NGramModel(n=2, smoothing='laplace')),\n",
    "    ('Trigram + Laplace', NGramModel(n=3, smoothing='laplace'))\n",
    "]\n",
    "\n",
    "perplexities = []\n",
    "for name, model in models:\n",
    "    model.train(train_tokens)\n",
    "    perp = calculate_perplexity(model, test_tokens)\n",
    "    perplexities.append((name, perp))\n",
    "    print(f\"{name:20} Perplexity: {perp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize perplexity comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "names = [name for name, _ in perplexities]\n",
    "values = [perp for _, perp in perplexities]\n",
    "\n",
    "bars = ax.bar(names, values, color=['#FF6B6B', '#4ECDC4', '#95E77E', '#FFA07A', '#98D8C8'])\n",
    "ax.set_ylabel('Perplexity (lower is better)')\n",
    "ax.set_title('Model Comparison: Perplexity on Test Data')\n",
    "ax.set_ylim(0, max(values) * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analyzing Model Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word_distribution(model: NGramModel, context: List[str], top_k: int = 10):\n",
    "    \"\"\"Get probability distribution for next word given context\"\"\"\n",
    "    # Adjust context length for model\n",
    "    if len(context) >= model.n - 1:\n",
    "        context = context[-(model.n-1):]\n",
    "    else:\n",
    "        context = ['<START>'] * (model.n - 1 - len(context)) + context\n",
    "    \n",
    "    # Get probabilities for all possible next words\n",
    "    next_word_probs = {}\n",
    "    \n",
    "    for ngram in model.ngram_counts:\n",
    "        if ngram[:-1] == tuple(context):\n",
    "            next_word = ngram[-1]\n",
    "            prob = model.probability(ngram)\n",
    "            next_word_probs[next_word] = prob\n",
    "    \n",
    "    # Sort by probability\n",
    "    sorted_probs = sorted(next_word_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_probs[:top_k]\n",
    "\n",
    "# Test with different contexts\n",
    "test_contexts = [\n",
    "    ['alice', 'was'],\n",
    "    ['the', 'rabbit'],\n",
    "    ['she', 'had'],\n",
    "    ['white', 'rabbit']\n",
    "]\n",
    "\n",
    "print(\"=== Next Word Predictions ===\")\n",
    "\n",
    "for context in test_contexts:\n",
    "    print(f\"\\nContext: {' '.join(context)}\")\n",
    "    \n",
    "    for model_name, model in [('Bigram', bigram), ('Trigram', trigram)]:\n",
    "        predictions = get_next_word_distribution(model, context, top_k=5)\n",
    "        \n",
    "        if predictions:\n",
    "            print(f\"  {model_name} predictions:\")\n",
    "            for word, prob in predictions:\n",
    "                print(f\"    '{word}': {prob:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {model_name}: No predictions (unseen context)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most common n-grams\n",
    "def show_top_ngrams(model: NGramModel, k: int = 10):\n",
    "    \"\"\"Display most frequent n-grams\"\"\"\n",
    "    top_ngrams = sorted(model.ngram_counts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    print(f\"\\nTop {k} {model.n}-grams:\")\n",
    "    for ngram, count in top_ngrams:\n",
    "        ngram_str = ' '.join(ngram)\n",
    "        print(f\"  '{ngram_str}': {count}\")\n",
    "    \n",
    "    return top_ngrams\n",
    "\n",
    "# Show top n-grams for each model\n",
    "for name, model in [('Unigram', unigram), ('Bigram', bigram), ('Trigram', trigram)]:\n",
    "    top = show_top_ngrams(model, k=8)\n",
    "    \n",
    "# Visualize bigram frequencies\n",
    "top_bigrams = show_top_ngrams(bigram, k=15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bigram_labels = [' '.join(bg[0]) for bg in top_bigrams]\n",
    "bigram_counts = [bg[1] for bg in top_bigrams]\n",
    "\n",
    "ax.barh(range(len(bigram_labels)), bigram_counts)\n",
    "ax.set_yticks(range(len(bigram_labels)))\n",
    "ax.set_yticklabels(bigram_labels)\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Most Common Bigrams')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Advanced - Interpolation and Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedNGramModel:\n",
    "    \"\"\"N-gram model with interpolation between different orders\"\"\"\n",
    "    \n",
    "    def __init__(self, max_n: int = 3, lambdas: List[float] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_n: Maximum n-gram order\n",
    "            lambdas: Interpolation weights (must sum to 1)\n",
    "        \"\"\"\n",
    "        self.max_n = max_n\n",
    "        self.models = [NGramModel(n=i, smoothing='laplace') for i in range(1, max_n+1)]\n",
    "        \n",
    "        if lambdas is None:\n",
    "            # Equal weights by default\n",
    "            self.lambdas = [1/max_n] * max_n\n",
    "        else:\n",
    "            assert len(lambdas) == max_n and abs(sum(lambdas) - 1.0) < 1e-6\n",
    "            self.lambdas = lambdas\n",
    "    \n",
    "    def train(self, tokens: List[str]):\n",
    "        \"\"\"Train all component models\"\"\"\n",
    "        for model in self.models:\n",
    "            model.train(tokens)\n",
    "        print(f\"Trained interpolated model with orders 1-{self.max_n}\")\n",
    "        print(f\"Interpolation weights: {self.lambdas}\")\n",
    "    \n",
    "    def probability(self, word: str, context: List[str]) -> float:\n",
    "        \"\"\"Calculate interpolated probability\"\"\"\n",
    "        prob = 0\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            n = i + 1\n",
    "            if n == 1:\n",
    "                # Unigram\n",
    "                ngram = (word,)\n",
    "            else:\n",
    "                # Use appropriate context length\n",
    "                context_len = min(n-1, len(context))\n",
    "                if context_len < n-1:\n",
    "                    # Pad with START tokens\n",
    "                    padded_context = ['<START>'] * (n-1-context_len) + context[-context_len:]\n",
    "                else:\n",
    "                    padded_context = context[-(n-1):]\n",
    "                ngram = tuple(padded_context) + (word,)\n",
    "            \n",
    "            prob += self.lambdas[i] * model.probability(ngram)\n",
    "        \n",
    "        return prob\n",
    "\n",
    "# Train interpolated model\n",
    "interpolated = InterpolatedNGramModel(max_n=3, lambdas=[0.1, 0.3, 0.6])\n",
    "interpolated.train(train_tokens)\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\n=== Probability Comparison ===\")\n",
    "test_cases = [\n",
    "    (['alice'], 'was'),\n",
    "    (['the', 'white'], 'rabbit'),\n",
    "    (['she', 'had'], 'never')\n",
    "]\n",
    "\n",
    "for context, word in test_cases:\n",
    "    print(f\"\\nP({word} | {' '.join(context)}):\")\n",
    "    \n",
    "    # Individual models\n",
    "    for i, model in enumerate(interpolated.models):\n",
    "        n = i + 1\n",
    "        if n == 1:\n",
    "            ngram = (word,)\n",
    "        else:\n",
    "            context_for_model = context[-(n-1):] if len(context) >= n-1 else ['<START>'] * (n-1-len(context)) + context\n",
    "            ngram = tuple(context_for_model) + (word,)\n",
    "        \n",
    "        prob = model.probability(ngram)\n",
    "        print(f\"  {n}-gram: {prob:.4f}\")\n",
    "    \n",
    "    # Interpolated\n",
    "    interp_prob = interpolated.probability(word, context)\n",
    "    print(f\"  Interpolated: {interp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercise - Build Your Own Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create your own specialized language model\n",
    "print(\"=== Exercise: Domain-Specific Language Model ===\")\n",
    "print(\"\\nTry creating a language model for a specific domain!\")\n",
    "print(\"Ideas:\")\n",
    "print(\"  1. Scientific abstracts\")\n",
    "print(\"  2. Recipe instructions\")\n",
    "print(\"  3. News headlines\")\n",
    "print(\"  4. Poetry\")\n",
    "print(\"  5. Technical documentation\")\n",
    "\n",
    "# Example: Simple recipe corpus\n",
    "recipe_corpus = \"\"\"Preheat oven to 350 degrees. Mix flour and sugar in bowl.\n",
    "Add eggs and milk. Stir until smooth. Pour batter into pan.\n",
    "Bake for 30 minutes. Let cool before serving.\n",
    "Preheat oven to 400 degrees. Season chicken with salt and pepper.\n",
    "Place in baking dish. Add vegetables around chicken.\n",
    "Bake for 45 minutes until golden. Serve hot with rice.\"\"\"\n",
    "\n",
    "# Train a model on recipe text\n",
    "recipe_tokens = preprocess_text(recipe_corpus)\n",
    "recipe_model = NGramModel(n=3, smoothing='laplace')\n",
    "recipe_model.train(recipe_tokens)\n",
    "\n",
    "print(\"\\n=== Recipe Language Model ===\")\n",
    "print(\"Generated recipe instructions:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. {recipe_model.generate(20, temperature=0.8)}\")\n",
    "\n",
    "# Show common patterns\n",
    "print(\"\\nCommon recipe phrases:\")\n",
    "show_top_ngrams(recipe_model, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **N-gram Models**: Statistical approach to language modeling based on sequences of n tokens\n",
    "\n",
    "2. **Perplexity**: Metric for evaluating language models (lower is better)\n",
    "   - Measures how \"surprised\" the model is by test data\n",
    "   - Related to cross-entropy\n",
    "\n",
    "3. **Trade-offs**:\n",
    "   - **Unigrams**: Simple but ignore context\n",
    "   - **Bigrams**: Capture local dependencies\n",
    "   - **Trigrams**: Better context but sparser data\n",
    "   - **Higher-order**: Diminishing returns, data sparsity\n",
    "\n",
    "4. **Smoothing Techniques**:\n",
    "   - **Laplace (Add-1)**: Simple but can over-smooth\n",
    "   - **Add-k**: Tunable smoothing parameter\n",
    "   - **Interpolation**: Combine different n-gram orders\n",
    "\n",
    "5. **Limitations of N-grams**:\n",
    "   - Fixed context window\n",
    "   - Exponential growth in parameters\n",
    "   - No semantic understanding\n",
    "   - Can't capture long-range dependencies\n",
    "\n",
    "### Next Steps:\n",
    "- Week 2: Word embeddings to capture semantic meaning\n",
    "- Week 3: RNNs for variable-length contexts\n",
    "- Week 4: Attention mechanisms for long-range dependencies\n",
    "- Week 5: Transformers - the modern approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Experiment with corpus size**: How does perplexity change with more training data?\n",
    "\n",
    "2. **Compare smoothing techniques**: Implement Good-Turing or Kneser-Ney smoothing\n",
    "\n",
    "3. **Character-level models**: Build n-gram models at the character level\n",
    "\n",
    "4. **Cross-domain evaluation**: Train on one domain, test on another\n",
    "\n",
    "5. **Optimize interpolation weights**: Use held-out data to find optimal lambdas\n",
    "\n",
    "6. **Build a spell checker**: Use n-grams to detect and correct typos\n",
    "\n",
    "7. **Language identification**: Use character n-grams to identify languages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}