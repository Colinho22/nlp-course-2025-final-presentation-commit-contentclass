\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{array}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{positioning,arrows.meta,shapes}

% Custom commands
\newcommand{\highlight}[1]{\textbf{#1}}
\newcommand{\answer}[1]{\textcolor{blue}{\textit{#1}}}
\newcommand{\teaching}[1]{\textcolor{red}{\textbf{Teaching Note: }#1}}

% Box for exercises
\newtcolorbox{exercise}[1][]{
    colback=blue!5!white,
    colframe=blue!75!black,
    title=#1,
    fonttitle=\bfseries
}

\newtcolorbox{discovery}[1][]{
    colback=yellow!5!white,
    colframe=orange!75!black,
    title=Discovery Moment,
    fonttitle=\bfseries
}

\newtcolorbox{think}[1][]{
    colback=green!5!white,
    colframe=green!75!black,
    title=Think About It,
    fonttitle=\bfseries
}

\newtcolorbox{teachingnote}[1][]{
    colback=red!5!white,
    colframe=red!75!black,
    title=Teaching Note,
    fonttitle=\bfseries
}

\title{\textbf{Week 3: Teaching Networks to Remember}\\
\large Discovering RNNs, LSTMs, and the Memory Problem\\
\large Pre-Lab Exercise (No Programming Required)\\
\textcolor{red}{\textbf{INSTRUCTOR VERSION WITH ANSWER KEY}}}
\author{NLP Course 2025}
\date{}

\begin{document}
\maketitle

\noindent\textbf{Time:} 40 minutes\\
\textbf{Objective:} Discover why networks need memory and how gates solve the vanishing gradient problem.

\begin{teachingnote}
This handout builds intuition for RNNs through discovery. Students don't know these architectures yet. Let them struggle with the memory problem before revealing solutions. The "aha!" moment when they realize they've invented LSTM gates is powerful.
\end{teachingnote}

\section*{Part 1: The Context Problem (10 minutes)}

\begin{exercise}[Why Order Matters]
\textbf{Task 1: Same words, different meanings}

Rearrange these words to create two sentences with opposite meanings:
\begin{center}
\{dog, bites, man\}
\end{center}

Sentence 1: \answer{Dog bites man (common event)}

Sentence 2: \answer{Man bites dog (newsworthy event)}

\teaching{This simple example shows that word order fundamentally changes meaning - something Word2Vec cannot capture.}

\vspace{0.5em}
\textbf{Task 2: Context tracking}

Read this sentence word by word and track what you remember:

"The student who studied hard..."

\begin{enumerate}
    \item After "The": I expect \answer{a noun/person/thing}
    \item After "student": I remember \answer{the subject is "student" (singular)}
    \item After "who": I expect \answer{a description/action about the student}
    \item After "studied hard": I'm waiting for \answer{what happened to the student / main verb}
\end{enumerate}

\textbf{Task 3: What information are you maintaining?}

List three things your brain tracks while reading:
\begin{itemize}
    \item \answer{Subject of the sentence (who/what)}
    \item \answer{Tense and number agreement (singular/plural, past/present)}
    \item \answer{Incomplete dependencies (waiting for main verb)}
\end{itemize}

\teaching{Students should realize they naturally maintain multiple types of information - this motivates why simple feedforward networks fail.}
\end{exercise}

\begin{think}
Word embeddings (Week 2) treat "dog bites man" and "man bites dog" as identical - they have the same word vectors. How can we teach networks that order matters?
\end{think}

\section*{Part 2: Building Memory - Discovering Hidden States (10 minutes)}

\begin{exercise}[Designing Memory for Networks]
\textbf{Scenario:} You're teaching a computer to read "The cat sat on the mat" word by word.

\textbf{Task 1: Design your memory system}

The computer can only see one word at a time. How would you help it remember previous words?

Your design: \answer{Keep a summary/compressed representation of what we've seen so far. Update this summary with each new word. This summary should influence how we process the next word.}

\teaching{Look for students who suggest: keeping all previous words (too much memory), keeping just the last word (not enough context), or some form of summary (correct intuition).}

\textbf{Task 2: Memory updates}

Fill in how memory should update at each step:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Current Word} & \textbf{Previous Memory} & \textbf{New Memory Should Contain} \\
\hline
"The" & (empty) & \answer{"The" / article / expect noun} \\
\hline
"cat" & "The" & \answer{"The cat" / subject identified} \\
\hline
"sat" & "The cat" & \answer{"The cat sat" / subject + action} \\
\hline
\end{tabular}
\end{center}

\textbf{Task 3: Mathematical pattern}

The pattern you discovered can be written as:

New Memory = Function(Current Word, Previous Memory)

Or mathematically: $h_t = f(x_t, h_{t-1})$

What are the inputs to this function?
\begin{itemize}
    \item Input 1: \answer{Current word ($x_t$)}
    \item Input 2: \answer{Previous memory/hidden state ($h_{t-1}$)}
\end{itemize}

What is the output? \answer{Updated memory/hidden state ($h_t$) that combines both inputs}
\end{exercise}

\begin{discovery}
Congratulations! You've just invented the core idea of RNNs - using hidden states ($h_t$) to maintain memory of previous inputs!
\end{discovery}

\teaching{This is the key conceptual breakthrough. Make sure everyone understands that $h_t$ is a compressed representation of the entire history up to time $t$.}

\section*{Part 3: When Memory Fails - The Vanishing Gradient Problem (10 minutes)}

\begin{exercise}[Long-Distance Dependencies]
\textbf{Task 1: The forgetting problem}

Try to complete this sentence:
"The keys that I left on the table in the kitchen yesterday before going to work \answer{are} missing."

What word did you need to remember? \answer{"keys" (plural)}

How many words back was it? \answer{13-14 words}

\teaching{This demonstrates that agreement (are vs is) requires long-distance memory. Simple RNNs fail at this distance.}

\textbf{Task 2: Memory decay simulation}

Imagine your memory fades by 10\% at each word. Starting with strength 1.0:

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Memory Strength} \\
\hline
Start & 1.0 & 1.0 \\
\hline
After 1 word & $1.0 \times 0.9$ & 0.9 \\
\hline
After 2 words & $0.9 \times 0.9$ & \answer{0.81} \\
\hline
After 5 words & $0.9^5$ & \answer{0.59} \\
\hline
After 10 words & $0.9^{10}$ & \answer{0.35} \\
\hline
After 20 words & $0.9^{20}$ & \answer{0.12} \\
\hline
\end{tabular}
\end{center}

\textbf{Task 3: The problem}

At what point does memory become effectively zero (< 0.01)? \answer{After ~44 words ($0.9^{44} \approx 0.01$)}

This is why RNNs can't handle long sequences - the gradient (learning signal) vanishes!

\teaching{Key insight: This isn't just about forward pass memory - it's about the gradient during backprop. If gradient = 0, no learning happens for early words.}
\end{exercise}

\begin{think}
If memory decays exponentially, how can we preserve important information for longer?
\end{think}

\section*{Part 4: The Gate Solution - Selective Memory (10 minutes)}

\begin{exercise}[Designing Gates]
\textbf{Scenario:} You're managing your email inbox. You have three actions:
\begin{itemize}
    \item Delete (forget) spam
    \item Save (input) important emails
    \item Reply to (output) urgent emails
\end{itemize}

\textbf{Task 1: Gate design}

For the sentence "The cat that was black sat on the mat", decide what to do with each word:

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Word} & \textbf{Forget?} & \textbf{Save?} & \textbf{Use Now?} \\
 & (0=forget all, 1=keep all) & (0=ignore, 1=save) & (0=hide, 1=use) \\
\hline
"The" & 0.5 & 0.3 & 0.2 \\
\hline
"cat" & \answer{0.9} & \answer{0.9} & \answer{0.8} \\
\hline
"that" & \answer{0.7} & \answer{0.2} & \answer{0.1} \\
\hline
"was" & \answer{0.7} & \answer{0.2} & \answer{0.1} \\
\hline
"black" & \answer{0.8} & \answer{0.5} & \answer{0.3} \\
\hline
"sat" & \answer{0.9} & \answer{0.9} & \answer{0.9} \\
\hline
\end{tabular}
\end{center}

\answer{Note: "cat" and "sat" are crucial (high values), while "that was" are less important (lower values)}

\teaching{There's no single correct answer - the key is that students understand selective importance. Main words (cat, sat) should have higher values than function words (that, was).}

\textbf{Task 2: Gate benefits}

How do gates help with the vanishing gradient problem?

If forget gate = 1.0, then memory decay = $1.0^{20}$ = \answer{1.0 (no decay!)}

This creates a "highway" for gradients!

\textbf{Task 3: Memory update with gates}

New formula with gates:
\begin{center}
New Memory = (Forget Gate × Old Memory) + (Input Gate × New Info)
\end{center}

Write this mathematically using your notation:

\answer{$c_t = f_t \cdot c_{t-1} + i_t \cdot \tilde{c}_t$}

\teaching{This is the actual LSTM cell state equation! Students have derived it from first principles.}
\end{exercise}

\begin{discovery}
You've just invented LSTM! The three gates you designed (Forget, Input/Save, Output/Use) are exactly how LSTMs work!
\end{discovery}

\section*{Part 5: Comparing Memory Systems (10 minutes)}

\begin{exercise}[RNN vs LSTM vs GRU]
\textbf{Task 1: Complete the comparison}

Based on your discoveries, fill in this table:

\begin{center}
\begin{tabular}{|l|p{3cm}|p{3cm}|p{3cm}|}
\hline
\textbf{Aspect} & \textbf{Simple RNN} & \textbf{LSTM} & \textbf{GRU} \\
& (No gates) & (3 gates) & (2 gates) \\
\hline
How it handles & Overwrites & \answer{Selective} & Selective \\
memory & completely & \answer{forget/remember} & update \\
\hline
Maximum & ~10-20 & \answer{100+} & ~100 \\
sequence length & words & \answer{words} & words \\
\hline
Gradient & Exponential & \answer{Preserved via} & Good \\
flow & decay & \answer{cell state highway} & preservation \\
\hline
Best for & \answer{Short sequences,} & Long & Quick \\
& \answer{simple patterns} & documents & training \\
\hline
\end{tabular}
\end{center}

\textbf{Task 2: Application matching}

Which architecture would you choose for:
\begin{enumerate}
    \item Predicting the next character in a name: \answer{Simple RNN (short sequence)}
    \item Summarizing a 10-page document: \answer{LSTM (long-range dependencies)}
    \item Real-time speech recognition on phone: \answer{GRU (good performance, faster)}
    \item Analyzing sentiment in tweets: \answer{GRU or simple RNN (short text)}
\end{enumerate}

\textbf{Task 3: Design your own architecture}

If you could add a fourth gate to LSTM, what would it do?

Gate name: \answer{Example: "Attention gate" or "Importance gate"}

Function: \answer{Example: "Identifies which parts of memory are most relevant for current prediction"}

When would it activate (0 or 1)? \answer{Example: "High (near 1) when seeing question words, low for filler words"}

\teaching{This is open-ended - look for creative thinking. Some students might independently invent attention mechanisms!}
\end{exercise}

\section*{Synthesis Questions}

\begin{enumerate}
    \item \textbf{Why can't we just make the memory infinitely large?}
    
    \answer{Computational cost grows with memory size. More parameters = harder to train, risk of overfitting. Also, infinite memory would remember irrelevant details instead of learning to extract important features.}
    
    \item \textbf{The forget gate seems counterintuitive - why would we want to forget?}
    
    \answer{Forgetting irrelevant information is crucial. Example: After processing "The cat sat. The dog ran." we need to forget "cat" when focusing on "dog". Selective forgetting = selective attention.}
    
    \item \textbf{How is an LSTM like a computer's RAM?}
    
    \answer{Both have: 1) Read/write operations (gates), 2) Selective memory updates, 3) Ability to preserve information long-term, 4) Limited capacity requiring intelligent management.}
    
    \item \textbf{Could we have more than 3 gates? What would be the trade-off?}
    
    \answer{Yes, but more gates = more parameters = slower training and risk of overfitting. The 3-gate design balances expressiveness with trainability. (Note: Attention mechanisms essentially add more sophisticated gating.)}
\end{enumerate}

\section*{Instructor Notes}

\subsection*{Key Teaching Points}
\begin{enumerate}
    \item \textbf{Build intuition before math}: Let students discover the need for memory naturally
    \item \textbf{Vanishing gradient is not intuitive}: Use the decay calculation to make it concrete
    \item \textbf{Gates as decisions}: Frame gates as binary decisions (keep/forget) before introducing continuous values
    \item \textbf{LSTM complexity}: Don't worry if students don't fully grasp LSTM - the key is understanding why gates help
\end{enumerate}

\subsection*{Common Misconceptions}
\begin{itemize}
    \item \teaching{RNNs don't "remember everything" - they compress history into fixed-size hidden state}
    \item \teaching{Vanishing gradient affects learning, not just forward pass}
    \item \teaching{LSTM doesn't solve vanishing gradient completely, just mitigates it}
    \item \teaching{More gates isn't always better - GRU often outperforms LSTM}
\end{itemize}

\subsection*{Extension Activities}
For advanced students:
\begin{itemize}
    \item Implement RNN from scratch in NumPy
    \item Visualize hidden states during text processing
    \item Experiment with gradient clipping
    \item Compare LSTM vs GRU on same task
\end{itemize}

\subsection*{Assessment Ideas}
\begin{itemize}
    \item Can student explain why order matters in language?
    \item Can they describe the vanishing gradient problem?
    \item Do they understand how gates create gradient highways?
    \item Can they choose appropriate architecture for a given task?
\end{itemize}

\vspace{1cm}
\noindent\rule{\textwidth}{0.4pt}
\begin{center}
\textit{End of Instructor Version}
\end{center}

\end{document}