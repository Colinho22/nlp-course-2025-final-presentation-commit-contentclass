%==============================================================================
% WEEK 4: SEQUENCE-TO-SEQUENCE MODELS - BSC CLEAN VERSION
% Using consistent layout templates for professional presentation
%==============================================================================

\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Frankfurt}
\usecolortheme{seahorse}
\setbeamertemplate{navigation symbols}{}

% Packages
\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{tcolorbox}
\usepackage{fontawesome5}

% Include layout templates
\input{../slide_layouts}

% BSc-level pedagogical commands
\definecolor{prereqblue}{RGB}{230,240,255}
\definecolor{misconred}{RGB}{255,240,240}
\definecolor{intuitionpurple}{RGB}{250,240,255}
\definecolor{checkpointyellow}{RGB}{255,250,230}
\definecolor{tryitgreen}{RGB}{240,255,240}

\newcommand{\prereq}[1]{
    \begin{tcolorbox}[colback=prereqblue,colframe=blue!50!black,title={\faIcon{book} Prerequisite}]
    #1
    \end{tcolorbox}
}

\newcommand{\misconception}[1]{
    \begin{tcolorbox}[colback=misconred,colframe=red!50!black,title={\faIcon{exclamation-triangle} Common Misconception}]
    #1
    \end{tcolorbox}
}

\newcommand{\intuition}[1]{
    \begin{tcolorbox}[colback=intuitionpurple,colframe=purple!50!black,title={\faIcon{brain} Build Your Intuition}]
    #1
    \end{tcolorbox}
}

\newcommand{\checkpoint}[1]{
    \begin{tcolorbox}[colback=checkpointyellow,colframe=orange!50!black,title={\faIcon{check-circle} Checkpoint}]
    #1
    \end{tcolorbox}
}

\newcommand{\tryit}[1]{
    \begin{tcolorbox}[colback=tryitgreen,colframe=green!50!black,title={\faIcon{hand-point-right} Try It}]
    #1
    \end{tcolorbox}
}

% Code listing settings
\lstset{
    language=Python,
    basicstyle=\ttfamily\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!5}
}

\title{Week 4: Sequence-to-Sequence Models}
\subtitle{From Variable Lengths to Attention Mechanisms}
\author{BSc-Level Enhanced Version with Layout Templates}
\date{}

\begin{document}

%==============================================================================
% TITLE SLIDE
%==============================================================================
\begin{frame}
    \titlepage
\end{frame}

%==============================================================================
% OVERVIEW
%==============================================================================
\begin{frame}{Today's Journey}
    \begin{columns}[T]
        \column{0.6\textwidth}
        \textbf{Where We Are:}
        \begin{itemize}
            \item Week 3: RNNs handle sequences
            \item Week 3: Fixed input $\rightarrow$ fixed output
            \item \highlight{Today: Variable input $\rightarrow$ variable output}
        \end{itemize}
        
        \vspace{0.5em}
        \textbf{Today's Learning Objectives:}
        \begin{enumerate}
            \item Understand why translation needs special architecture
            \item Master encoder-decoder framework
            \item Discover attention mechanism
            \item Apply seq2seq in practice
        \end{enumerate}
        
        \column{0.35\textwidth}
        \prereq{
            You should know:
            \begin{itemize}
                \item RNN basics
                \item Hidden states
                \item Backpropagation
            \end{itemize}
        }
    \end{columns}
\end{frame}

%==============================================================================
% PART 1: THE TRANSLATION CHALLENGE
%==============================================================================
\section{Part 1: The Translation Challenge}

% Using conceptslide for theory
\begin{frame}[t]{Why Can't We Just Use RNNs?}
    \begin{columns}[T]
        \column{0.6\textwidth}
        \intuition{
            RNNs expect: one input $\rightarrow$ one output\\
            Translation needs: any length $\rightarrow$ any length
        }
        
        \vspace{0.5em}
        \textbf{The Fundamental Problem:}
        \begin{itemize}
            \item English: ``I love you'' (3 words)
            \item French: ``Je t'aime'' (2 words)  
            \item Japanese: ``Aishiteru'' (1 word)
        \end{itemize}
        
        \vspace{0.5em}
        \textbf{RNN Limitation:}
        \begin{itemize}
            \item Fixed mapping between positions
            \item Can't handle length mismatch
            \item No way to ``wait'' or ``generate multiple''
        \end{itemize}
        
        \column{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/encoder_decoder_flow.pdf}
        
        \vspace{0.5em}
        \misconception{
            ``Just pad with zeros!''\\
            No - that changes meaning and wastes computation.
        }
    \end{columns}
\end{frame}

% Concrete numerical example
\begin{frame}[t]{Concrete Example: ``Hello World'' Translation}
    \textbf{Let's trace through actual numbers:}
    
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Input: ``Hello world'' (English)}
        \begin{itemize}
            \item Word 1: ``Hello'' $\rightarrow$ embed = [0.2, -0.1, 0.5]
            \item Word 2: ``world'' $\rightarrow$ embed = [0.3, 0.4, -0.2]
        \end{itemize}
        
        \textbf{Target: ``Bonjour monde'' (French)}
        \begin{itemize}
            \item Word 1: ``Bonjour''
            \item Word 2: ``monde''
        \end{itemize}
        
        \column{0.5\textwidth}
        \tryit{
            With standard RNN:
            \begin{itemize}
                \item Position 1: Hello $\rightarrow$ Bonjour \checkmark
                \item Position 2: world $\rightarrow$ monde \checkmark
                \item Position 3: ??? $\rightarrow$ Nothing needed
            \end{itemize}
            
            But what if French had 3 words?\\
            The system breaks!
        }
    \end{columns}
    
    \vspace{0.5em}
    \checkpoint{
        Can you explain why padding doesn't solve this?\\
        Answer: Because we don't know output length in advance!
    }
\end{frame}

%==============================================================================
% PART 2: THE ENCODER-DECODER ARCHITECTURE  
%==============================================================================
\section{Part 2: The Encoder-Decoder Architecture}

% The key insight - using standard frame due to complexity
\begin{frame}[t]{The Brilliant Insight: Two-Stage Process}
    \intuition{
        Think like a human translator:
        \begin{enumerate}
            \item READ and understand the whole sentence
            \item THINK about the meaning
            \item WRITE the translation
        \end{enumerate}
    }
    
    \vspace{0.5em}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/encoder_decoder_flow.pdf}
    
    \vspace{0.5em}
    \textbf{The Seq2Seq Solution:}
    \begin{itemize}
        \item \textcolor{blue}{\textbf{Encoder:}} Reads entire input $\rightarrow$ builds understanding
        \item \textcolor{orange}{\textbf{Context Vector:}} Fixed-size representation of meaning
        \item \textcolor{green}{\textbf{Decoder:}} Generates output from understanding
    \end{itemize}
\end{frame}

% Mathematical formulation with concrete numbers
\begin{frame}[fragile,t]{Encoder: Step-by-Step with Numbers}
    \textbf{Processing ``The cat sat'':}
    
    \begin{columns}[T]
        \column{0.6\textwidth}
        \textbf{Step 1: Embedding}
        \begin{itemize}
            \item ``The'' $\rightarrow$ $x_1 = [0.1, -0.2, 0.3]$
            \item ``cat'' $\rightarrow$ $x_2 = [0.5, 0.1, -0.1]$
            \item ``sat'' $\rightarrow$ $x_3 = [0.3, 0.6, 0.2]$
        \end{itemize}
        
        \textbf{Step 2: LSTM Processing}
        \begin{align*}
            h_0 &= [0, 0, 0] \text{ (initial)}\\
            h_1 &= \text{LSTM}(x_1, h_0) = [0.2, -0.1, 0.1]\\
            h_2 &= \text{LSTM}(x_2, h_1) = [0.4, 0.3, -0.2]\\
            h_3 &= \text{LSTM}(x_3, h_2) = [0.6, 0.5, 0.3]
        \end{align*}
        
        \textbf{Step 3: Context Vector}
        $$c = h_3 = [0.6, 0.5, 0.3]$$
        
        \column{0.35\textwidth}
        \tryit{
            Notice how each hidden state builds on the previous:
            \begin{itemize}
                \item $h_1$: knows ``The''
                \item $h_2$: knows ``The cat''
                \item $h_3$: knows ``The cat sat''
            \end{itemize}
            
            Final $h_3$ contains the complete meaning!
        }
    \end{columns}
\end{frame}

% Code implementation
\begin{frame}[fragile]{Encoder Implementation}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \begin{lstlisting}[language=Python]
class Encoder(nn.Module):
    def __init__(self, vocab_size, 
                 embed_dim=100, 
                 hidden_dim=256):
        super().__init__()
        self.embedding = nn.Embedding(
            vocab_size, embed_dim)
        self.lstm = nn.LSTM(
            embed_dim, hidden_dim)
    
    def forward(self, source):
        # source: [seq_len, batch]
        embedded = self.embedding(source)
        # embedded: [seq_len, batch, embed]
        
        outputs, (hidden, cell) = \
            self.lstm(embedded)
        # hidden: [1, batch, hidden_dim]
        
        return hidden, cell  # Context!
        \end{lstlisting}
        
        \column{0.43\textwidth}
        \textbf{Key Implementation Points:}
        \begin{itemize}
            \item \texttt{vocab\_size}: How many words we know
            \item \texttt{embed\_dim}: Word vector size (100)
            \item \texttt{hidden\_dim}: Context size (256)
        \end{itemize}
        
        \vspace{0.5em}
        \textbf{Dimensions Example:}
        \begin{itemize}
            \item Input: ``Hello world'' = [2, 1]
            \item After embedding: [2, 1, 100]
            \item Context output: [1, 1, 256]
        \end{itemize}
        
        \vspace{0.5em}
        \checkpoint{
            The context vector is always the same size (256) regardless of input length!
        }
    \end{columns}
\end{frame}

% Decoder with concrete example
\begin{frame}[fragile,t]{Decoder: Generating Step by Step}
    \textbf{Generating ``Le chat'' from context $c = [0.6, 0.5, 0.3]$:}
    
    \begin{columns}[T]
        \column{0.6\textwidth}
        \textbf{Step 1: Initialize with context}
        \begin{itemize}
            \item $h_0^{dec} = c = [0.6, 0.5, 0.3]$
            \item Input: <START> token
        \end{itemize}
        
        \textbf{Step 2: Generate first word}
        \begin{itemize}
            \item $h_1^{dec} = \text{LSTM}(<START>, h_0^{dec})$
            \item Output probabilities: \{Le: 0.7, La: 0.2, Un: 0.1\}
            \item Select: ``Le''
        \end{itemize}
        
        \textbf{Step 3: Generate second word}
        \begin{itemize}
            \item $h_2^{dec} = \text{LSTM}(``Le'', h_1^{dec})$
            \item Output probabilities: \{chat: 0.8, chien: 0.2\}
            \item Select: ``chat''
        \end{itemize}
        
        \column{0.35\textwidth}
        \begin{lstlisting}[language=Python]
# Decoder forward pass
def forward(self, context):
    hidden = context
    outputs = []
    
    word = "<START>"
    for _ in range(max_len):
        embed = embedding(word)
        hidden = lstm(embed, hidden)
        probs = softmax(hidden)
        word = sample(probs)
        
        if word == "<END>":
            break
        outputs.append(word)
    
    return outputs
        \end{lstlisting}
    \end{columns}
\end{frame}

%==============================================================================
% PART 3: SOLVING PROBLEMS WITH ATTENTION
%==============================================================================
\section{Part 3: Solving Problems with Attention}

% The bottleneck problem
\begin{frame}[t]{The Information Bottleneck}
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/bottleneck_visualization.pdf}
    
    \vspace{0.5em}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{The Problem:}
        \begin{itemize}
            \item 10-word sentence: $\approx$ 100 bits of info
            \item 256-dim vector: 256 numbers capacity
            \item 50-word sentence: $\approx$ 500 bits of info
            \item Same 256-dim vector! \textcolor{red}{Overflow!}
        \end{itemize}
        
        \column{0.5\textwidth}
        \misconception{
            ``Just make the vector bigger!''\\
            \vspace{0.2em}
            Problems with that:
            \begin{itemize}
                \item More parameters to learn
                \item Slower training
                \item Still fixed size
            \end{itemize}
        }
    \end{columns}
    
    \vspace{0.5em}
    \checkpoint{
        Calculate: If each word needs 10 bits and vector has 256 dimensions,\\
        what's the maximum sentence length? Answer: $\approx$ 25 words
    }
\end{frame}

% Attention mechanism with numbers
\begin{frame}[fragile,t]{Attention: The Solution}
    \textbf{Key Insight:} Look back at ALL encoder states, not just the last one!
    
    \begin{columns}[T]
        \column{0.6\textwidth}
        \textbf{Generating ``chat'' (cat in French):}
        
        \textbf{Step 1: Score each encoder state}
        \begin{itemize}
            \item $e_1 = \text{score}(h_{chat}^{dec}, h_{The}^{enc}) = 0.1$
            \item $e_2 = \text{score}(h_{chat}^{dec}, h_{cat}^{enc}) = 0.9$
            \item $e_3 = \text{score}(h_{chat}^{dec}, h_{sat}^{enc}) = 0.3$
        \end{itemize}
        
        \textbf{Step 2: Normalize to probabilities}
        \begin{itemize}
            \item $\alpha_1 = \text{softmax}(0.1) = 0.15$
            \item $\alpha_2 = \text{softmax}(0.9) = 0.70$
            \item $\alpha_3 = \text{softmax}(0.3) = 0.15$
        \end{itemize}
        
        \textbf{Step 3: Weighted combination}
        $$c_{chat} = 0.15 \cdot h_1 + 0.70 \cdot h_2 + 0.15 \cdot h_3$$
        
        \column{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/attention_heatmap.pdf}
        
        \vspace{0.5em}
        \tryit{
            Attention weights tell us:
            \begin{itemize}
                \item 70\% focus on ``cat''
                \item 15\% on ``The''
                \item 15\% on ``sat''
            \end{itemize}
            Makes sense for translating ``cat''!
        }
    \end{columns}
\end{frame}

% Attention implementation
\begin{frame}[fragile]{Attention Implementation}
    \begin{columns}[T]
        \column{0.55\textwidth}
        \begin{lstlisting}[language=Python]
class BahdanauAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.W1 = nn.Linear(hidden_dim, hidden_dim)
        self.W2 = nn.Linear(hidden_dim, hidden_dim)
        self.V = nn.Linear(hidden_dim, 1)
    
    def forward(self, decoder_hidden, 
                encoder_outputs):
        # decoder_hidden: [batch, hidden]
        # encoder_outputs: [seq_len, batch, hidden]
        
        # Score each encoder output
        scores = self.V(torch.tanh(
            self.W1(decoder_hidden) + 
            self.W2(encoder_outputs)))
        # scores: [seq_len, batch, 1]
        
        # Convert to probabilities
        weights = F.softmax(scores, dim=0)
        
        # Weighted sum
        context = torch.sum(
            weights * encoder_outputs, dim=0)
        
        return context, weights
        \end{lstlisting}
        
        \column{0.43\textwidth}
        \textbf{Numerical Example:}
        
        Given:
        \begin{itemize}
            \item Decoder hidden: [0.5, 0.3]
            \item Encoder outputs:
            \begin{itemize}
                \item $h_1$: [0.1, 0.2]
                \item $h_2$: [0.7, 0.4]
                \item $h_3$: [0.3, 0.6]
            \end{itemize}
        \end{itemize}
        
        Computation:
        \begin{itemize}
            \item Scores: [0.2, 0.8, 0.3]
            \item Weights: [0.15, 0.70, 0.15]
            \item Context: $0.15 \times [0.1, 0.2] +$\\
                  $\phantom{Context: }0.70 \times [0.7, 0.4] +$\\
                  $\phantom{Context: }0.15 \times [0.3, 0.6]$\\
                  $\phantom{Context: }= [0.55, 0.46]$
        \end{itemize}
        
        \checkpoint{
            Attention gives different context for each output word!
        }
    \end{columns}
\end{frame}

%==============================================================================
% PART 4: PRACTICAL TRAINING & EVALUATION
%==============================================================================
\section{Part 4: Practical Training \& Evaluation}

% Teacher forcing with example
\begin{frame}[t]{Training: Teacher Forcing}
    \textbf{Problem:} Early in training, model makes mistakes that compound.
    
    \begin{columns}[T]
        \column{0.6\textwidth}
        \textbf{Without Teacher Forcing:}
        \begin{itemize}
            \item Target: ``Le chat dort''
            \item Step 1: Generate ``La'' (wrong!)
            \item Step 2: Based on ``La'', generate ``maison''
            \item Step 3: Based on ``maison'', generate ``est''
            \item Result: ``La maison est'' (completely wrong!)
        \end{itemize}
        
        \vspace{0.5em}
        \textbf{With Teacher Forcing:}
        \begin{itemize}
            \item Target: ``Le chat dort''
            \item Step 1: Generate ``La'', but feed ``Le'' to next
            \item Step 2: Based on ``Le'', generate ``chat'' \checkmark
            \item Step 3: Based on ``chat'', generate ``dort'' \checkmark
            \item Learning signal much stronger!
        \end{itemize}
        
        \column{0.35\textwidth}
        \tryit{
            Teacher forcing schedule:
            \begin{itemize}
                \item Epoch 1-10: 100\% forcing
                \item Epoch 11-20: 50\% forcing
                \item Epoch 21+: 0\% forcing
            \end{itemize}
            
            Gradually let the model learn to trust itself!
        }
        
        \vspace{0.5em}
        \misconception{
            ``Always use teacher forcing!''\\
            No - creates mismatch between training and inference.
        }
    \end{columns}
\end{frame}

% Beam search with concrete probabilities
\begin{frame}[t]{Inference: Beam Search}
    \textbf{Finding the best translation with beam size = 2:}
    
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/beam_search_tree.pdf}
    
    \vspace{0.5em}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Step 1 Probabilities:}
        \begin{itemize}
            \item ``Le'': 0.6
            \item ``La'': 0.3
            \item ``Un'': 0.1
        \end{itemize}
        Keep top 2: ``Le'', ``La''
        
        \column{0.5\textwidth}
        \textbf{Step 2 Probabilities:}
        \begin{itemize}
            \item ``Le chat'': $0.6 \times 0.8 = 0.48$
            \item ``Le chien'': $0.6 \times 0.2 = 0.12$
            \item ``La chat'': $0.3 \times 0.1 = 0.03$
            \item ``La maison'': $0.3 \times 0.7 = 0.21$
        \end{itemize}
        Keep top 2: ``Le chat'', ``La maison''
    \end{columns}
    
    \checkpoint{
        With beam=1 (greedy), we'd pick ``Le'' then ``chat''. \\
        With beam=2, we explore alternatives and might find better overall translation!
    }
\end{frame}

% BLEU score calculation
\begin{frame}[t]{Evaluation: BLEU Score Calculation}
    \textbf{Evaluating: ``The cat sits on mat''}
    
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Reference:} ``Le chat est sur le tapis''\\
        \textbf{Generated:} ``Le chat sur tapis''
        
        \vspace{0.5em}
        \textbf{Step 1: Count n-grams}
        \begin{itemize}
            \item 1-grams: Le(1/1), chat(1/1), sur(1/1), tapis(1/1)
            \item 2-grams: ``Le chat''(1/1), ``chat sur''(0/1), ``sur tapis''(0/1)
            \item 3-grams: None match
        \end{itemize}
        
        \textbf{Step 2: Calculate precision}
        \begin{itemize}
            \item $P_1 = 4/4 = 1.00$
            \item $P_2 = 1/3 = 0.33$
            \item $P_3 = 0/2 = 0.00$
        \end{itemize}
        
        \column{0.5\textwidth}
        \textbf{Step 3: Brevity penalty}
        \begin{itemize}
            \item Generated length: 4
            \item Reference length: 6
            \item $BP = e^{1-6/4} = e^{-0.5} = 0.61$
        \end{itemize}
        
        \textbf{Step 4: Final BLEU}
        $$BLEU = BP \times \sqrt[3]{P_1 \times P_2 \times P_3}$$
        $$= 0.61 \times \sqrt[3]{1.0 \times 0.33 \times 0.01}$$
        $$= 0.61 \times 0.22 = 0.13$$
        
        \tryit{
            BLEU scores:
            \begin{itemize}
                \item $< 0.1$: Useless
                \item $0.1-0.3$: Understandable
                \item $0.3-0.5$: Good
                \item $> 0.5$: Very good
            \end{itemize}
        }
    \end{columns}
\end{frame}

%==============================================================================
% SUMMARY
%==============================================================================
\begin{frame}{Summary: Key Takeaways}
    \begin{columns}[T]
        \column{0.6\textwidth}
        \textbf{What We Learned:}
        \begin{enumerate}
            \item \textbf{Problem:} Variable length sequences
            \item \textbf{Solution:} Encoder-Decoder architecture
            \item \textbf{Enhancement:} Attention mechanism
            \item \textbf{Training:} Teacher forcing
            \item \textbf{Inference:} Beam search
            \item \textbf{Evaluation:} BLEU score
        \end{enumerate}
        
        \vspace{0.5em}
        \textbf{Key Insights:}
        \begin{itemize}
            \item Context vector = compressed understanding
            \item Attention = dynamic focus on relevant parts
            \item Trade-off between exploration and exploitation
        \end{itemize}
        
        \column{0.35\textwidth}
        \textbf{Next Week:}
        
        Transformers - Attention is all you need!
        \begin{itemize}
            \item No more recurrence
            \item Parallel processing
            \item Self-attention
            \item Much faster training
        \end{itemize}
        
        \vspace{0.5em}
        \checkpoint{
            Final check: Can you explain why we need TWO networks?\\
            Answer: Input length $\neq$ output length!
        }
    \end{columns}
\end{frame}

%==============================================================================
% APPENDICES
%==============================================================================
\appendix

\section{Appendix A: Mathematical Details}

\begin{frame}[t]{Complete Mathematical Formulation}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Encoder:}
        \begin{align}
            h_t^{enc} &= f_{enc}(x_t, h_{t-1}^{enc})\\
            c &= h_T^{enc}
        \end{align}
        
        \textbf{Decoder (without attention):}
        \begin{align}
            h_t^{dec} &= f_{dec}(y_{t-1}, h_{t-1}^{dec}, c)\\
            p(y_t|y_{<t}, x) &= \text{softmax}(W_o h_t^{dec})
        \end{align}
        
        \column{0.5\textwidth}
        \textbf{Decoder (with attention):}
        \begin{align}
            e_{ti} &= a(h_{t-1}^{dec}, h_i^{enc})\\
            \alpha_{ti} &= \frac{\exp(e_{ti})}{\sum_j \exp(e_{tj})}\\
            c_t &= \sum_i \alpha_{ti} h_i^{enc}\\
            h_t^{dec} &= f_{dec}(y_{t-1}, h_{t-1}^{dec}, c_t)
        \end{align}
        
        Where $a(\cdot)$ is the attention scoring function.
    \end{columns}
\end{frame}

\section{Appendix B: Modern Applications}

\begin{frame}[t]{Seq2Seq in 2024}
    \begin{columns}[T]
        \column{0.5\textwidth}
        \textbf{Still Using Seq2Seq:}
        \begin{itemize}
            \item Machine Translation (Google Translate)
            \item Speech Recognition (Whisper)
            \item Summarization
            \item Code Generation (GitHub Copilot)
        \end{itemize}
        
        \textbf{Evolution to Transformers:}
        \begin{itemize}
            \item BERT: Encoder-only
            \item GPT: Decoder-only
            \item T5: Full encoder-decoder
        \end{itemize}
        
        \column{0.5\textwidth}
        \textbf{Key Improvements:}
        \begin{itemize}
            \item Self-attention (next week!)
            \item Multi-head attention
            \item Positional encoding
            \item Layer normalization
        \end{itemize}
        
        \textbf{Performance Gains:}
        \begin{itemize}
            \item 2014 Seq2Seq: BLEU 20
            \item 2017 Transformer: BLEU 28
            \item 2024 GPT-4: BLEU 45+
        \end{itemize}
    \end{columns}
\end{frame}

\end{document}