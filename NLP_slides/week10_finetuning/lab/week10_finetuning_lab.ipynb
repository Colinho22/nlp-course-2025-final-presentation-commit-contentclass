{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 10 Lab: Fine-tuning & Prompt Engineering\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different fine-tuning strategies\n",
        "- Implement LoRA (Low-Rank Adaptation)\n",
        "- Practice prompt engineering techniques\n",
        "- Compare full fine-tuning vs parameter-efficient methods\n",
        "\n",
        "## Prerequisites\n",
        "```bash\n",
        "pip install transformers torch numpy matplotlib peft\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Setup\n",
        "print('Week 10: Fine-tuning & Prompt Engineering')\n",
        "print('=' * 50)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Fine-tuning\n",
        "\n",
        "Fine-tuning adapts a pre-trained model to a specific task. Let's visualize the parameter space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained model and count parameters\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Count parameters by layer type\n",
        "def count_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "total, trainable = count_parameters(model)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Total parameters: {total:,}\")\n",
        "print(f\"Trainable parameters: {trainable:,}\")\n",
        "print(f\"Model size: {total * 4 / 1e6:.1f} MB (float32)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize parameter distribution\n",
        "layer_params = {}\n",
        "for name, param in model.named_parameters():\n",
        "    layer_type = name.split('.')[0]\n",
        "    if layer_type not in layer_params:\n",
        "        layer_params[layer_type] = 0\n",
        "    layer_params[layer_type] += param.numel()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "bars = ax.barh(list(layer_params.keys()), \n",
        "               [v/1e6 for v in layer_params.values()],\n",
        "               color='#3333B2')\n",
        "ax.set_xlabel('Parameters (millions)', fontsize=12)\n",
        "ax.set_title('Parameter Distribution by Layer Type', fontsize=14, fontweight='bold')\n",
        "\n",
        "for bar, val in zip(bars, layer_params.values()):\n",
        "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "            f'{val/1e6:.1f}M', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Implementing LoRA from Scratch\n",
        "\n",
        "LoRA (Low-Rank Adaptation) freezes the pre-trained weights and adds trainable low-rank matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"LoRA layer that wraps a linear layer\"\"\"\n",
        "    \n",
        "    def __init__(self, original_layer, rank=4, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.original_layer = original_layer\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # Freeze original weights\n",
        "        for param in self.original_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Get dimensions\n",
        "        in_features = original_layer.in_features\n",
        "        out_features = original_layer.out_features\n",
        "        \n",
        "        # Initialize low-rank matrices\n",
        "        # A: down-projection (in_features -> rank)\n",
        "        # B: up-projection (rank -> out_features)\n",
        "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
        "        \n",
        "        # Initialize A with random values, B with zeros\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "        \n",
        "        self.scaling = alpha / rank\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Original forward pass\n",
        "        original_output = self.original_layer(x)\n",
        "        \n",
        "        # LoRA forward pass: x @ A^T @ B^T * scaling\n",
        "        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
        "        \n",
        "        return original_output + lora_output\n",
        "    \n",
        "    def get_lora_params(self):\n",
        "        return self.lora_A.numel() + self.lora_B.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate LoRA on a simple linear layer\n",
        "original_linear = nn.Linear(768, 768)\n",
        "lora_linear = LoRALayer(original_linear, rank=8, alpha=16)\n",
        "\n",
        "# Count parameters\n",
        "original_params = sum(p.numel() for p in original_linear.parameters())\n",
        "lora_params = lora_linear.get_lora_params()\n",
        "\n",
        "print(\"LoRA Parameter Comparison:\")\n",
        "print(f\"  Original layer parameters: {original_params:,}\")\n",
        "print(f\"  LoRA trainable parameters: {lora_params:,}\")\n",
        "print(f\"  Parameter reduction: {100 * (1 - lora_params/original_params):.1f}%\")\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(2, 10, 768)\n",
        "with torch.no_grad():\n",
        "    output = lora_linear(x)\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize LoRA parameter efficiency\n",
        "ranks = [1, 2, 4, 8, 16, 32, 64]\n",
        "hidden_size = 768\n",
        "\n",
        "original_params = hidden_size * hidden_size\n",
        "lora_params = [2 * r * hidden_size for r in ranks]\n",
        "reduction = [100 * (1 - lp/original_params) for lp in lora_params]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Parameters vs rank\n",
        "ax1.plot(ranks, [original_params/1000]*len(ranks), 'r--', label='Full fine-tuning', linewidth=2)\n",
        "ax1.plot(ranks, [lp/1000 for lp in lora_params], 'b-o', label='LoRA', linewidth=2)\n",
        "ax1.set_xlabel('LoRA Rank', fontsize=12)\n",
        "ax1.set_ylabel('Parameters (thousands)', fontsize=12)\n",
        "ax1.set_title('Parameters vs LoRA Rank', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Reduction percentage\n",
        "ax2.bar(range(len(ranks)), reduction, color='#2CA02C')\n",
        "ax2.set_xticks(range(len(ranks)))\n",
        "ax2.set_xticklabels(ranks)\n",
        "ax2.set_xlabel('LoRA Rank', fontsize=12)\n",
        "ax2.set_ylabel('Parameter Reduction (%)', fontsize=12)\n",
        "ax2.set_title('Parameter Reduction by Rank', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylim(90, 100)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Prompt Engineering\n",
        "\n",
        "Prompt engineering is the art of designing effective prompts to guide model behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt templates for different tasks\n",
        "prompt_templates = {\n",
        "    'zero_shot': \"\"\"Classify the sentiment of the following text as positive or negative.\n",
        "\n",
        "Text: {text}\n",
        "Sentiment:\"\"\",\n",
        "    \n",
        "    'one_shot': \"\"\"Classify the sentiment of texts as positive or negative.\n",
        "\n",
        "Text: \"I love this product!\"\n",
        "Sentiment: positive\n",
        "\n",
        "Text: {text}\n",
        "Sentiment:\"\"\",\n",
        "    \n",
        "    'few_shot': \"\"\"Classify the sentiment of texts as positive or negative.\n",
        "\n",
        "Text: \"I love this product!\"\n",
        "Sentiment: positive\n",
        "\n",
        "Text: \"This is terrible, waste of money.\"\n",
        "Sentiment: negative\n",
        "\n",
        "Text: \"Best purchase I've ever made!\"\n",
        "Sentiment: positive\n",
        "\n",
        "Text: {text}\n",
        "Sentiment:\"\"\",\n",
        "    \n",
        "    'chain_of_thought': \"\"\"Analyze the sentiment of the following text step by step.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Step 1: Identify key emotional words.\n",
        "Step 2: Determine the overall tone.\n",
        "Step 3: Classify as positive or negative.\n",
        "\n",
        "Analysis:\"\"\"\n",
        "}\n",
        "\n",
        "test_text = \"The movie was okay, but the ending was disappointing.\"\n",
        "\n",
        "print(\"Prompt Engineering Examples\")\n",
        "print(\"=\" * 60)\n",
        "for name, template in prompt_templates.items():\n",
        "    print(f\"\\n--- {name.upper()} ---\")\n",
        "    print(template.format(text=test_text))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt template analysis\n",
        "def analyze_prompt(template, text):\n",
        "    \"\"\"Analyze prompt characteristics\"\"\"\n",
        "    prompt = template.format(text=text)\n",
        "    tokens = tokenizer.tokenize(prompt)\n",
        "    return {\n",
        "        'char_length': len(prompt),\n",
        "        'token_count': len(tokens),\n",
        "        'word_count': len(prompt.split()),\n",
        "        'example_count': prompt.count('Sentiment:') - 1  # Exclude final query\n",
        "    }\n",
        "\n",
        "print(\"Prompt Analysis\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Template':<20} {'Chars':<10} {'Tokens':<10} {'Examples':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for name, template in prompt_templates.items():\n",
        "    analysis = analyze_prompt(template, test_text)\n",
        "    print(f\"{name:<20} {analysis['char_length']:<10} {analysis['token_count']:<10} {analysis['example_count']:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Fine-tuning Strategies Comparison\n",
        "\n",
        "Let's compare different approaches to adapting pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated comparison of fine-tuning strategies\n",
        "strategies = {\n",
        "    'Full Fine-tuning': {'params': 100, 'memory': 100, 'accuracy': 95, 'training_time': 100},\n",
        "    'Freeze Embeddings': {'params': 85, 'memory': 85, 'accuracy': 93, 'training_time': 80},\n",
        "    'Only Classifier': {'params': 1, 'memory': 50, 'accuracy': 85, 'training_time': 10},\n",
        "    'LoRA (r=8)': {'params': 0.5, 'memory': 55, 'accuracy': 93, 'training_time': 30},\n",
        "    'LoRA (r=16)': {'params': 1, 'memory': 58, 'accuracy': 94, 'training_time': 35},\n",
        "    'Prefix Tuning': {'params': 0.1, 'memory': 52, 'accuracy': 90, 'training_time': 25},\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "metrics = ['params', 'memory', 'accuracy', 'training_time']\n",
        "titles = ['Trainable Parameters (%)', 'Memory Usage (%)', 'Accuracy (%)', 'Training Time (%)']\n",
        "colors = ['#3333B2', '#FF7F0E', '#2CA02C', '#D62728', '#9467BD', '#8C564B']\n",
        "\n",
        "for ax, metric, title in zip(axes.flat, metrics, titles):\n",
        "    values = [strategies[s][metric] for s in strategies]\n",
        "    bars = ax.barh(list(strategies.keys()), values, color=colors)\n",
        "    ax.set_xlabel(title.split('(')[0], fontsize=11)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    \n",
        "    for bar, val in zip(bars, values):\n",
        "        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
        "                f'{val}', va='center', fontsize=9)\n",
        "\n",
        "plt.suptitle('Fine-tuning Strategy Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Practical Fine-tuning Example\n",
        "\n",
        "Let's fine-tune a small model for sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple training example\n",
        "train_texts = [\n",
        "    \"I love this product, it's amazing!\",\n",
        "    \"Terrible experience, would not recommend.\",\n",
        "    \"Best purchase ever, highly satisfied!\",\n",
        "    \"Waste of money, very disappointed.\",\n",
        "    \"Excellent quality and fast shipping!\",\n",
        "    \"Poor customer service, never buying again.\",\n",
        "]\n",
        "train_labels = [1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
        "\n",
        "# Tokenize\n",
        "encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "labels = torch.tensor(train_labels)\n",
        "\n",
        "print(\"Training Data:\")\n",
        "for text, label in zip(train_texts, train_labels):\n",
        "    sentiment = 'positive' if label == 1 else 'negative'\n",
        "    print(f\"  [{sentiment:8}] {text[:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple training loop demonstration\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "losses = []\n",
        "print(\"Training Progress:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(**encodings, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, len(losses)+1), losses, 'b-o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **LoRA Implementation**: Extend the LoRALayer class to support different alpha values\n",
        "2. **Prompt Optimization**: Design prompts for a Q&A task and compare effectiveness\n",
        "3. **Fine-tuning**: Fine-tune a model on a custom dataset with different learning rates\n",
        "4. **Comparison**: Compare LoRA vs full fine-tuning on a classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise starter: Implement a prompt evaluation function\n",
        "def evaluate_prompt_quality(prompt, criteria):\n",
        "    \"\"\"\n",
        "    Evaluate prompt quality based on criteria.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The prompt string\n",
        "        criteria: Dict of criteria to check\n",
        "    \n",
        "    Returns:\n",
        "        Dict of scores\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    \n",
        "    # Length appropriateness (not too short, not too long)\n",
        "    tokens = tokenizer.tokenize(prompt)\n",
        "    if 10 < len(tokens) < 100:\n",
        "        scores['length'] = 1.0\n",
        "    elif 5 < len(tokens) < 200:\n",
        "        scores['length'] = 0.5\n",
        "    else:\n",
        "        scores['length'] = 0.0\n",
        "    \n",
        "    # Has clear instruction\n",
        "    instruction_words = ['classify', 'determine', 'analyze', 'identify', 'extract']\n",
        "    scores['instruction'] = 1.0 if any(w in prompt.lower() for w in instruction_words) else 0.0\n",
        "    \n",
        "    # Has examples (for few-shot)\n",
        "    scores['examples'] = min(prompt.count('Example:') / 3, 1.0)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "# Test\n",
        "test_prompt = prompt_templates['few_shot'].format(text=\"Test\")\n",
        "scores = evaluate_prompt_quality(test_prompt, {})\n",
        "print(\"Prompt Quality Scores:\")\n",
        "for criterion, score in scores.items():\n",
        "    print(f\"  {criterion}: {score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, we explored:\n",
        "\n",
        "1. **Fine-tuning basics**: Understanding parameter counts and layer distributions\n",
        "2. **LoRA implementation**: Built a low-rank adaptation layer from scratch\n",
        "3. **Prompt engineering**: Designed and analyzed different prompt templates\n",
        "4. **Strategy comparison**: Compared efficiency vs performance tradeoffs\n",
        "5. **Practical training**: Demonstrated a simple fine-tuning workflow\n",
        "\n",
        "**Key Takeaways**:\n",
        "- LoRA can achieve >99% parameter reduction with minimal accuracy loss\n",
        "- Prompt engineering is crucial for zero/few-shot performance\n",
        "- Different strategies suit different resource constraints"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
