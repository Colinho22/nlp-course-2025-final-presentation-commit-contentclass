{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Lab: Word Embeddings and Word2Vec\n",
    "\n",
    "## Learning Objectives\n",
    "- Build Word2Vec from scratch in PyTorch\n",
    "- Train embeddings on real text data\n",
    "- Explore semantic relationships\n",
    "- Visualize embedding spaces\n",
    "- Compare with pre-trained models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample text data\n",
    "# You can replace this with any text file\n",
    "sample_text = \"\"\"\n",
    "The cat sat on the mat. The dog played in the garden.\n",
    "Cats and dogs are common pets. They live with humans.\n",
    "The king ruled the kingdom. The queen lived in the palace.\n",
    "Paris is the capital of France. Rome is the capital of Italy.\n",
    "Berlin is the capital of Germany. London is the capital of England.\n",
    "Scientists study nature. Researchers conduct experiments.\n",
    "Students learn in schools. Teachers educate students.\n",
    "Books contain knowledge. Libraries store books.\n",
    "Computers process information. Programmers write code.\n",
    "Artists create paintings. Musicians compose songs.\n",
    "\"\"\"\n",
    "\n",
    "# For larger corpus, uncomment and use:\n",
    "# with open('your_text_file.txt', 'r', encoding='utf-8') as f:\n",
    "#     sample_text = f.read()\n",
    "\n",
    "print(f\"Corpus size: {len(sample_text)} characters\")\n",
    "print(f\"Sample: {sample_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "# Preprocess the corpus\n",
    "words = preprocess_text(sample_text)\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")\n",
    "print(f\"First 10 words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "class Vocabulary:\n",
    "    def __init__(self, words, min_count=1):\n",
    "        \"\"\"Build vocabulary from word list\"\"\"\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        # Filter by minimum count\n",
    "        valid_words = [w for w, c in word_counts.items() if c >= min_count]\n",
    "        \n",
    "        # Create mappings\n",
    "        self.word2idx = {w: i for i, w in enumerate(valid_words)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.word_counts = word_counts\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def encode(self, word):\n",
    "        return self.word2idx.get(word, -1)\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        return self.idx2word.get(idx, '<UNK>')\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = Vocabulary(words, min_count=1)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Most common words: {vocab.word_counts.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100):\n",
    "        \"\"\"\n",
    "        Word2Vec Skip-gram model\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        # Two embedding matrices\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.center_embeddings.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.context_embeddings.weight.data.uniform_(-0.5, 0.5)\n",
    "        \n",
    "    def forward(self, center_words, context_words, neg_words=None):\n",
    "        \"\"\"\n",
    "        Forward pass with negative sampling\n",
    "        Args:\n",
    "            center_words: Batch of center word indices\n",
    "            context_words: Batch of context word indices\n",
    "            neg_words: Batch of negative sample indices\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        center_embeds = self.center_embeddings(center_words)\n",
    "        context_embeds = self.context_embeddings(context_words)\n",
    "        \n",
    "        # Positive score (should be high)\n",
    "        pos_score = torch.sum(center_embeds * context_embeds, dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score))\n",
    "        \n",
    "        # Negative sampling loss\n",
    "        neg_loss = 0\n",
    "        if neg_words is not None:\n",
    "            neg_embeds = self.context_embeddings(neg_words)\n",
    "            neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze()\n",
    "            neg_loss = -torch.log(torch.sigmoid(-neg_score)).sum(dim=1)\n",
    "        \n",
    "        return (pos_loss + neg_loss).mean()\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        \"\"\"Get the embedding vector for a word\"\"\"\n",
    "        return self.center_embeddings.weight[word_idx].detach().cpu().numpy()\n",
    "    \n",
    "    def similarity(self, word_idx1, word_idx2):\n",
    "        \"\"\"Compute cosine similarity between two words\"\"\"\n",
    "        vec1 = self.get_embedding(word_idx1)\n",
    "        vec2 = self.get_embedding(word_idx2)\n",
    "        \n",
    "        cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cos_sim\n",
    "\n",
    "# Create model\n",
    "embed_dim = 50  # Smaller for visualization\n",
    "model = Word2Vec(vocab.vocab_size, embed_dim).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_dataset(words, vocab, window_size=2, neg_samples=5):\n",
    "    \"\"\"\n",
    "    Create training pairs for skip-gram model\n",
    "    Args:\n",
    "        words: List of words in corpus\n",
    "        vocab: Vocabulary object\n",
    "        window_size: Context window size\n",
    "        neg_samples: Number of negative samples\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Word frequency for negative sampling\n",
    "    word_freqs = np.array([vocab.word_counts[vocab.decode(i)] for i in range(len(vocab))])\n",
    "    word_freqs = word_freqs ** 0.75  # Smooth distribution\n",
    "    word_freqs = word_freqs / word_freqs.sum()\n",
    "    \n",
    "    for i, center_word in enumerate(words):\n",
    "        center_idx = vocab.encode(center_word)\n",
    "        if center_idx == -1:\n",
    "            continue\n",
    "            \n",
    "        # Get context words\n",
    "        context_range = range(max(0, i - window_size), \n",
    "                            min(len(words), i + window_size + 1))\n",
    "        \n",
    "        for j in context_range:\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            context_word = words[j]\n",
    "            context_idx = vocab.encode(context_word)\n",
    "            if context_idx == -1:\n",
    "                continue\n",
    "            \n",
    "            # Sample negative words\n",
    "            neg_indices = np.random.choice(\n",
    "                len(vocab), size=neg_samples, p=word_freqs\n",
    "            )\n",
    "            \n",
    "            dataset.append((center_idx, context_idx, neg_indices))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create training dataset\n",
    "train_data = create_skipgram_dataset(words, vocab, window_size=2, neg_samples=5)\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "\n",
    "# Show sample training data\n",
    "sample = train_data[0]\n",
    "print(f\"\\nSample training instance:\")\n",
    "print(f\"Center word: {vocab.decode(sample[0])}\")\n",
    "print(f\"Context word: {vocab.decode(sample[1])}\")\n",
    "print(f\"Negative samples: {[vocab.decode(idx) for idx in sample[2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(model, train_data, vocab, epochs=100, lr=0.01, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train Word2Vec model\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch = train_data[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch tensors\n",
    "            center_words = torch.tensor([x[0] for x in batch], dtype=torch.long).to(device)\n",
    "            context_words = torch.tensor([x[1] for x in batch], dtype=torch.long).to(device)\n",
    "            neg_words = torch.tensor([x[2] for x in batch], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(center_words, context_words, neg_words)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / (len(train_data) // batch_size)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Word2Vec model...\")\n",
    "losses = train_word2vec(model, train_data, vocab, epochs=100, lr=0.01)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Word2Vec Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Explore Semantic Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(model, vocab, word, k=5):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbors to a word\n",
    "    \"\"\"\n",
    "    word_idx = vocab.encode(word)\n",
    "    if word_idx == -1:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    # Get target embedding\n",
    "    target_embed = model.get_embedding(word_idx)\n",
    "    \n",
    "    # Compute similarities with all words\n",
    "    similarities = []\n",
    "    for idx in range(len(vocab)):\n",
    "        if idx == word_idx:\n",
    "            continue\n",
    "        embed = model.get_embedding(idx)\n",
    "        sim = np.dot(target_embed, embed) / (np.linalg.norm(target_embed) * np.linalg.norm(embed))\n",
    "        similarities.append((vocab.decode(idx), sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:k]\n",
    "\n",
    "# Test with different words\n",
    "test_words = ['cat', 'king', 'paris', 'student']\n",
    "\n",
    "for word in test_words:\n",
    "    if vocab.encode(word) != -1:\n",
    "        print(f\"\\nNearest neighbors to '{word}':\")\n",
    "        neighbors = find_nearest_neighbors(model, vocab, word, k=5)\n",
    "        for neighbor, sim in neighbors:\n",
    "            print(f\"  {neighbor}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model, vocab, a, b, c, k=5):\n",
    "    \"\"\"\n",
    "    Solve analogies: a:b :: c:?\n",
    "    Example: king:queen :: man:woman\n",
    "    \"\"\"\n",
    "    # Get word indices\n",
    "    a_idx = vocab.encode(a)\n",
    "    b_idx = vocab.encode(b)\n",
    "    c_idx = vocab.encode(c)\n",
    "    \n",
    "    if -1 in [a_idx, b_idx, c_idx]:\n",
    "        print(\"One or more words not in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    # Get embeddings\n",
    "    a_vec = model.get_embedding(a_idx)\n",
    "    b_vec = model.get_embedding(b_idx)\n",
    "    c_vec = model.get_embedding(c_idx)\n",
    "    \n",
    "    # Compute target vector: b - a + c\n",
    "    target = b_vec - a_vec + c_vec\n",
    "    \n",
    "    # Find nearest neighbors\n",
    "    similarities = []\n",
    "    for idx in range(len(vocab)):\n",
    "        if idx in [a_idx, b_idx, c_idx]:\n",
    "            continue\n",
    "        embed = model.get_embedding(idx)\n",
    "        sim = np.dot(target, embed) / (np.linalg.norm(target) * np.linalg.norm(embed))\n",
    "        similarities.append((vocab.decode(idx), sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "# Test analogies (with small corpus, results may vary)\n",
    "print(\"Testing analogies:\")\n",
    "print(\"\\nking : queen :: paris : ?\")\n",
    "results = analogy(model, vocab, 'king', 'queen', 'paris', k=3)\n",
    "for word, sim in results:\n",
    "    print(f\"  {word}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\ncat : cats :: dog : ?\")\n",
    "results = analogy(model, vocab, 'cat', 'cats', 'dog', k=3)\n",
    "for word, sim in results:\n",
    "    print(f\"  {word}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, vocab, method='tsne', n_words=30):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings in 2D\n",
    "    \"\"\"\n",
    "    # Get most common words\n",
    "    common_words = vocab.word_counts.most_common(n_words)\n",
    "    word_indices = [vocab.encode(word) for word, _ in common_words]\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = np.array([model.get_embedding(idx) for idx in word_indices])\n",
    "    \n",
    "    # Reduce dimensions\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, n_words-1))\n",
    "    else:  # PCA\n",
    "        reducer = PCA(n_components=2)\n",
    "    \n",
    "    embeddings_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, (word, _) in enumerate(common_words):\n",
    "        plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    xytext=(5, 2), textcoords='offset points',\n",
    "                    fontsize=9, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title(f'Word Embeddings Visualization ({method.upper()})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize with t-SNE\n",
    "print(\"t-SNE visualization:\")\n",
    "visualize_embeddings(model, vocab, method='tsne', n_words=25)\n",
    "\n",
    "# Visualize with PCA\n",
    "print(\"\\nPCA visualization:\")\n",
    "visualize_embeddings(model, vocab, method='pca', n_words=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Compare with Pre-trained Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section requires gensim\n",
    "# !pip install gensim\n",
    "\n",
    "try:\n",
    "    from gensim.models import KeyedVectors\n",
    "    import gensim.downloader as api\n",
    "    \n",
    "    print(\"Loading pre-trained Word2Vec model (this may take a moment)...\")\n",
    "    # Load a smaller pre-trained model\n",
    "    pretrained = api.load('glove-wiki-gigaword-50')\n",
    "    \n",
    "    # Compare nearest neighbors\n",
    "    test_word = 'king'\n",
    "    if test_word in pretrained:\n",
    "        print(f\"\\nPre-trained model - Nearest neighbors to '{test_word}':\")\n",
    "        for word, sim in pretrained.most_similar(test_word, topn=5):\n",
    "            print(f\"  {word}: {sim:.3f}\")\n",
    "    \n",
    "    # Test analogy\n",
    "    print(\"\\nPre-trained model - king : queen :: man : ?\")\n",
    "    result = pretrained.most_similar(positive=['queen', 'man'], negative=['king'], topn=3)\n",
    "    for word, sim in result:\n",
    "        print(f\"  {word}: {sim:.3f}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Gensim not installed. To compare with pre-trained models, install with:\")\n",
    "    print(\"pip install gensim\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load pre-trained model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = 'word2vec_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': vocab.vocab_size,\n",
    "    'embed_dim': embed_dim,\n",
    "    'vocab': vocab\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "loaded_model = Word2Vec(checkpoint['vocab_size'], checkpoint['embed_dim']).to(device)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Verify loaded model works\n",
    "test_word = 'cat'\n",
    "if vocab.encode(test_word) != -1:\n",
    "    print(f\"\\nTesting loaded model - Nearest neighbors to '{test_word}':\")\n",
    "    neighbors = find_nearest_neighbors(loaded_model, vocab, test_word, k=3)\n",
    "    for neighbor, sim in neighbors:\n",
    "        print(f\"  {neighbor}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Experiment with hyperparameters:**\n",
    "   - Try different embedding dimensions (50, 100, 200)\n",
    "   - Vary the window size (1, 3, 5)\n",
    "   - Change the number of negative samples\n",
    "\n",
    "2. **Use a larger corpus:**\n",
    "   - Download a book from Project Gutenberg\n",
    "   - Train on Wikipedia articles\n",
    "   - Compare results with the small corpus\n",
    "\n",
    "3. **Implement CBOW:**\n",
    "   - Modify the model to implement CBOW instead of Skip-gram\n",
    "   - Compare the results\n",
    "\n",
    "4. **Explore bias:**\n",
    "   - Test for gender bias in embeddings\n",
    "   - Try debiasing techniques\n",
    "\n",
    "5. **Advanced visualizations:**\n",
    "   - Create interactive plots with plotly\n",
    "   - Visualize semantic clusters\n",
    "   - Show analogy vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you have:\n",
    "- Built Word2Vec from scratch in PyTorch\n",
    "- Implemented the Skip-gram architecture with negative sampling\n",
    "- Trained embeddings on text data\n",
    "- Explored semantic relationships and analogies\n",
    "- Visualized embedding spaces\n",
    "- Learned how to save and load trained models\n",
    "\n",
    "Key takeaways:\n",
    "- Word embeddings capture semantic relationships\n",
    "- Similar words have similar vectors\n",
    "- Vector arithmetic can encode analogies\n",
    "- Quality depends on corpus size and diversity\n",
    "\n",
    "Next week: Using these embeddings in Recurrent Neural Networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
