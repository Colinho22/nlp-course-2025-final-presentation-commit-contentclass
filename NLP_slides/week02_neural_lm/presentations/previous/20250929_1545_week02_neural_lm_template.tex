% Week 2: Neural Language Models and Word Embeddings
% Professional Template Edition - Enhanced Version
% Created: 2025-09-29 15:45
% 45+ slides following template_beamer_final.tex structure

\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}

% Professional lavender color scheme from template
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255,127,14}
\definecolor{mlgreen}{RGB}{44,160,44}
\definecolor{mlred}{RGB}{214,39,40}
\definecolor{mlgray}{RGB}{127,127,127}
\definecolor{lightgray}{RGB}{240,240,240}
\definecolor{midgray}{RGB}{180,180,180}

% Apply colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}
\setbeamercolor{itemize item}{fg=mlpurple}
\setbeamercolor{itemize subitem}{fg=mlblue}
\setbeamercolor{enumerate item}{fg=mlpurple}
\setbeamercolor{enumerate subitem}{fg=mlblue}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Custom commands
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\newcommand{\highlight}[1]{\textcolor{mlblue}{\textbf{#1}}}
\newcommand{\eqbox}[1]{\colorbox{mllavender4}{$\displaystyle #1$}}
\newcommand{\given}{\mid}
\newcommand{\prob}[1]{P(#1)}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}

% Code listing settings
\lstset{
    backgroundcolor=\color{lightgray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    commentstyle=\color{mlgreen},
    keywordstyle=\color{mlblue},
    stringstyle=\color{mlpurple},
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{midgray},
    language=Python
}

\title{Neural Language Models and Word Embeddings}
\subtitle{Week 2: From Discrete Symbols to Continuous Representations}
\author{NLP Course 2025}
\institute{Professional Template Edition}
\date{\today}

\begin{document}

% ==================== TITLE SLIDE ====================
\begin{frame}[plain]
\titlepage
\end{frame}

% ==================== TABLE OF CONTENTS ====================
\begin{frame}[t]{Week 2: Comprehensive Overview}
\tableofcontents
\vfill
\footnotesize
\textbf{Learning Journey:} From one-hot encodings to dense semantic vectors. Master Word2Vec, understand negative sampling, explore semantic arithmetic, and build the foundation for modern NLP.
\end{frame}

% ==================== LEARNING OBJECTIVES ====================
\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this lecture, you will:}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Understand}
\begin{itemize}
\item Why one-hot encodings fail
\item Distributional hypothesis principle
\item Word2Vec architectures (CBOW vs Skip-gram)
\item Negative sampling optimization
\item Semantic vector arithmetic
\end{itemize}

\column{0.48\textwidth}
\textbf{Be Able To}
\begin{itemize}
\item Train word embeddings
\item Evaluate embedding quality
\item Visualize semantic spaces
\item Apply embeddings to NLP tasks
\item Debug common issues
\end{itemize}
\end{columns}

\vspace{8mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.85\textwidth}{
\centering
\textbf{Core Message:} Words are not just symbols - they carry meaning in their vector geometry
}
}
\end{center}

\bottomnote{Prerequisites: Basic linear algebra, probability theory, neural network fundamentals}
\end{frame}

% ==================== PART 1: MOTIVATION ====================
\section{Part 1: Motivation \& Problem Setting}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center,shadow=true]{title}
\usebeamerfont{title}\Large Part 1: Motivation \& Problem Setting\par
\vspace{5mm}
\normalsize \textit{Why do we need word embeddings?}
\end{beamercolorbox}
\vfill
\end{frame}

% Interactive Word Association
\begin{frame}[t]{Interactive Exercise: Word Association}
\textbf{Complete these sentences - What word naturally comes next?}

\vspace{8mm}
\begin{columns}[T]
\column{0.48\textwidth}
\begin{enumerate}
\item The cat sat on the \underline{\hspace{2cm}}
\pause
\item[] \textcolor{mlgreen}{mat, floor, chair}

\vspace{5mm}
\item Coffee with milk and \underline{\hspace{2cm}}
\pause
\item[] \textcolor{mlgreen}{sugar, cream}

\vspace{5mm}
\item The capital of France is \underline{\hspace{2cm}}
\pause
\item[] \textcolor{mlgreen}{Paris}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item She was happy but also \underline{\hspace{2cm}}
\pause
\item[] \textcolor{mlgreen}{sad, anxious, tired}

\vspace{5mm}
\item King is to queen as man is to \underline{\hspace{2cm}}
\pause
\item[] \textcolor{mlgreen}{woman}

\vspace{5mm}
\item Python is a programming \underline{\hspace{2cm}}
\pause
\item[] \textcolor{mlgreen}{language}
\end{enumerate}
\end{columns}

\vspace{8mm}
\pause
\begin{center}
\colorbox{mllavender4}{
\parbox{0.75\textwidth}{
\centering
\textbf{Key Insight:} You use semantic understanding to predict words!
}
}
\end{center}

\bottomnote{Humans naturally understand word relationships - how can we teach this to machines?}
\end{frame}

% Evolution Timeline
\begin{frame}[t]{Historical Context: The Evolution Journey}
\centering
\includegraphics[width=0.85\textwidth]{../figures/evolution_timeline.pdf}

\vspace{5mm}
\begin{columns}[T]
\column{0.24\textwidth}
\textbf{1980s-2000s}
\begin{itemize}
\item N-gram models
\item Count-based
\item No semantics
\item Sparse data issues
\end{itemize}

\column{0.24\textwidth}
\textbf{2003-2013}
\begin{itemize}
\item Neural LMs
\item Bengio et al. 2003
\item Dense representations
\item Limited scale
\end{itemize}

\column{0.24\textwidth}
\textbf{2013-2017}
\begin{itemize}
\item \highlight{Word2Vec era}
\item Semantic learning
\item Efficient training
\item Wide adoption
\end{itemize}

\column{0.24\textwidth}
\textbf{2017-Present}
\begin{itemize}
\item Transformers
\item Contextual embeddings
\item BERT, GPT
\item Foundation models
\end{itemize}
\end{columns}

\bottomnote{Word2Vec (2013) was the breakthrough that made word representations practical at scale}
\end{frame}

% The Fundamental Problem
\begin{frame}[t]{The Fundamental Problem: One-Hot Encoding}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Traditional Approach}
\begin{itemize}
\item Vocabulary size: 50,000 words
\item Each word = unique ID
\item One-hot vectors:
\end{itemize}

\vspace{3mm}
\footnotesize
\begin{tabular}{ll}
cat = & [0,0,1,0,0,...,0] \\
dog = & [0,0,0,1,0,...,0] \\
car = & [0,0,0,0,1,...,0] \\
democracy = & [0,0,0,0,0,...,1]
\end{tabular}

\vspace{5mm}
\normalsize
\textcolor{mlred}{\textbf{Critical Problems:}}
\begin{itemize}
\item No similarity information
\item distance(cat, dog) = distance(cat, car)
\item 50,000-dimensional vectors!
\item Can't generalize patterns
\end{itemize}

\column{0.48\textwidth}
\textbf{Mathematical Issues}

\vspace{3mm}
Cosine similarity between any two words:
\[\cos(\vec{w_i}, \vec{w_j}) = \frac{\vec{w_i} \cdot \vec{w_j}}{|\vec{w_i}| \cdot |\vec{w_j}|} = 0\]

\vspace{5mm}
All words are orthogonal!

\vspace{5mm}
\textbf{Storage nightmare:}
\begin{itemize}
\item 1M vocabulary = 1M dimensions
\item Neural network input: 1M × hidden size
\item Impossible to scale
\end{itemize}
\end{columns}

\bottomnote{One-hot encoding treats all words as equally different - destroying semantic information}
\end{frame}

% Why It Matters
\begin{frame}[t]{Why This Problem Matters}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Example Task: Sentiment Analysis}

Training data:
\begin{itemize}
\item ``This movie is \textcolor{mlgreen}{great}'' → Positive
\item ``This film is \textcolor{mlgreen}{excellent}'' → ?
\end{itemize}

\vspace{5mm}
With one-hot encoding:
\begin{itemize}
\item ``great'' and ``excellent'' unrelated
\item Can't transfer knowledge
\item Need examples for every word
\end{itemize}

\vspace{5mm}
With embeddings:
\begin{itemize}
\item Similar words → similar vectors
\item Knowledge transfers automatically
\item Generalizes to unseen combinations
\end{itemize}

\column{0.48\textwidth}
\textbf{Real-World Impact}

\vspace{3mm}
\textcolor{mlblue}{\textbf{Search Engines:}}
\begin{itemize}
\item Query: ``cheap cars''
\item Finds: ``affordable vehicles''
\end{itemize}

\vspace{3mm}
\textcolor{mlgreen}{\textbf{Translation:}}
\begin{itemize}
\item Unseen word combinations
\item Cross-lingual transfer
\end{itemize}

\vspace{3mm}
\textcolor{mlorange}{\textbf{Recommendations:}}
\begin{itemize}
\item Similar content discovery
\item User preference modeling
\end{itemize}

\vspace{3mm}
\textcolor{mlpurple}{\textbf{Chatbots:}}
\begin{itemize}
\item Intent understanding
\item Paraphrase detection
\end{itemize}
\end{columns}

\bottomnote{Dense embeddings enable knowledge transfer - the foundation of modern NLP success}
\end{frame}

% Solution Preview
\begin{frame}[t]{The Solution: Dense Distributed Representations}
\centering
\includegraphics[width=0.8\textwidth]{../figures/sparse_to_dense.pdf}

\vspace{5mm}
\begin{columns}[T]
\column{0.31\textwidth}
\textbf{From Sparse...}
\begin{itemize}
\item 50,000 dimensions
\item Only one 1, rest 0s
\item No similarity info
\item Huge storage
\end{itemize}

\column{0.31\textwidth}
\textbf{...To Dense}
\begin{itemize}
\item 100-300 dimensions
\item Real-valued vectors
\item Encodes semantics
\item 99\% smaller!
\end{itemize}

\column{0.31\textwidth}
\textbf{Benefits}
\begin{itemize}
\item Similar words cluster
\item Smooth transitions
\item Enables arithmetic
\item Efficient computation
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.75\textwidth}{
\centering
cat = [0.2, -0.4, 0.7, 0.1, ...] (300d) \\
dog = [0.3, -0.3, 0.8, 0.2, ...] (300d) \\
similarity(cat, dog) = 0.85
}
}
\end{center}

\bottomnote{Dense vectors capture meaning in their geometry - similar words have similar vectors}
\end{frame}

% Real Applications Dashboard
\begin{frame}[t]{Real-World Applications Today}
\centering
\includegraphics[width=0.85\textwidth]{../figures/applications_dashboard.pdf}

\vspace{5mm}
\begin{columns}[T]
\column{0.24\textwidth}
\centering
\textbf{Google Search}
\begin{itemize}
\item 8.5B searches/day
\item Semantic matching
\item Query expansion
\end{itemize}

\column{0.24\textwidth}
\centering
\textbf{Netflix}
\begin{itemize}
\item 230M users
\item Content similarity
\item Recommendation
\end{itemize}

\column{0.24\textwidth}
\centering
\textbf{ChatGPT}
\begin{itemize}
\item 100M+ users
\item Foundation layer
\item Context understanding
\end{itemize}

\column{0.24\textwidth}
\centering
\textbf{Grammarly}
\begin{itemize}
\item 30M daily users
\item Style suggestions
\item Paraphrase detection
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender3}{
\parbox{0.8\textwidth}{
\centering
\Large
\textbf{Market Impact: \$100+ Billion powered by embeddings}
}
}
\end{center}

\bottomnote{Word embeddings are the hidden foundation powering virtually all modern NLP applications}
\end{frame}

% ==================== PART 2: CORE THEORY ====================
\section{Part 2: Core Theory \& Architecture}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center,shadow=true]{title}
\usebeamerfont{title}\Large Part 2: Core Theory \& Architecture\par
\vspace{5mm}
\normalsize \textit{How Word2Vec learns meaning from context}
\end{beamercolorbox}
\vfill
\end{frame}

% Distributional Hypothesis
\begin{frame}[t]{The Distributional Hypothesis}
\begin{center}
\Large
``You shall know a word by the company it keeps''
\end{center}
\textit{- J.R. Firth (1957)}

\vspace{8mm}
\textbf{Core Principle:} Words appearing in similar contexts have similar meanings

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Example Sentences:}
\begin{itemize}
\item The \textcolor{mlblue}{cat} sat on the mat
\item The \textcolor{mlgreen}{dog} sat on the floor
\item A \textcolor{mlblue}{cat} chased the mouse
\item A \textcolor{mlgreen}{dog} chased the ball
\item The \textcolor{mlblue}{cat} is sleeping
\item The \textcolor{mlgreen}{dog} is sleeping
\end{itemize}

\vspace{3mm}
\textbf{Shared Contexts:}
\begin{itemize}
\item Both follow ``The'' and ``A''
\item Both precede ``sat'', ``chased'', ``is''
\item Similar syntactic positions
\end{itemize}

\column{0.48\textwidth}
\textbf{Context Windows:}

\vspace{3mm}
Window size = 2:
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
The & cat & \cellcolor{mllavender3}sat & on & the \\
\hline
\multicolumn{2}{|c|}{←context} & target & \multicolumn{2}{|c|}{context→} \\
\hline
\end{tabular}
\end{center}

\vspace{5mm}
\textbf{Mathematical View:}
\begin{align*}
\text{context}(\text{cat}) &= \{the, sat, on, chased, is, ...\} \\
\text{context}(\text{dog}) &= \{the, sat, on, chased, is, ...\} \\
&\Downarrow \\
\text{similar contexts} &\Rightarrow \text{similar vectors}
\end{align*}
\end{columns}

\bottomnote{This simple linguistic observation drives all modern word embedding methods}
\end{frame}

% Word2Vec Architectures
\begin{frame}[t]{Word2Vec: Two Complementary Architectures}
\centering
\includegraphics[width=0.85\textwidth]{../figures/word2vec_architectures.pdf}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{CBOW (Continuous Bag-of-Words)}
\begin{itemize}
\item Predict center from context
\item Input: [the, cat, on, the]
\item Output: sat
\item Average context vectors
\item Faster training
\item Better for frequent words
\end{itemize}

\vspace{3mm}
\textbf{Architecture:}
\begin{enumerate}
\item Look up context embeddings
\item Average them
\item Linear layer
\item Softmax prediction
\end{enumerate}

\column{0.48\textwidth}
\textbf{Skip-gram}
\begin{itemize}
\item Predict context from center
\item Input: sat
\item Output: [the, cat, on, the]
\item Multiple predictions
\item Better quality vectors
\item Better for rare words
\end{itemize}

\vspace{3mm}
\textbf{Architecture:}
\begin{enumerate}
\item Look up center embedding
\item Linear layer
\item Multiple softmax predictions
\item Sum losses
\end{enumerate}
\end{columns}

\bottomnote{Skip-gram became the standard - predicting multiple contexts creates richer representations}
\end{frame}

% Skip-gram Mathematics
\begin{frame}[t]{Skip-gram: Mathematical Foundation}
\textbf{Objective: Maximize probability of context words given center word}

\vspace{5mm}
\textbf{Formal Objective Function:}
\[\eqbox{J(\theta) = \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log \prob{w_{t+j} \given w_t}}\]

Where:
\begin{itemize}
\item $T$ = Total words in corpus
\item $c$ = Context window size
\item $w_t$ = Center word at position $t$
\item $w_{t+j}$ = Context word at offset $j$
\end{itemize}

\vspace{5mm}
\textbf{Probability using Softmax:}
\[\prob{w_O \given w_I} = \frac{\exp(v_{w_O}^T \cdot v_{w_I})}{\sum_{w=1}^{|V|} \exp(v_w^T \cdot v_{w_I})}\]

\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Two Embedding Matrices:}
\begin{itemize}
\item $W_{in} \in \mathbb{R}^{|V| \times d}$ (input embeddings)
\item $W_{out} \in \mathbb{R}^{|V| \times d}$ (output embeddings)
\end{itemize}

\column{0.48\textwidth}
\textcolor{mlred}{\textbf{Computational Problem:}}
\begin{itemize}
\item Denominator sums over entire vocabulary
\item 50,000 words = 50,000 exponentials!
\item Too expensive per training step
\end{itemize}
\end{columns}

\bottomnote{The softmax normalization is beautiful mathematically but computationally prohibitive}
\end{frame}

% Negative Sampling
\begin{frame}[t]{Negative Sampling: Making Training Feasible}
\textbf{Key Idea: Convert to binary classification problem}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Instead of Full Softmax:}
\begin{itemize}
\item Classify real vs fake pairs
\item Positive: (center, actual context)
\item Negative: (center, random word)
\item Much faster computation
\end{itemize}

\vspace{5mm}
\textbf{Example:}
\begin{itemize}
\item Center word: ``sat''
\item Positive: (sat, cat) → 1
\item Negative samples:
  \begin{itemize}
  \item (sat, democracy) → 0
  \item (sat, quantum) → 0
  \item (sat, purple) → 0
  \end{itemize}
\end{itemize}

\column{0.48\textwidth}
\textbf{New Objective Function:}
\[\log \sigmoid(v_{w_O}^T \cdot v_{w_I}) + \]
\[\sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigmoid(-v_{w_i}^T \cdot v_{w_I})]\]

Where:
\begin{itemize}
\item $\sigmoid(x) = \frac{1}{1 + e^{-x}}$
\item $k$ = number of negative samples (typically 5-20)
\item $P_n(w)$ = noise distribution $\propto f(w)^{3/4}$
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.8\textwidth}{
\centering
\textbf{Computational Speedup:} From O(|V|) to O(k) where k << |V|
}
}
\end{center}

\bottomnote{Negative sampling reduces 50,000 computations to just 5-20 - enabling practical training}
\end{frame}

% Training Process Visualization
\begin{frame}[t]{Training Dynamics: How Embeddings Evolve}
\centering
\includegraphics[width=0.85\textwidth]{../figures/training_dynamics.pdf}

\vspace{5mm}
\begin{columns}[T]
\column{0.24\textwidth}
\textbf{Epoch 0}
\begin{itemize}
\item Random initialization
\item No structure
\item High loss
\item Random predictions
\end{itemize}

\column{0.24\textwidth}
\textbf{Epoch 10}
\begin{itemize}
\item Clusters forming
\item Syntactic groups
\item Loss decreasing
\item Basic patterns
\end{itemize}

\column{0.24\textwidth}
\textbf{Epoch 25}
\begin{itemize}
\item Clear semantics
\item Topic clusters
\item Analogies emerging
\item Good accuracy
\end{itemize}

\column{0.24\textwidth}
\textbf{Epoch 50}
\begin{itemize}
\item Fine-grained structure
\item Rich relationships
\item Low loss
\item Ready to use
\end{itemize}
\end{columns}

\vspace{5mm}
\textbf{Training Details:}
\begin{itemize}
\item Learning rate: 0.025 (linear decay)
\item Min count: 5 (ignore rare words)
\item Iterations: 5-50 epochs
\item Subsampling frequent words: $P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}$
\end{itemize}

\bottomnote{Training typically converges in 5-50 epochs depending on corpus size and quality}
\end{frame}

% Implementation Details
\begin{frame}[fragile,t]{Implementation: Critical Details}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Preprocessing:}
\begin{itemize}
\item Lowercase text
\item Remove punctuation (optional)
\item Build vocabulary
\item Subsampling threshold: $10^{-3}$
\end{itemize}

\vspace{3mm}
\textbf{Initialization:}
\begin{lstlisting}[basicstyle=\tiny\ttfamily]
# Random init [-0.5, 0.5] / dim
W_in = (np.random.rand(V, d) - 0.5) / d
W_out = np.zeros((V, d))
\end{lstlisting}

\vspace{3mm}
\textbf{Hyperparameters:}
\begin{itemize}
\item Embedding dim: 100-300
\item Window size: 5-10
\item Negative samples: 5-20
\item Min word frequency: 5
\end{itemize}

\column{0.48\textwidth}
\textbf{Optimization Tricks:}

\vspace{3mm}
\textbf{1. Hierarchical Softmax:}
\begin{itemize}
\item Binary tree over vocabulary
\item O(log |V|) instead of O(|V|)
\item Good for large vocabularies
\end{itemize}

\vspace{3mm}
\textbf{2. Subsampling Frequent Words:}
\begin{itemize}
\item ``the'', ``a'', ``is'' less informative
\item Probability of keeping: $P(w_i) = \sqrt{\frac{t}{f(w_i)}}$
\item Speeds training, improves quality
\end{itemize}

\vspace{3mm}
\textbf{3. Dynamic Window:}
\begin{itemize}
\item Actual window: random(1, window\_size)
\item Gives more weight to closer words
\end{itemize}
\end{columns}

\bottomnote{These implementation details make the difference between mediocre and excellent embeddings}
\end{frame}

% ==================== CHECKPOINT QUIZ ====================
\begin{frame}[t]{Checkpoint Quiz: Understanding Check}
\textbf{Test your understanding so far:}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Questions:}

\vspace{3mm}
\textbf{Q1:} Why do we need dense embeddings?

\vspace{8mm}
\textbf{Q2:} What's the key insight of distributional hypothesis?

\vspace{8mm}
\textbf{Q3:} Skip-gram vs CBOW - which to use?

\vspace{8mm}
\textbf{Q4:} Why negative sampling?

\column{0.48\textwidth}
\pause
\textbf{Answers:}

\vspace{3mm}
\textbf{A1:} One-hot is sparse, no similarity info, can't generalize

\vspace{5mm}
\textbf{A2:} Words with similar contexts have similar meanings

\vspace{5mm}
\textbf{A3:} Skip-gram for quality, CBOW for speed/frequent words

\vspace{5mm}
\textbf{A4:} Avoid expensive softmax over entire vocabulary
\end{columns}

\vspace{8mm}
\pause
\begin{center}
\colorbox{mllavender4}{
\parbox{0.75\textwidth}{
\centering
\textbf{If you understand these, you grasp the core of Word2Vec!}
}
}
\end{center}

\bottomnote{These four concepts form the foundation of all modern word embedding methods}
\end{frame}

% ==================== PART 3: PROPERTIES & EVALUATION ====================
\section{Part 3: Properties \& Evaluation}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center,shadow=true]{title}
\usebeamerfont{title}\Large Part 3: Properties \& Evaluation\par
\vspace{5mm}
\normalsize \textit{The surprising geometry of meaning}
\end{beamercolorbox}
\vfill
\end{frame}

% Semantic Arithmetic
\begin{frame}[t]{The Magic: Semantic Vector Arithmetic}
\centering
\includegraphics[width=0.8\textwidth]{../figures/semantic_arithmetic.pdf}

\vspace{5mm}
\textbf{Relationships are encoded as vector operations!}

\vspace{5mm}
\begin{columns}[T]
\column{0.31\textwidth}
\textbf{Gender Relations:}
\begin{itemize}
\item king - man + woman = \textcolor{mlgreen}{queen}
\item actor - man + woman = \textcolor{mlgreen}{actress}
\item he - man + woman = \textcolor{mlgreen}{she}
\end{itemize}

\column{0.31\textwidth}
\textbf{Geographic:}
\begin{itemize}
\item Paris - France + Italy = \textcolor{mlgreen}{Rome}
\item Tokyo - Japan + Germany = \textcolor{mlgreen}{Berlin}
\item Madrid - Spain + UK = \textcolor{mlgreen}{London}
\end{itemize}

\column{0.31\textwidth}
\textbf{Grammar:}
\begin{itemize}
\item walked - walk + run = \textcolor{mlgreen}{ran}
\item better - good + bad = \textcolor{mlgreen}{worse}
\item biggest - big + small = \textcolor{mlgreen}{smallest}
\end{itemize}
\end{columns}

\vspace{5mm}
\textbf{Mathematical Explanation:}
\[\vec{v}_{king} - \vec{v}_{man} \approx \vec{v}_{queen} - \vec{v}_{woman} = \text{``royalty vector''}\]

\bottomnote{These analogies emerge automatically from training - no explicit programming!}
\end{frame}

% Embedding Space Visualization
\begin{frame}[t]{Visualizing the Semantic Space}
\centering
\includegraphics[width=0.75\textwidth]{../figures/embedding_space_3d.pdf}

\vspace{3mm}
\begin{columns}[T]
\column{0.24\textwidth}
\textbf{Animals}
\begin{itemize}
\item cat
\item dog
\item mouse
\item elephant
\end{itemize}

\column{0.24\textwidth}
\textbf{Countries}
\begin{itemize}
\item France
\item Germany
\item Japan
\item Brazil
\end{itemize}

\column{0.24\textwidth}
\textbf{Verbs}
\begin{itemize}
\item run/ran
\item walk/walked
\item eat/ate
\item sleep/slept
\end{itemize}

\column{0.24\textwidth}
\textbf{Professions}
\begin{itemize}
\item doctor
\item engineer
\item teacher
\item artist
\end{itemize}
\end{columns}

\vspace{5mm}
\textbf{Key Observations:}
\begin{itemize}
\item Semantic clusters form automatically
\item Grammatical relationships preserved
\item Smooth transitions between concepts
\item Multiple organizational principles simultaneously
\end{itemize}

\bottomnote{t-SNE projection reveals the rich structure learned purely from context patterns}
\end{frame}

% Nearest Neighbors
\begin{frame}[t]{Exploring Neighborhoods: Semantic Similarity}
\textbf{Finding nearest neighbors reveals semantic understanding:}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Query: ``king''}
\begin{enumerate}
\item queen (0.72)
\item prince (0.70)
\item monarch (0.68)
\item emperor (0.65)
\item royal (0.64)
\end{enumerate}

\vspace{5mm}
\textbf{Query: ``computer''}
\begin{enumerate}
\item laptop (0.78)
\item PC (0.75)
\item software (0.71)
\item desktop (0.70)
\item machine (0.68)
\end{enumerate}

\column{0.48\textwidth}
\textbf{Query: ``beautiful''}
\begin{enumerate}
\item gorgeous (0.83)
\item lovely (0.81)
\item stunning (0.78)
\item pretty (0.76)
\item wonderful (0.72)
\end{enumerate}

\vspace{5mm}
\textbf{Query: ``Microsoft''}
\begin{enumerate}
\item Google (0.75)
\item Apple (0.73)
\item IBM (0.71)
\item Amazon (0.70)
\item Intel (0.68)
\end{enumerate}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.75\textwidth}{
\centering
\textbf{Similarity = Cosine Distance in Embedding Space}
}
}
\end{center}

\bottomnote{Nearest neighbors reveal both synonyms and related concepts in the same category}
\end{frame}

% Evaluation Methods
\begin{frame}[t]{Evaluation: How Good Are Your Embeddings?}
\begin{columns}[T]
\column{0.31\textwidth}
\textbf{1. Intrinsic Evaluation}

\vspace{3mm}
\textbf{Word Similarity:}
\begin{itemize}
\item WordSim-353
\item SimLex-999
\item MEN dataset
\end{itemize}

\vspace{3mm}
\textbf{Analogies:}
\begin{itemize}
\item Google analogy test
\item BATS dataset
\item Semantic/syntactic
\end{itemize}

\vspace{3mm}
\textbf{Metrics:}
\begin{itemize}
\item Spearman correlation
\item Accuracy@1, @5
\end{itemize}

\column{0.31\textwidth}
\textbf{2. Extrinsic Evaluation}

\vspace{3mm}
\textbf{Downstream Tasks:}
\begin{itemize}
\item Sentiment analysis
\item Named entity recognition
\item POS tagging
\end{itemize}

\vspace{3mm}
\textbf{Improvements:}
\begin{itemize}
\item +3-5\% accuracy typical
\item Faster convergence
\item Better generalization
\end{itemize}

\vspace{3mm}
\textbf{Transfer Learning:}
\begin{itemize}
\item Cross-domain
\item Cross-lingual
\end{itemize}

\column{0.31\textwidth}
\textbf{3. Visualization}

\vspace{3mm}
\textbf{Techniques:}
\begin{itemize}
\item t-SNE projection
\item PCA analysis
\item UMAP
\end{itemize}

\vspace{3mm}
\textbf{Quality Checks:}
\begin{itemize}
\item Cluster coherence
\item Outlier detection
\item Coverage analysis
\end{itemize}

\vspace{3mm}
\textbf{Tools:}
\begin{itemize}
\item TensorBoard
\item Embedding Projector
\item Custom notebooks
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender3}{
\parbox{0.85\textwidth}{
\centering
\textbf{Good embeddings:} 0.6+ correlation on similarity, 80\%+ on analogies, clear visual clusters
}
}
\end{center}

\bottomnote{Always evaluate with multiple methods - no single metric tells the complete story}
\end{frame}

% Common Analogy Categories
\begin{frame}[t]{Analogy Categories: Testing Semantic Understanding}
\textbf{Google Analogy Test Set: 19,544 questions across categories}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Semantic (8,869 questions)}

\vspace{3mm}
\textcolor{mlblue}{Capital-Country (506):}
\begin{itemize}
\item Athens : Greece :: Tokyo : Japan
\item Paris : France :: Rome : Italy
\end{itemize}

\vspace{3mm}
\textcolor{mlgreen}{Currency (866):}
\begin{itemize}
\item USA : dollar :: Japan : yen
\item Europe : euro :: UK : pound
\end{itemize}

\vspace{3mm}
\textcolor{mlorange}{Family (506):}
\begin{itemize}
\item brother : sister :: father : mother
\item uncle : aunt :: nephew : niece
\end{itemize}

\column{0.48\textwidth}
\textbf{Syntactic (10,675 questions)}

\vspace{3mm}
\textcolor{mlpurple}{Plural (1,332):}
\begin{itemize}
\item cat : cats :: dog : dogs
\item child : children :: person : people
\end{itemize}

\vspace{3mm}
\textcolor{mlred}{Past Tense (1,560):}
\begin{itemize}
\item walk : walked :: run : ran
\item see : saw :: go : went
\end{itemize}

\vspace{3mm}
\textcolor{mlgray}{Comparative (1,332):}
\begin{itemize}
\item good : better :: bad : worse
\item big : bigger :: small : smaller
\end{itemize}
\end{columns}

\vspace{5mm}
\textbf{Typical Performance: Word2Vec on 1B word corpus}
\begin{itemize}
\item Semantic analogies: 75-85\% accuracy
\item Syntactic analogies: 70-80\% accuracy
\end{itemize}

\bottomnote{Analogies test if embeddings capture both meaning and grammatical patterns}
\end{frame}

% Limitations
\begin{frame}[t]{Limitations and Challenges}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Fundamental Limitations:}

\vspace{3mm}
\textcolor{mlred}{\textbf{1. Single Vector per Word}}
\begin{itemize}
\item ``bank'' (river) = ``bank'' (financial)
\item No polysemy handling
\item Context-independent
\end{itemize}

\vspace{3mm}
\textcolor{mlred}{\textbf{2. Out-of-Vocabulary}}
\begin{itemize}
\item Can't handle new words
\item Misspellings break
\item Rare words poorly represented
\end{itemize}

\vspace{3mm}
\textcolor{mlred}{\textbf{3. Bias in Data}}
\begin{itemize}
\item ``doctor'' closer to ``he''
\item ``nurse'' closer to ``she''
\item Amplifies stereotypes
\end{itemize}

\column{0.48\textwidth}
\textbf{Solutions \& Evolution:}

\vspace{3mm}
\textcolor{mlgreen}{\textbf{FastText (2016):}}
\begin{itemize}
\item Character n-grams
\item Handles OOV words
\item Better for morphology
\end{itemize}

\vspace{3mm}
\textcolor{mlgreen}{\textbf{ELMo (2018):}}
\begin{itemize}
\item Context-dependent
\item Different vectors per use
\item Bidirectional LSTM
\end{itemize}

\vspace{3mm}
\textcolor{mlgreen}{\textbf{BERT (2018):}}
\begin{itemize}
\item Transformer-based
\item Deeply contextual
\item Masked language modeling
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.8\textwidth}{
\centering
\textbf{Word2Vec's limitations directly inspired the transformer revolution}
}
}
\end{center}

\bottomnote{Understanding these limitations is crucial for choosing the right embedding method}
\end{frame}

% ==================== PART 4: IMPLEMENTATION ====================
\section{Part 4: Practical Implementation}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center,shadow=true]{title}
\usebeamerfont{title}\Large Part 4: Practical Implementation\par
\vspace{5mm}
\normalsize \textit{From theory to practice}
\end{beamercolorbox}
\vfill
\end{frame}

% Code Example - Training
\begin{frame}[fragile,t]{Implementation: Training Word2Vec}
\begin{lstlisting}
import gensim.downloader as api
from gensim.models import Word2Vec

# Load corpus (or use your own)
corpus = api.load('text8')  # Wikipedia sample

# Train Word2Vec model
model = Word2Vec(
    sentences=corpus,
    vector_size=300,      # Embedding dimension
    window=5,             # Context window
    min_count=5,          # Ignore rare words
    workers=4,            # Parallel training
    sg=1,                 # 1=skip-gram, 0=CBOW
    negative=15,          # Negative samples
    epochs=5,             # Training iterations
    seed=42              # Reproducibility
)

# Save model
model.save("word2vec.model")

# Get vocabulary size
print(f"Vocabulary: {len(model.wv)} words")
print(f"Embedding shape: {model.wv.vectors.shape}")
\end{lstlisting}

\bottomnote{Gensim makes Word2Vec incredibly accessible - production-quality embeddings in minutes}
\end{frame}

% Code Example - Using Embeddings
\begin{frame}[fragile,t]{Using Pre-trained Embeddings}
\begin{lstlisting}
# Load pre-trained model
model = Word2Vec.load("word2vec.model")

# 1. Get word vector
king_vector = model.wv['king']
print(f"Vector for 'king': {king_vector[:5]}...")

# 2. Find similar words
similar = model.wv.most_similar('computer', topn=5)
for word, score in similar:
    print(f"{word}: {score:.3f}")

# 3. Word analogies
result = model.wv.most_similar(
    positive=['king', 'woman'],
    negative=['man'],
    topn=3
)
print(f"king - man + woman = {result[0][0]}")

# 4. Similarity scores
sim = model.wv.similarity('cat', 'dog')
print(f"Similarity(cat, dog) = {sim:.3f}")

# 5. Odd one out
odd = model.wv.doesnt_match(['breakfast', 'lunch', 'dinner', 'car'])
print(f"Odd one out: {odd}")
\end{lstlisting}

\bottomnote{Pre-trained embeddings can be used immediately for various NLP tasks}
\end{frame}

% Advanced Training Tips
\begin{frame}[t]{Advanced Training: Tips from Experience}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Data Preparation:}
\begin{itemize}
\item \textbf{Corpus Size:} Minimum 1M words
\item \textbf{Preprocessing:}
  \begin{itemize}
  \item Lowercase consistently
  \item Keep punctuation (context!)
  \item Handle numbers: ``123'' → ``NUM''
  \end{itemize}
\item \textbf{Sentence Boundaries:} Respect them
\item \textbf{Domain-Specific:} Add your data
\end{itemize}

\vspace{5mm}
\textbf{Hyperparameter Tuning:}
\begin{itemize}
\item \textbf{Dimension:} 100 (small), 300 (standard)
\item \textbf{Window:} 5 (syntax), 10 (semantics)
\item \textbf{Min Count:} 5-10 typical
\item \textbf{Negative:} 5-20 (more is better)
\end{itemize}

\column{0.48\textwidth}
\textbf{Training Strategy:}
\begin{itemize}
\item \textbf{Epochs:} 5-10 for large corpus
\item \textbf{Learning Rate:} Start 0.025, decay
\item \textbf{Subsampling:} $t = 10^{-5}$ typical
\item \textbf{Threads:} Use multiple cores
\end{itemize}

\vspace{5mm}
\textbf{Quality Checks:}
\begin{itemize}
\item Monitor loss curve
\item Check known analogies
\item Visualize samples with t-SNE
\item Test on downstream task early
\item Compare against baselines
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.8\textwidth}{
\centering
\textbf{Pro Tip:} Start with pre-trained, fine-tune on your domain
}
}
\end{center}

\bottomnote{Good embeddings require careful attention to both data and hyperparameters}
\end{frame}

% Debugging Common Issues
\begin{frame}[t]{Debugging: Common Issues and Solutions}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Problem 1: Poor Analogies}
\begin{itemize}
\item \textcolor{mlred}{Symptom:} Random results
\item \textcolor{mlgreen}{Causes:}
  \begin{itemize}
  \item Too small corpus
  \item Too few epochs
  \item Window too small
  \end{itemize}
\item \textcolor{mlblue}{Fix:} More data, longer training
\end{itemize}

\vspace{3mm}
\textbf{Problem 2: Memory Issues}
\begin{itemize}
\item \textcolor{mlred}{Symptom:} Out of memory
\item \textcolor{mlgreen}{Causes:}
  \begin{itemize}
  \item Vocabulary too large
  \item Dimension too high
  \end{itemize}
\item \textcolor{mlblue}{Fix:} Increase min\_count, reduce dim
\end{itemize}

\column{0.48\textwidth}
\textbf{Problem 3: Slow Training}
\begin{itemize}
\item \textcolor{mlred}{Symptom:} Takes days
\item \textcolor{mlgreen}{Causes:}
  \begin{itemize}
  \item Single thread
  \item No negative sampling
  \item Full softmax
  \end{itemize}
\item \textcolor{mlblue}{Fix:} Use workers, negative sampling
\end{itemize}

\vspace{3mm}
\textbf{Problem 4: Domain Mismatch}
\begin{itemize}
\item \textcolor{mlred}{Symptom:} Works poorly on your data
\item \textcolor{mlgreen}{Causes:}
  \begin{itemize}
  \item Pre-trained on different domain
  \item Technical jargon missing
  \end{itemize}
\item \textcolor{mlblue}{Fix:} Fine-tune or train from scratch
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender3}{
\parbox{0.75\textwidth}{
\centering
\textbf{Golden Rule: Always validate on your specific use case}
}
}
\end{center}

\bottomnote{Most embedding issues stem from data quality or hyperparameter mismatches}
\end{frame}

% Integration with Deep Learning
\begin{frame}[fragile,t]{Integration: Embeddings in Neural Networks}
\begin{lstlisting}
import torch
import torch.nn as nn
import numpy as np

class SentimentClassifier(nn.Module):
    def __init__(self, word2vec_model, num_classes=2):
        super().__init__()

        # Load pre-trained embeddings
        weights = word2vec_model.wv.vectors
        vocab_size, embed_dim = weights.shape

        # Embedding layer (frozen initially)
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.embedding.weight = nn.Parameter(torch.FloatTensor(weights))
        self.embedding.weight.requires_grad = False  # Freeze

        # Classification layers
        self.lstm = nn.LSTM(embed_dim, 128, batch_first=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        embeds = self.embedding(x)  # Use pre-trained
        lstm_out, _ = self.lstm(embeds)
        pooled = lstm_out[:, -1, :]  # Last hidden state
        return self.fc(self.dropout(pooled))
\end{lstlisting}

\bottomnote{Pre-trained embeddings provide excellent initialization for downstream tasks}
\end{frame}

% ==================== APPLICATIONS & IMPACT ====================
\begin{frame}[t]{Real-World Applications Gallery}
\centering
\includegraphics[width=0.85\textwidth]{../figures/applications_gallery.pdf}

\vspace{5mm}
\begin{columns}[T]
\column{0.24\textwidth}
\centering
\textbf{Search Engines}
\begin{itemize}
\item Query expansion
\item Synonym matching
\item Intent understanding
\end{itemize}

\column{0.24\textwidth}
\centering
\textbf{Recommendations}
\begin{itemize}
\item Content similarity
\item User modeling
\item Cold start problem
\end{itemize}

\column{0.24\textwidth}
\centering
\textbf{Translation}
\begin{itemize}
\item Word alignment
\item Zero-shot transfer
\item Quality estimation
\end{itemize}

\column{0.24\textwidth}
\centering
\textbf{Chatbots}
\begin{itemize}
\item Intent detection
\item Entity extraction
\item Response generation
\end{itemize}
\end{columns}

\bottomnote{Word embeddings power billions of queries and interactions daily}
\end{frame}

% Historical Impact
\begin{frame}[t]{Historical Impact: The Word2Vec Revolution}
\textbf{Timeline of Impact:}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{2013: Word2Vec Published}
\begin{itemize}
\item Mikolov et al. papers
\item Open-source release
\item Immediate adoption
\end{itemize}

\vspace{3mm}
\textbf{2014-2015: Rapid Adoption}
\begin{itemize}
\item GloVe competition (Stanford)
\item Industry integration
\item 10,000+ citations
\end{itemize}

\vspace{3mm}
\textbf{2016-2017: Refinements}
\begin{itemize}
\item FastText (Facebook)
\item Multilingual embeddings
\item Domain-specific models
\end{itemize}

\column{0.48\textwidth}
\textbf{2018: Contextual Era}
\begin{itemize}
\item ELMo introduces context
\item BERT revolution begins
\item GPT demonstrates scale
\end{itemize}

\vspace{3mm}
\textbf{2019-Present: Foundation}
\begin{itemize}
\item Still used in production
\item Initialization for transformers
\item 50,000+ citations
\item Taught in every NLP course
\end{itemize}
\end{columns}

\vspace{8mm}
\begin{center}
\colorbox{mllavender3}{
\parbox{0.8\textwidth}{
\centering
\Large
\textbf{Word2Vec: The spark that ignited the NLP revolution}
}
}
\end{center}

\bottomnote{No other NLP method has had such immediate and lasting impact}
\end{frame}

% ==================== FINAL QUIZ ====================
\begin{frame}[t]{Final Quiz: Mastery Check}
\textbf{Can you answer these advanced questions?}

\vspace{5mm}
\begin{enumerate}
\item Why does Word2Vec use two embedding matrices (input and output)?
\pause
\item[] \textcolor{mlgreen}{→ Asymmetric relationship: predicting context from center}

\vspace{3mm}
\item How does negative sampling approximate the softmax?
\pause
\item[] \textcolor{mlgreen}{→ Binary classification: real pairs vs random pairs}

\vspace{3mm}
\item Why does ``king - man + woman = queen'' work mathematically?
\pause
\item[] \textcolor{mlgreen}{→ Relationships encoded as consistent vector offsets}

\vspace{3mm}
\item What's the computational complexity: full softmax vs negative sampling?
\pause
\item[] \textcolor{mlgreen}{→ O(|V|) vs O(k), where k << |V|}

\vspace{3mm}
\item Why can't Word2Vec handle ``apple'' (fruit) vs ``Apple'' (company)?
\pause
\item[] \textcolor{mlgreen}{→ Single vector per word type, no context dependence}
\end{enumerate}

\bottomnote{If you can answer these, you truly understand Word2Vec's design and limitations}
\end{frame}

% ==================== SUMMARY ====================
\begin{frame}[t]{Summary: What We've Learned}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Core Concepts}
\begin{itemize}
\item One-hot → Dense embeddings
\item Distributional hypothesis
\item Skip-gram architecture
\item Negative sampling trick
\item Semantic arithmetic
\item Evaluation methods
\end{itemize}

\vspace{5mm}
\textbf{Technical Skills}
\begin{itemize}
\item Training Word2Vec
\item Hyperparameter tuning
\item Using pre-trained models
\item Integration with neural nets
\item Debugging embeddings
\end{itemize}

\column{0.48\textwidth}
\textbf{Key Insights}
\begin{itemize}
\item Context defines meaning
\item Geometry encodes semantics
\item Relationships are vectors
\item Efficiency enables scale
\item Limitations inspire progress
\end{itemize}

\vspace{5mm}
\textbf{Practical Impact}
\begin{itemize}
\item Powers search engines
\item Enables recommendations
\item Foundation for transformers
\item \$100B+ market impact
\item 50,000+ citations
\end{itemize}
\end{columns}

\vspace{8mm}
\begin{center}
\colorbox{mllavender2}{
\parbox{0.85\textwidth}{
\centering
\Large
\textbf{Word2Vec: Simple idea, profound impact, lasting legacy}
}
}
\end{center}

\bottomnote{Next week: How RNNs use embeddings to understand and generate sequences}
\end{frame}

% ==================== LAB PREVIEW ====================
\begin{frame}[t]{Lab Session Preview}
\textbf{Hands-On Exercises: week02\_word\_embeddings\_lab.ipynb}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Part 1: Training (45 min)}
\begin{itemize}
\item Load and preprocess corpus
\item Train Word2Vec from scratch
\item Experiment with hyperparameters
\item Compare CBOW vs Skip-gram
\item Visualize training progress
\end{itemize}

\vspace{5mm}
\textbf{Part 2: Exploration (30 min)}
\begin{itemize}
\item Find nearest neighbors
\item Test word analogies
\item Visualize with t-SNE
\item Explore semantic clusters
\item Identify interesting patterns
\end{itemize}

\column{0.48\textwidth}
\textbf{Part 3: Application (45 min)}
\begin{itemize}
\item Build similarity search engine
\item Create analogy solver
\item Simple sentiment classifier
\item Document similarity system
\item Performance evaluation
\end{itemize}

\vspace{5mm}
\textbf{Bonus Challenges:}
\begin{itemize}
\item Train on your own corpus
\item Implement negative sampling
\item Cross-lingual embeddings
\item Bias analysis and debiasing
\item Compare with GloVe/FastText
\end{itemize}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{mllavender4}{
\parbox{0.75\textwidth}{
\centering
\textbf{Goal: Hands-on mastery of word embeddings}
}
}
\end{center}

\bottomnote{Bring your laptop with Python environment ready - we'll build real systems!}
\end{frame}

% ==================== RESOURCES ====================
\begin{frame}[t]{Resources for Deep Dive}
\begin{columns}[T]
\column{0.31\textwidth}
\textbf{Essential Papers}
\footnotesize
\begin{itemize}
\item Mikolov et al. (2013a): Efficient Estimation
\item Mikolov et al. (2013b): Distributed Representations
\item Goldberg \& Levy (2014): word2vec Explained
\item Pennington et al. (2014): GloVe
\item Bojanowski et al. (2016): FastText
\end{itemize}

\column{0.31\textwidth}
\textbf{Implementations}
\footnotesize
\begin{itemize}
\item Gensim (Python)
\item TensorFlow/Keras
\item PyTorch
\item FastText library
\item spaCy integration
\end{itemize}

\normalsize
\vspace{3mm}
\textbf{Tutorials}
\footnotesize
\begin{itemize}
\item TensorFlow tutorials
\item PyTorch examples
\item Gensim documentation
\end{itemize}

\column{0.31\textwidth}
\textbf{Pre-trained Models}
\footnotesize
\begin{itemize}
\item Google News (3M words)
\item GloVe (6B tokens)
\item FastText (157 languages)
\item Domain-specific models
\end{itemize}

\normalsize
\vspace{3mm}
\textbf{Visualization}
\footnotesize
\begin{itemize}
\item TensorBoard Projector
\item Embedding Explorer
\item Custom t-SNE tools
\end{itemize}
\end{columns}

\vspace{8mm}
\begin{center}
\colorbox{mllavender3}{
\parbox{0.8\textwidth}{
\centering
\textbf{Continue Learning:} \url{https://github.com/your-course/week02-resources}
}
}
\end{center}

\bottomnote{These resources will deepen your understanding and practical skills}
\end{frame}

% ==================== CLOSING ====================
\begin{frame}[t]{Looking Ahead}
\textbf{What's Next in Our Journey:}

\vspace{5mm}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Next Week: RNNs \& LSTMs}
\begin{itemize}
\item Sequential processing
\item Hidden state dynamics
\item Vanishing gradient problem
\item LSTM architecture
\item Text generation
\end{itemize}

\vspace{5mm}
\textbf{Building On Word2Vec:}
\begin{itemize}
\item Embeddings as RNN input
\item Sequence modeling
\item Context evolution
\item Memory mechanisms
\end{itemize}

\column{0.48\textwidth}
\textbf{Future Topics:}
\begin{itemize}
\item Week 4: Seq2Seq \& Attention
\item Week 5: Transformers
\item Week 6: BERT \& GPT
\item Week 7: Advanced architectures
\item Week 8-12: Modern NLP
\end{itemize}

\vspace{5mm}
\textbf{The Big Picture:}
\begin{itemize}
\item Word2Vec → RNN → Transformer
\item Static → Contextual embeddings
\item Understanding → Generation
\end{itemize}
\end{columns}

\vspace{8mm}
\begin{center}
\colorbox{mllavender2}{
\parbox{0.8\textwidth}{
\centering
\Large
\textbf{You now understand the foundation of all modern NLP!}
}
}
\end{center}

\bottomnote{See you next week for the exciting world of recurrent neural networks!}
\end{frame}

\end{document}