\documentclass[8pt,aspectratio=169,8pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{subfigure}
\usepackage{hyperref}

% Theme settings
\usetheme{Frankfurt}
\usecolortheme{seahorse}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

% Custom colors
\definecolor{darkblue}{RGB}{0,51,102}
\definecolor{lightblue}{RGB}{173,216,230}
\definecolor{codegreen}{RGB}{0,128,0}
\definecolor{codegray}{RGB}{150,150,150}
\definecolor{codepurple}{RGB}{128,0,128}
\definecolor{backcolor}{RGB}{245,245,245}

% Code listing settings
\lstset{
    backgroundcolor=\color{backcolor},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    showstringspaces=false,
    frame=single,
    numbers=left,
    language=Python
}

% Include slide layouts
\input{../slide_layouts.tex}

\title[Week 7: Advanced]{Natural Language Processing Course}
\subtitle{Week 7: Advanced Transformers}
\author{Joerg R. Osterrieder \\ \url{www.joergosterrieder.com}}
\date{}

\begin{document}

% Title page
\begin{frame}
    \titlepage
\end{frame}

\section{Week 7: Advanced Transformers}

% Title slide
\begin{frame}
    \centering
    \vspace{2cm}
    {\Large \textbf{Week 7}}\\
    \vspace{0.5cm}
    {\huge \textbf{Advanced Transformers}}\\
    \vspace{1cm}
    {\large The Race to 1 Trillion Parameters}
\end{frame}

% Motivation: The scaling discovery
\begin{frame}[t]{The Accidental Discovery That Changed Everything}
    \textbf{OpenAI's experiment (2020):}\footnotemark
    
    \vspace{0.5em}
    "What happens if we just... make BERT 100x bigger?"
    
    \vspace{0.5em}
    \textbf{Expected:}
    \begin{itemize}
        \item Slightly better performance
        \item Diminishing returns
        \item Waste of compute
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{What actually happened:}
    \begin{itemize}
        \item Model started doing tasks it was NEVER trained for
        \item Could translate languages without translation training
        \item Solved math problems without math training
        \item Wrote code without code training!
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{green!20}{
        \parbox{0.8\textwidth}{
            \centering
            Emergent abilities: Skills that appear only at massive scale
        }
    }
    \end{center}
    
    \footnotetext{Brown et al. (2020). "Language Models are Few-Shot Learners" (GPT-3)}
\end{frame}

% The scale comparison
\begin{frame}[t]{The Mind-Boggling Scale of Modern LLMs}
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/model_scale_timeline.pdf}
    
    \vspace{0.5em}
    \textbf{To put this in perspective:}
    \begin{itemize}
        \item BERT (2018): 340M parameters = 1 book
        \item GPT-2 (2019): 1.5B parameters = Small library
        \item GPT-3 (2020): 175B parameters = Library of Congress
        \item GPT-4 (2023): 1.76T parameters = All books ever written\footnotemark
    \end{itemize}
    
    \footnotetext{Estimated based on parameter count and training data scale}
\end{frame}

% Real-world impact
\begin{frame}[t]{Advanced Transformers in Production (2024)}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Consumer Products:}
            \begin{itemize}
                \item ChatGPT: 180M+ users\footnotemark
                \item GitHub Copilot: 1.2M+ subscribers
                \item Claude: Advanced reasoning
                \item Bard/Gemini: Multimodal
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Enterprise Applications:}
            \begin{itemize}
                \item Code generation: 40\% productivity boost\footnotemark
                \item Document analysis
                \item Customer service automation
                \item Content creation at scale
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Key Capabilities:}
            \begin{itemize}
                \item Zero-shot task solving
                \item In-context learning
                \item Chain-of-thought reasoning
                \item Multimodal understanding
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Scale Advantages:}
            \begin{itemize}
                \item Better factual knowledge
                \item Stronger reasoning
                \item More creative outputs
                \item Fewer hallucinations\footnotemark
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            2024: Every major tech company has a 100B+ parameter model
        }
    }
    \end{center}
    
    \footnotetext[1]{OpenAI statistics, 2024}
    \footnotetext[2]{GitHub Copilot productivity study}
    \footnotetext[3]{Though hallucination remains a challenge}
\end{frame}

% Learning objectives
\begin{frame}[t]{Week 7: What You'll Master}
    \textbf{By the end of this week, you will:}
    \begin{itemize}
        \item \textbf{Understand} why scale unlocks new capabilities
        \item \textbf{Master} architectural improvements for scale
        \item \textbf{Implement} efficient attention mechanisms
        \item \textbf{Explore} few-shot and zero-shot learning
        \item \textbf{Apply} modern prompting techniques
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            \textbf{Core Insight:} Scale isn't just "bigger" - it's qualitatively different
        }
    }
    \end{center}
\end{frame}

% Emergent abilities
\begin{frame}[t]{Emergent Abilities: Magic or Science?}
    \textbf{Small models can't, large models can:}\footnotemark
    
    \vspace{0.5em}
    \textbf{1. Three-digit arithmetic}
    \begin{itemize}
        \item $<$10B parameters: 0\% accuracy
        \item 100B parameters: 90\% accuracy
        \item No arithmetic in training objective!
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{2. Chain-of-thought reasoning}
    \begin{itemize}
        \item "Let's think step by step..."
        \item Small models: Output nonsense
        \item Large models: Logical reasoning emerges
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{3. In-context learning}
    \begin{itemize}
        \item Show 3 examples, model learns pattern
        \item No gradient updates!
        \item Pattern recognition at massive scale
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            Phase transitions: Abilities appear suddenly at specific scales
        }
    }
    \end{center}
    
    \footnotetext{Wei et al. (2022). "Emergent Abilities of Large Language Models"}
\end{frame}

% Scaling laws
\begin{frame}[t]{The Scaling Laws: Predictable Progress}
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/scaling_laws.pdf}
    
    \vspace{0.5em}
    \textbf{Kaplan et al. (2020) discovered:}\footnotemark
    \begin{itemize}
        \item Loss = $C \cdot N^{-\alpha}$ (N = parameters)
        \item Performance improves predictably with scale
        \item Optimal model/data ratio exists
        \item Compute-optimal training recipes
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Chinchilla's revision (2022):} Need more data than thought!
    
    \footnotetext{Kaplan et al. (2020) "Scaling Laws for Neural Language Models"}
\end{frame}

% Architectural improvements
\begin{frame}[t]{Architectural Innovations for Scale}
    \textbf{Problem: Attention is $O(n^2)$ - breaks at long sequences}
    
    \vspace{0.5em}
    \textbf{Solutions developed:}
    
    \begin{enumerate}
        \item \textbf{Sparse Attention (GPT-3):}
        \begin{itemize}
            \item Only attend to local + global tokens
            \item Reduces to $O(n\sqrt{n})$
            \item Maintains performance
        \end{itemize}
        
        \vspace{0.3em}
        \item \textbf{Flash Attention (2022):}\footnotemark
        \begin{itemize}
            \item IO-aware algorithm
            \item 2-4x faster, less memory
            \item Enables 100k+ context
        \end{itemize}
        
        \vspace{0.3em}
        \item \textbf{Rotary Position Embeddings (RoPE):}
        \begin{itemize}
            \item Better length extrapolation
            \item Used in LLaMA, GPT-Neo
            \item Relative positions naturally
        \end{itemize}
    \end{enumerate}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            Innovation focus: Make attention practical at massive scale
        }
    }
    \end{center}
    
    \footnotetext{Dao et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention"}
\end{frame}

% Sparse attention implementation
\begin{frame}[fragile]{Implementing Sparse Attention}
    \begin{columns}[T]
\column{0.55\textwidth}
\begin{lstlisting}[language=Python,basicstyle=\ttfamily\tiny]
import torch
import torch.nn as nn
import torch.nn.functional as F

class SparseAttention(nn.Module):
    def __init__(self, n_heads, seq_len, sparsity_factor=8):
        # Sparse attention for long sequences
        super().__init__()
        self.n_heads = n_heads
        self.seq_len = seq_len
        self.sparsity = sparsity_factor
        
        # Define sparse pattern
        self.register_buffer('mask', self.create_sparse_mask(seq_len))
        
    def create_sparse_mask(self, seq_len):
        # Create sparse attention pattern
        mask = torch.zeros(seq_len, seq_len)
        
        for i in range(seq_len):
            # Attend to self
            mask[i, i] = 1
            
            # Local attention (previous k tokens)
            for j in range(max(0, i - self.sparsity), i):
                mask[i, j] = 1
                
            # Strided attention (every k-th token)
            for j in range(0, i, self.sparsity):
                mask[i, j] = 1
                
            # Global attention (first few tokens)
            for j in range(min(self.sparsity, seq_len)):
                mask[i, j] = 1
                
        return mask.bool()
    
    def forward(self, query, key, value):
        # Compute sparse attention
        B, H, L, D = query.shape
        
        # Compute attention scores
        scores = torch.matmul(query, key.transpose(-2, -1)) / (D ** 0.5)
        
        # Apply sparse mask
        scores = scores.masked_fill(~self.mask, -1e9)
        
        # Softmax and apply to values
        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, value)
        
        return output, attn_weights
\end{lstlisting}
        \column{0.43\textwidth}

        \codeexplanation{
            \textbf{Sparse Patterns:}
            \begin{itemize}
                \item Local: Nearby tokens (capture local context)
                \item Strided: Every k-th token (long-range)
                \item Global: First tokens (task instructions)
            \end{itemize}
            
            \vspace{0.3em}
            \textbf{Benefits:}
            \begin{itemize}
                \item Memory: $O(n)$ instead of $O(n^2)$
                \item Speed: 10x faster on long sequences
                \item Quality: 95\% of dense performance
            \end{itemize}
            
            \vspace{0.3em}
            \textbf{Used in:}
            \begin{itemize}
                \item GPT-3: Custom sparse patterns
                \item BigBird: Random + sliding window
                \item Longformer: Task-specific sparsity
            \end{itemize}
        }
    \end{columns}
\end{frame}

% Few-shot learning
\begin{frame}[t]{Few-Shot Learning: Teaching by Example}
    \textbf{The GPT-3 breakthrough: Learning from context alone}
    
    \vspace{0.5em}
    \textbf{Example - Sentiment Analysis:}
    \begin{small}
    \texttt{Review: "The movie was fantastic!" Sentiment: Positive}\\
    \texttt{Review: "Terrible waste of time." Sentiment: Negative}\\
    \texttt{Review: "Best film I've seen all year!" Sentiment: Positive}\\
    \texttt{Review: "Boring and predictable." Sentiment: [Model predicts: Negative]}
    \end{small}
    
    \vspace{0.5em}
    \textbf{No fine-tuning needed! The model:}
    \begin{itemize}
        \item Recognizes the pattern from examples
        \item Applies it to new inputs
        \item Works for ANY task with clear examples
    \end{itemize}
    
    \vspace{0.5em}
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/few_shot_performance.pdf}
\end{frame}

% Prompt engineering
\begin{frame}[t]{Prompt Engineering: The New Programming}
    \textbf{Discovery: HOW you ask matters as much as WHAT you ask}
    
    \vspace{0.5em}
    \textbf{Example - Math Problem:}
    
    \colorbox{red!20}{Bad prompt:} "What is 37 * 48?"
    
    Result: Often wrong
    
    \vspace{0.5em}
    \colorbox{green!20}{Good prompt:} "Let's solve 37 * 48 step by step:
    First, break it down..."
    
    Result: Much more accurate!
    
    \vspace{0.5em}
    \textbf{Effective techniques:}
    \begin{enumerate}
        \item \textbf{Chain-of-thought:} "Let's think step by step"
        \item \textbf{Role prompting:} "You are an expert mathematician"
        \item \textbf{Format specification:} "Answer in JSON format"
        \item \textbf{Self-consistency:} Generate multiple answers, vote
    \end{enumerate}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            Prompt engineering: 10x performance difference on same model!
        }
    }
    \end{center}
\end{frame}

% Training challenges
\begin{frame}[t]{Training at Scale: The Engineering Challenge}
    \textbf{Training GPT-3 scale models:}
    
    \vspace{0.5em}
    \textbf{The numbers:}
    \begin{itemize}
        \item Parameters: 175 billion
        \item Training data: 45TB of text
        \item Compute: 3.14 × $10^{23}$ FLOPs
        \item Cost: \$4.6 million\footnotemark
        \item Time: 34 days on 10,000 GPUs
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Technical challenges:}
    \begin{enumerate}
        \item \textbf{Model parallelism:} Split layers across GPUs
        \item \textbf{Pipeline parallelism:} Micro-batches through layers
        \item \textbf{Data parallelism:} Different batches per GPU
        \item \textbf{Mixed precision:} FP16 with FP32 master weights
        \item \textbf{Gradient checkpointing:} Trade compute for memory
    \end{enumerate}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{red!20}{
        \parbox{0.8\textwidth}{
            \centering
            One bug = \$100,000 wasted. Debugging at scale is critical!
        }
    }
    \end{center}
    
    \footnotetext{Lambda Labs estimate based on cloud GPU pricing}
\end{frame}

% Model parallelism visualization
\begin{frame}[t]{Model Parallelism: Splitting Across GPUs}
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/model_parallelism.pdf}
    
    \vspace{0.5em}
    \textbf{Key strategies:}
    \begin{itemize}
        \item Tensor parallelism: Split matrices across GPUs
        \item Pipeline parallelism: Assign layers to GPUs
        \item Sequence parallelism: Split sequence dimension
        \item Optimizer sharding: Distribute Adam states
    \end{itemize}
\end{frame}

% Results and capabilities
\resultslide{What Scale Enables: GPT-3 Capabilities}{
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/gpt3_capabilities.pdf}
}{
    \begin{itemize}
        \item Translation without translation training
        \item Code generation from natural language
        \item Complex reasoning tasks
        \item Creative writing with style transfer
        \item Question answering without fine-tuning
        \item All from the SAME model!
    \end{itemize}
}

% Modern landscape
\begin{frame}[t]{The Large Model Landscape (2024)}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Closed Models:}
            \begin{itemize}
                \item GPT-4: Multimodal, 1.76T\footnotemark
                \item Claude 3: Constitutional AI
                \item Gemini Ultra: 1.75T parameters
                \item ChatGPT: Continually updated
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Open Models:}
            \begin{itemize}
                \item LLaMA 2: 7B-70B, efficient
                \item Mistral: 7B beats 30B models
                \item Falcon: 180B, permissive license
                \item BLOOM: 176B, multilingual
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Specialized Giants:}
            \begin{itemize}
                \item Codex: Programming focus
                \item Med-PaLM: Medical expertise
                \item Galactica: Scientific knowledge
                \item Bloomberg GPT: Financial
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Efficiency Focus:}
            \begin{itemize}
                \item Chinchilla: Better data scaling
                \item LLaMA: Quality at smaller size
                \item Alpaca: Efficient fine-tuning
                \item QLoRA: 4-bit quantization
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            2024 trend: "Small" 7B models matching 2020's 175B performance
        }
    }
    \end{center}
    
    \footnotetext{Parameter counts estimated from capabilities}
\end{frame}

% Future directions
\begin{frame}[t]{Pushing the Boundaries: What's Next?}
    \textbf{Current frontiers:}
    
    \vspace{0.5em}
    \textbf{1. Multimodal models:}
    \begin{itemize}
        \item GPT-4V: Understands images
        \item Gemini: Audio, video, images, text
        \item DALL-E 3: Integrated generation
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{2. Longer context:}
    \begin{itemize}
        \item Claude: 100k tokens (75k words)
        \item GPT-4 Turbo: 128k tokens
        \item Research: 1M+ tokens (entire books)
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{3. Reasoning improvements:}
    \begin{itemize}
        \item Chain-of-thought built-in
        \item Tool use and function calling
        \item Self-correction mechanisms
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{4. Efficiency at scale:}
    \begin{itemize}
        \item Mixture of Experts (MoE)
        \item Sparse models: 1T params, 100B active
        \item Better architectures than transformers?
    \end{itemize}
\end{frame}

% Exercise
\begin{frame}[t]{Week 7 Exercise: Explore Emergent Abilities}
    \textbf{Your Mission:} Discover what large models can do that small ones can't
    
    \vspace{0.5em}
    \textbf{Part 1: Scale Comparison}
    \begin{itemize}
        \item Use models of different sizes (GPT-2 vs GPT-3.5)
        \item Test arithmetic, reasoning, translation
        \item Document where abilities emerge
        \item Plot performance vs model size
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Part 2: Few-Shot Learning}
    \begin{itemize}
        \item Create custom tasks with 0, 1, 3, 5 examples
        \item Test on: classification, generation, reasoning
        \item Measure how examples improve performance
        \item Find optimal number of shots
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Part 3: Prompt Engineering}
    \begin{itemize}
        \item Compare basic vs chain-of-thought prompts
        \item Test role prompting effectiveness
        \item Try self-consistency voting
        \item Quantify prompt impact
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{You'll discover:} Why scale isn't just "bigger" - it's different!
\end{frame}

% Summary
\begin{frame}[t]{Key Takeaways: The Scale Revolution}
    \textbf{What we learned:}
    \begin{itemize}
        \item Scale enables emergent abilities
        \item Few-shot learning works at large scale
        \item Architectural innovations enable efficiency
        \item Prompt engineering is crucial
        \item Engineering challenges are immense
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{The paradigm shifts:}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            Fine-tuning everything → Few-shot learning\\
            Model architecture → Scale + data\\
            Programming → Prompt engineering
        }
    }
    \end{center}
    
    \vspace{0.5em}
    \textbf{Why it matters:}
    \begin{itemize}
        \item Democratizes AI capabilities
        \item Enables new applications
        \item Changes how we interact with computers
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Next week: Tokenization and Subword Models}
    
    How do these models handle any text in any language?
\end{frame}

% References
\begin{frame}[t]{References and Further Reading}
    \textbf{Foundational Papers:}
    \begin{itemize}
        \item Brown et al. (2020). "Language Models are Few-Shot Learners" (GPT-3)
        \item Kaplan et al. (2020). "Scaling Laws for Neural Language Models"
        \item Wei et al. (2022). "Emergent Abilities of Large Language Models"
    \end{itemize}
    
    \textbf{Technical Advances:}
    \begin{itemize}
        \item Dao et al. (2022). "FlashAttention"
        \item Hoffmann et al. (2022). "Training Compute-Optimal LLMs" (Chinchilla)
        \item Touvron et al. (2023). "LLaMA: Open and Efficient Foundation Models"
    \end{itemize}
    
    \textbf{Practical Resources:}
    \begin{itemize}
        \item "GPT-3 Powers and Limits" - OpenAI blog
        \item "The Illustrated GPT-3" - Jay Alammar
        \item Anthropic's research on scaling
    \end{itemize}
\end{frame}
\end{document}
