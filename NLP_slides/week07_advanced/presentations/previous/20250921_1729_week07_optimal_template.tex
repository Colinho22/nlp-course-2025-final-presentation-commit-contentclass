% Week 7: Advanced Transformers
% Using the Master Optimal Readability Template

\input{../../common/master_template.tex}

\title{Advanced Transformers}
\subtitle{\secondary{Week 7 - T5, GPT-3, and the Era of Scale}}
\author{NLP Course 2025}
\date{\today}

\begin{document}

% Title slide
\begin{frame}
\titlepage
\vfill
\begin{center}
\secondary{\footnotesize From BERT to GPT-3: The Scaling Revolution}
\end{center}
\end{frame}

% Overview
\begin{frame}{Week 7: The Era of Giant Models}
\begin{center}
{\Large \textbf{When Size Started to Matter}}
\end{center}

\vspace{10mm}

\begin{columns}[T]
\column{0.32\textwidth}
\textbf{The Discovery}
\begin{itemize}
\item Scaling \warning{changes everything}
\item Emergent abilities appear
\item \highlight{Quality} from quantity
\item Power laws rule
\end{itemize}

\column{0.32\textwidth}
\textbf{The Models}
\begin{itemize}
\item T5: \data{11B parameters}
\item GPT-3: \dataalt{175B parameters}
\item Switch: \success{1.6T parameters}
\item Compute as currency
\end{itemize}

\column{0.32\textwidth}
\textbf{The Impact}
\begin{itemize}
\item Few-shot learning works
\item In-context learning emerges
\item Task-agnostic models
\item AI becomes mainstream
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize The moment language models became foundation models}
\end{frame}

% Part 1: The Scaling Hypothesis
\begin{frame}{\Large Part 1: The Scaling Hypothesis}
\begin{center}
{\huge \textbf{Bigger is Different}}
\end{center}

\vfill

\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Kaplan Scaling Laws (2020)}

\formula{L = aN^{-\alpha} + bD^{-\beta} + L_{\infty}}

Where:
\begin{itemize}
\item $N$ = number of parameters
\item $D$ = dataset size (tokens)
\item $\alpha \approx 0.076$, $\beta \approx 0.095$
\item Loss decreases predictably with scale
\end{itemize}

\column{0.48\textwidth}
\textbf{The Chinchilla Laws (2022)}

\keypoint{Compute-optimal training: $N \propto D^{0.5}$}

Key insight:
\begin{itemize}
\item Most models are \warning{undertrained}
\item Need 20 tokens per parameter
\item Smaller models + more data = better
\item Changes entire industry approach
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize From ``make it bigger'' to ``train it longer''}
\end{frame}

% Emergent Abilities
\begin{frame}{Emergent Abilities: The Phase Transition}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What Are Emergent Abilities?}

Capabilities that:
\begin{itemize}
\item Appear \highlight{suddenly} at scale
\item Were \warning{not} explicitly trained
\item Show sharp phase transitions
\item Cannot be predicted from smaller models
\end{itemize}

\vspace{5mm}

\textbf{Examples at Different Scales}

\begin{tabular}{ll}
\toprule
\textbf{Parameters} & \textbf{Emergent Ability} \\
\midrule
1B & Basic syntax \\
10B & Multi-step reasoning \\
50B & Chain-of-thought \\
100B+ & In-context learning \\
\bottomrule
\end{tabular}

\column{0.48\textwidth}
\includegraphics[width=\textwidth]{../figures/emergent_abilities_chart.pdf}

\vspace{5mm}

\textbf{The Mystery}

Nobody knows why:
\begin{itemize}
\item Sharp transitions occur
\item Specific scales matter
\item Some tasks need 100B+
\item Others emerge at 1B
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize The most surprising discovery in modern AI}
\end{frame}

% T5: Text-to-Text Transfer Transformer
\begin{frame}{T5: Everything is Text Generation}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Unified Framework}

Every task as text-to-text:
\begin{itemize}
\item Translation: \data{``translate English to French: hello''}
\item Summarization: \data{``summarize: [article]''}
\item Question: \data{``question: what is NLP?''}
\item Classification: \data{``sentiment: great movie''}
\end{itemize}

\vspace{5mm}

\textbf{Architecture Choices}

\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Decision} \\
\midrule
Model & Encoder-decoder \\
Size & 60M to 11B \\
Objective & Span corruption \\
Dataset & C4 (750GB text) \\
\bottomrule
\end{tabular}

\column{0.48\textwidth}
\textbf{Key Innovation: Span Corruption}

\includegraphics[width=\textwidth]{../figures/t5_span_corruption.pdf}

\vspace{5mm}

\textbf{Performance Impact}

\begin{itemize}
\item SOTA on 20+ benchmarks
\item Single model, many tasks
\item Better than task-specific models
\item Scales predictably
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize Google's answer to GPT: unify everything}
\end{frame}

% GPT-3: The 175B Monster
\begin{frame}[fragile]{GPT-3: The Model That Changed Everything}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Scale}

\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Parameters & 175 billion \\
Layers & 96 \\
Hidden size & 12,288 \\
Attention heads & 96 \\
Training tokens & 300 billion \\
Training cost & \$4.6 million \\
\bottomrule
\end{tabular}

\vspace{5mm}

\textbf{Few-Shot Learning}

No gradient updates needed:
\begin{itemize}
\item 0-shot: Just describe task
\item 1-shot: One example
\item Few-shot: 2-10 examples
\item \success{Works surprisingly well!}
\end{itemize}

\column{0.48\textwidth}
\textbf{In-Context Learning Example}

\begin{lstlisting}[language={}]
Translate to French:
sea otter -> loutre de mer
cheese -> fromage
peppermint ->
(*@\textcolor{DarkGreen}{menthe poivr\'ee}@*)
\end{lstlisting}

\vspace{5mm}

\includegraphics[width=\textwidth]{../figures/gpt3_performance.pdf}

\vspace{5mm}

\keypoint{First model to show general intelligence}
\end{columns}

\vfill
\secondary{\footnotesize The moment AI became a mainstream conversation}
\end{frame}

% Architecture Deep Dive
\begin{frame}{Architecture Evolution: From BERT to GPT-3}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Three Paradigms}

\includegraphics[width=\textwidth]{../figures/architecture_comparison_advanced.pdf}

\begin{enumerate}
\item \textbf{Encoder-only} (BERT)
   \begin{itemize}
   \item Bidirectional context
   \item Best for understanding
   \item Classification tasks
   \end{itemize}

\item \textbf{Decoder-only} (GPT)
   \begin{itemize}
   \item Autoregressive
   \item Best for generation
   \item Most scalable
   \end{itemize}

\item \textbf{Encoder-Decoder} (T5)
   \begin{itemize}
   \item Flexible input/output
   \item Best for seq2seq
   \item More parameters needed
   \end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{Why Decoder-Only Won}

\begin{tabular}{ll}
\toprule
\textbf{Advantage} & \textbf{Reason} \\
\midrule
Simplicity & One stack vs two \\
Efficiency & Better GPU utilization \\
Scaling & More predictable \\
Generation & Natural fit \\
Training & Simpler objective \\
\bottomrule
\end{tabular}

\vspace{5mm}

\textbf{The Convergence}

All roads lead to autoregressive:
\begin{itemize}
\item BERT team moves to decoder (PaLM)
\item T5 team adopts decoder (Flan)
\item Industry standardizes on GPT-style
\item Even vision models follow (ViT-GPT)
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize The architecture debate is over: decoder-only won}
\end{frame}

% Sparse Models and MoE
\begin{frame}{Mixture of Experts: Scaling Without Cost}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Problem with Dense Models}

Every token uses ALL parameters:
\begin{itemize}
\item 175B params = 175B operations
\item Linear scaling of compute
\item Hit hardware limits quickly
\item \warning{Unsustainable growth}
\end{itemize}

\vspace{5mm}

\textbf{The MoE Solution}

\includegraphics[width=\textwidth]{../figures/moe_architecture.pdf}

Only activate what you need!

\column{0.48\textwidth}
\textbf{Switch Transformer (2021)}

\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total params & 1.6 trillion \\
Active params & 100B per token \\
Experts & 2048 \\
Speedup & 7x \\
\bottomrule
\end{tabular}

\vspace{5mm}

\textbf{How It Works}

\begin{enumerate}
\item Router selects experts
\item Each token â†’ 1-2 experts
\item Experts specialize automatically
\item Load balancing critical
\end{enumerate}

\vspace{5mm}

\keypoint{1.6T params, 100B compute cost!}
\end{columns}

\vfill
\secondary{\footnotesize The clever way to scale: use sparsity}
\end{frame}

% Training at Scale
\begin{frame}{Training Infrastructure: The Hidden Challenge}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Model Parallelism Types}

\begin{enumerate}
\item \textbf{Data Parallel}
   \begin{itemize}
   \item Split batch across GPUs
   \item Replicate model
   \item Synchronize gradients
   \end{itemize}

\item \textbf{Pipeline Parallel}
   \begin{itemize}
   \item Split layers across GPUs
   \item Micro-batching
   \item Bubble overhead
   \end{itemize}

\item \textbf{Tensor Parallel}
   \begin{itemize}
   \item Split matrices across GPUs
   \item High communication
   \item Best for large layers
   \end{itemize}
\end{enumerate}

\vspace{5mm}

\textbf{3D Parallelism}: Combine all three!

\column{0.48\textwidth}
\includegraphics[width=\textwidth]{../figures/3d_parallelism.pdf}

\vspace{5mm}

\textbf{GPT-3 Training Stats}

\begin{tabular}{ll}
\toprule
\textbf{Resource} & \textbf{Amount} \\
\midrule
GPUs & 10,000 V100s \\
Training time & 34 days \\
FLOPs & $3.14 \times 10^{23}$ \\
Power usage & 1,287 MWh \\
CO2 emissions & 552 tons \\
\bottomrule
\end{tabular}
\end{columns}

\vfill
\secondary{\footnotesize Training large models is an engineering feat}
\end{frame}

% Practical Applications
\begin{frame}{From Research to Production}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The API Revolution}

No more training needed:
\begin{itemize}
\item OpenAI API (GPT-3)
\item Google Cloud (PaLM)
\item Anthropic (Claude)
\item Cohere, AI21, etc.
\end{itemize}

\vspace{5mm}

\textbf{Prompt Engineering}

The new programming:
\begin{itemize}
\item Zero-shot prompts
\item Few-shot examples
\item Chain-of-thought
\item Instruction following
\end{itemize}

\vspace{5mm}

\textbf{Cost Per 1M Tokens}

\begin{tabular}{ll}
\toprule
\textbf{Model} & \textbf{Price} \\
\midrule
GPT-3 Ada & \$0.40 \\
GPT-3 Curie & \$2.00 \\
GPT-3 Davinci & \$20.00 \\
\bottomrule
\end{tabular}

\column{0.48\textwidth}
\textbf{Real Applications (2021-2023)}

\includegraphics[width=\textwidth]{../figures/gpt3_applications.pdf}

\vspace{5mm}

\textbf{Success Stories}

\begin{itemize}
\item GitHub Copilot: 40\% of code
\item Jasper.ai: \$125M revenue
\item Copy.ai: 10M users
\item ChatGPT: 100M in 2 months
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize From lab to product in record time}
\end{frame}

% The Compute Race
\begin{frame}{The Compute Race: Power Laws and Politics}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Compute Requirements Over Time}

\includegraphics[width=\textwidth]{../figures/compute_scaling.pdf}

\vspace{5mm}

Doubling every 3.4 months!

\vspace{5mm}

\textbf{The Players (2023)}

\begin{tabular}{ll}
\toprule
\textbf{Company} & \textbf{Largest Model} \\
\midrule
OpenAI & GPT-4 (1T?) \\
Google & PaLM 2 (340B) \\
Meta & Llama 2 (70B) \\
Anthropic & Claude (52B) \\
\bottomrule
\end{tabular}

\column{0.48\textwidth}
\textbf{The Hardware Arms Race}

\begin{itemize}
\item NVIDIA A100: \$10,000
\item NVIDIA H100: \$30,000
\item TPU v4: Not for sale
\item Custom chips emerging
\end{itemize}

\vspace{5mm}

\textbf{National AI Strategies}

\begin{itemize}
\item US: Export controls on chips
\item China: \$150B investment
\item EU: Sovereign cloud initiative
\item UK: Safety focus
\end{itemize}

\vspace{5mm}

\keypoint{Compute is the new oil}
\end{columns}

\vfill
\secondary{\footnotesize The geopolitics of artificial intelligence}
\end{frame}

% Limitations and Challenges
\begin{frame}{The Dark Side of Scale}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Known Limitations}

\begin{enumerate}
\item \textbf{Hallucinations}
   \begin{itemize}
   \item Confident wrong answers
   \item Made-up citations
   \item No uncertainty estimates
   \end{itemize}

\item \textbf{Reasoning Failures}
   \begin{itemize}
   \item Simple math errors
   \item Logic puzzles fail
   \item Common sense gaps
   \end{itemize}

\item \textbf{Control Problems}
   \begin{itemize}
   \item Can't guarantee safety
   \item Prompt injection attacks
   \item Jailbreaking possible
   \end{itemize}
\end{enumerate}

\vspace{5mm}

\warning{Bigger models = bigger problems}

\column{0.48\textwidth}
\textbf{The Cost Crisis}

\begin{itemize}
\item Training GPT-4: \$100M+
\item Running costs: \$700K/day
\item Environmental impact huge
\item Excludes most researchers
\end{itemize}

\vspace{5mm}

\textbf{The Alignment Problem}

\includegraphics[width=\textwidth]{../figures/alignment_challenge.pdf}

\vspace{5mm}

How do we ensure:
\begin{itemize}
\item Models do what we want?
\item They're honest and helpful?
\item They refuse harmful requests?
\item They remain controllable?
\end{itemize}
\end{columns}

\vfill
\secondary{\footnotesize With great scale comes great responsibility}
\end{frame}

% Future Directions
\begin{frame}{The Road Ahead: What's Next?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Scaling Continues}

GPT-5 and beyond:
\begin{itemize}
\item 10T parameters coming
\item Multimodal by default
\item Video understanding
\item Reasoning breakthroughs?
\end{itemize}

\vspace{5mm}

\textbf{Efficiency Revolution}

Making models smaller:
\begin{itemize}
\item Quantization (1-bit models!)
\item Knowledge distillation
\item Efficient architectures
\item On-device inference
\end{itemize}

\vspace{5mm}

\textbf{New Paradigms}

\begin{itemize}
\item Retrieval-augmented generation
\item Tool use and plugins
\item Constitutional AI
\item Mechanistic interpretability
\end{itemize}

\column{0.48\textwidth}
\textbf{The Cambrian Explosion}

\includegraphics[width=\textwidth]{../figures/model_explosion.pdf}

\vspace{5mm}

\textbf{Open Questions}

\begin{enumerate}
\item Will scaling laws hold forever?
\item Can we solve hallucinations?
\item Is AGI possible this way?
\item Who controls the models?
\item How do we ensure safety?
\end{enumerate}
\end{columns}

\vfill
\secondary{\footnotesize The next chapter is being written now}
\end{frame}

% Key Takeaways
\begin{frame}{Key Takeaways}
\begin{center}
{\Large \textbf{What We Learned About Scale}}
\end{center}

\vspace{10mm}

\begin{columns}[T]
\column{0.32\textwidth}
\textbf{Technical Insights}
\begin{itemize}
\item \highlight{Scale} brings emergence
\item Decoder-only won
\item Sparsity enables scale
\item In-context learning works
\item Compute is everything
\end{itemize}

\column{0.32\textwidth}
\textbf{Practical Lessons}
\begin{itemize}
\item APIs democratize AI
\item Prompting is programming
\item Few-shot often enough
\item Fine-tuning less needed
\item Costs dropping fast
\end{itemize}

\column{0.32\textwidth}
\textbf{Future Challenges}
\begin{itemize}
\item Hallucination problem
\item Alignment crucial
\item Efficiency needed
\item Access inequality
\item Safety concerns real
\end{itemize}
\end{columns}

\vspace{10mm}

\keypoint{The scaling revolution changed everything. We're still figuring out what that means.}

\vfill
\secondary{\footnotesize Next week: How these models actually read text (Tokenization)}
\end{frame}

% References
\begin{frame}{References}
\footnotesize
\begin{itemize}
\item Kaplan et al. (2020). ``Scaling Laws for Neural Language Models''
\item Brown et al. (2020). ``Language Models are Few-Shot Learners'' (GPT-3)
\item Raffel et al. (2020). ``Exploring the Limits of Transfer Learning with T5''
\item Fedus et al. (2021). ``Switch Transformers: Scaling to Trillion Parameter Models''
\item Hoffmann et al. (2022). ``Training Compute-Optimal Large Language Models'' (Chinchilla)
\item Wei et al. (2022). ``Emergent Abilities of Large Language Models''
\item Chowdhery et al. (2022). ``PaLM: Scaling Language Modeling with Pathways''
\item Anil et al. (2023). ``PaLM 2 Technical Report''
\item OpenAI (2023). ``GPT-4 Technical Report''
\item Anthropic (2023). ``Constitutional AI: Harmlessness from AI Feedback''
\end{itemize}
\end{frame}

\end{document}