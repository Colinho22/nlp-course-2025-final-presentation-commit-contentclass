{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Lab: Advanced Transformers - T5 and GPT-3\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand and implement T5's text-to-text framework\n",
    "- Explore GPT-3-style few-shot learning\n",
    "- Analyze emergent abilities at different scales\n",
    "- Compare different transformer architectures\n",
    "- Implement efficient inference techniques\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install transformers torch datasets evaluate accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer,\n",
    "    GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    AutoModelForCausalLM, AutoTokenizer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: T5 - Text-to-Text Framework\n",
    "\n",
    "T5 treats every NLP task as a text generation problem. Let's explore this unified approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 model and tokenizer (using small version for demo)\n",
    "print(\"Loading T5-small model...\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in t5_model.parameters())\n",
    "print(f\"T5-small parameters: {total_params/1e6:.1f}M\")\n",
    "print(f\"Vocabulary size: {t5_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 T5 for Multiple Tasks\n",
    "\n",
    "Demonstrate T5's versatility across different NLP tasks using task prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_generate(model, tokenizer, prompt, max_length=128):\n",
    "    \"\"\"Generate text using T5 model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test different tasks\n",
    "tasks = [\n",
    "    (\"translate English to French: The weather is beautiful today.\",\n",
    "     \"Translation\"),\n",
    "    (\"summarize: The transformer architecture has revolutionized natural language processing. \"\n",
    "     \"It uses self-attention mechanisms to process sequences in parallel, achieving state-of-the-art \"\n",
    "     \"results on many NLP tasks. Unlike RNNs, transformers can capture long-range dependencies efficiently.\",\n",
    "     \"Summarization\"),\n",
    "    (\"question: What is the capital of France? context: Paris is the capital and largest city of France.\",\n",
    "     \"Question Answering\"),\n",
    "    (\"sentiment: This movie was absolutely fantastic! Great acting and storyline.\",\n",
    "     \"Sentiment Analysis\")\n",
    "]\n",
    "\n",
    "print(\"T5 Text-to-Text Examples:\\n\" + \"=\"*50)\n",
    "for prompt, task_name in tasks:\n",
    "    result = t5_generate(t5_model, t5_tokenizer, prompt)\n",
    "    print(f\"\\n{task_name}:\")\n",
    "    print(f\"Input: {prompt[:60]}...\")\n",
    "    print(f\"Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 T5 Span Corruption Training\n",
    "\n",
    "Implement T5's unique pre-training objective: span corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_span_corruption_example(text, mask_ratio=0.15, mean_span_length=3):\n",
    "    \"\"\"Create T5-style span corruption example.\"\"\"\n",
    "    tokens = text.split()\n",
    "    n_tokens = len(tokens)\n",
    "    n_masks = int(n_tokens * mask_ratio / mean_span_length)\n",
    "    \n",
    "    # Generate random spans to mask\n",
    "    masked_tokens = tokens.copy()\n",
    "    masked_spans = []\n",
    "    mask_id = 0\n",
    "    \n",
    "    for _ in range(n_masks):\n",
    "        if len(masked_tokens) < mean_span_length:\n",
    "            break\n",
    "            \n",
    "        start = np.random.randint(0, len(masked_tokens) - mean_span_length + 1)\n",
    "        span_length = np.random.poisson(mean_span_length - 1) + 1\n",
    "        span_length = min(span_length, len(masked_tokens) - start)\n",
    "        \n",
    "        # Extract span\n",
    "        span = masked_tokens[start:start + span_length]\n",
    "        masked_spans.append((f\"<extra_id_{mask_id}>\", ' '.join(span)))\n",
    "        \n",
    "        # Replace with mask token\n",
    "        masked_tokens[start:start + span_length] = [f\"<extra_id_{mask_id}>\"]\n",
    "        mask_id += 1\n",
    "    \n",
    "    # Create input and target\n",
    "    input_text = ' '.join(masked_tokens)\n",
    "    target_text = ' '.join([f\"{mask} {span}\" for mask, span in masked_spans])\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "# Example\n",
    "text = \"The quick brown fox jumps over the lazy dog in the sunny afternoon\"\n",
    "corrupted, target = create_span_corruption_example(text)\n",
    "\n",
    "print(\"T5 Span Corruption Example:\")\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Corrupted Input: {corrupted}\")\n",
    "print(f\"Target Output: {target}\")\n",
    "\n",
    "# Visualize the corruption process\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "words = text.split()\n",
    "positions = np.arange(len(words))\n",
    "\n",
    "# Highlight masked spans\n",
    "colors = ['lightblue' if '<extra_id' not in w else 'salmon' \n",
    "          for w in corrupted.split()]\n",
    "\n",
    "ax.bar(positions, [1]*len(words), color=colors, edgecolor='black')\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "ax.set_ylim([0, 1.5])\n",
    "ax.set_ylabel('Token')\n",
    "ax.set_title('T5 Span Corruption Visualization')\n",
    "ax.axis('off')\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(i, 0.5, word, ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GPT-Style Few-Shot Learning\n",
    "\n",
    "Explore in-context learning with GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model (as GPT-3 proxy)\n",
    "print(\"Loading GPT-2 model...\")\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "total_params = sum(p.numel() for p in gpt_model.parameters())\n",
    "print(f\"GPT-2 Medium parameters: {total_params/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Zero-Shot, One-Shot, and Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_generate(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    \"\"\"Generate text using GPT model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Return only the new tokens\n",
    "    return generated[len(prompt):].strip()\n",
    "\n",
    "# Sentiment classification examples\n",
    "task = \"Classify the sentiment as positive or negative.\"\n",
    "\n",
    "# Zero-shot\n",
    "zero_shot_prompt = f\"\"\"{task}\n",
    "Text: This restaurant has amazing food and great service!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# One-shot\n",
    "one_shot_prompt = f\"\"\"{task}\n",
    "Text: The movie was boring and too long.\n",
    "Sentiment: negative\n",
    "\n",
    "Text: This restaurant has amazing food and great service!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Few-shot\n",
    "few_shot_prompt = f\"\"\"{task}\n",
    "Text: The movie was boring and too long.\n",
    "Sentiment: negative\n",
    "\n",
    "Text: I love this product! Works perfectly.\n",
    "Sentiment: positive\n",
    "\n",
    "Text: Terrible experience, would not recommend.\n",
    "Sentiment: negative\n",
    "\n",
    "Text: This restaurant has amazing food and great service!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(\"In-Context Learning Comparison:\\n\" + \"=\"*50)\n",
    "print(\"\\nZero-shot:\")\n",
    "print(f\"Result: {gpt_generate(gpt_model, gpt_tokenizer, zero_shot_prompt, 5)}\")\n",
    "\n",
    "print(\"\\nOne-shot:\")\n",
    "print(f\"Result: {gpt_generate(gpt_model, gpt_tokenizer, one_shot_prompt, 5)}\")\n",
    "\n",
    "print(\"\\nFew-shot:\")\n",
    "print(f\"Result: {gpt_generate(gpt_model, gpt_tokenizer, few_shot_prompt, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyzing Emergent Abilities\n",
    "\n",
    "Test various tasks that emerge at different model scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different emergent abilities\n",
    "emergent_tasks = [\n",
    "    (\"Basic arithmetic\", \"What is 13 + 27? Answer:\"),\n",
    "    (\"Logic puzzle\", \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Answer:\"),\n",
    "    (\"Code generation\", \"Write a Python function to calculate factorial:\\ndef factorial(n):\"),\n",
    "    (\"Chain of thought\", \"Question: A store has 23 apples. They sell 8 and buy 15 more. How many apples do they have?\\nLet's think step by step:\"),\n",
    "]\n",
    "\n",
    "print(\"Testing Emergent Abilities:\\n\" + \"=\"*50)\n",
    "for task_name, prompt in emergent_tasks:\n",
    "    print(f\"\\n{task_name}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    result = gpt_generate(gpt_model, gpt_tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"Response: {result}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Scaling Analysis\n",
    "\n",
    "Analyze how model performance changes with scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate scaling laws (using hypothetical data)\n",
    "# In practice, you would test with actual models of different sizes\n",
    "\n",
    "# Model sizes (parameters)\n",
    "model_sizes = np.logspace(7, 11, 20)  # 10M to 100B parameters\n",
    "\n",
    "# Simulated performance metrics following scaling laws\n",
    "# Loss = a * N^(-alpha) + L_inf\n",
    "alpha = 0.076  # From Kaplan et al.\n",
    "a = 10\n",
    "L_inf = 1.69  # Irreducible loss\n",
    "\n",
    "loss = a * model_sizes**(-alpha) + L_inf\n",
    "perplexity = np.exp(loss)\n",
    "\n",
    "# Emergent abilities (sigmoid emergence)\n",
    "def emergence_curve(size, threshold, steepness=4):\n",
    "    return 100 / (1 + np.exp(-steepness * (np.log10(size) - threshold)))\n",
    "\n",
    "arithmetic = emergence_curve(model_sizes, 9.5, 3)\n",
    "reasoning = emergence_curve(model_sizes, 10.5, 4)\n",
    "coding = emergence_curve(model_sizes, 11, 5)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Scaling law\n",
    "axes[0].loglog(model_sizes, loss, 'b-', linewidth=2, label='Loss')\n",
    "axes[0].axhline(y=L_inf, color='r', linestyle='--', label='Irreducible loss')\n",
    "axes[0].set_xlabel('Model Parameters')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Scaling Laws for Language Models')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Emergent abilities\n",
    "axes[1].semilogx(model_sizes, arithmetic, 'g-', linewidth=2, label='Arithmetic')\n",
    "axes[1].semilogx(model_sizes, reasoning, 'b-', linewidth=2, label='Reasoning')\n",
    "axes[1].semilogx(model_sizes, coding, 'r-', linewidth=2, label='Code Generation')\n",
    "axes[1].set_xlabel('Model Parameters')\n",
    "axes[1].set_ylabel('Task Performance (%)')\n",
    "axes[1].set_title('Emergent Abilities at Scale')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Compute requirements\n",
    "compute_flops = 6 * model_sizes * 1e12  # Approximate FLOPs for 1T tokens\n",
    "axes[2].loglog(model_sizes, compute_flops, 'purple', linewidth=2)\n",
    "axes[2].set_xlabel('Model Parameters')\n",
    "axes[2].set_ylabel('Training FLOPs')\n",
    "axes[2].set_title('Compute Requirements')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for notable models\n",
    "notable_models = [\n",
    "    (1.75e8, 'GPT-2'),\n",
    "    (1.5e9, 'GPT-2 XL'),\n",
    "    (1.75e11, 'GPT-3')\n",
    "]\n",
    "\n",
    "for size, name in notable_models:\n",
    "    if size <= model_sizes[-1]:\n",
    "        idx = np.argmin(np.abs(model_sizes - size))\n",
    "        axes[1].annotate(name, xy=(model_sizes[idx], emergence_curve(size, 10, 4)),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        fontsize=8, ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Efficient Inference Techniques\n",
    "\n",
    "Implement techniques for efficient inference with large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientTransformer(nn.Module):\n",
    "    \"\"\"Demonstration of efficiency techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, n_heads=8, use_flash_attention=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.use_flash = use_flash_attention\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads)\n",
    "        \n",
    "    def forward(self, x, use_kv_cache=False, past_kv=None):\n",
    "        \"\"\"Forward pass with optional KV caching.\"\"\"\n",
    "        if use_kv_cache and past_kv is not None:\n",
    "            # Simulate KV cache usage\n",
    "            # In practice, this would concatenate past keys/values\n",
    "            print(\"Using KV cache for faster inference\")\n",
    "            \n",
    "        if self.use_flash:\n",
    "            print(\"Using Flash Attention for memory efficiency\")\n",
    "            \n",
    "        # Standard attention (simplified)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "# Demonstrate quantization impact\n",
    "def simulate_quantization(model_size_gb, bit_width):\n",
    "    \"\"\"Calculate model size after quantization.\"\"\"\n",
    "    original_bits = 32  # FP32\n",
    "    compression_ratio = original_bits / bit_width\n",
    "    quantized_size = model_size_gb / compression_ratio\n",
    "    return quantized_size, compression_ratio\n",
    "\n",
    "# Model sizes in GB (FP32)\n",
    "models = {\n",
    "    'GPT-3 175B': 700,  # ~700GB in FP32\n",
    "    'T5-11B': 44,\n",
    "    'GPT-2 1.5B': 6\n",
    "}\n",
    "\n",
    "quantization_levels = [32, 16, 8, 4]  # Bits\n",
    "\n",
    "print(\"Quantization Impact on Model Size:\\n\" + \"=\"*50)\n",
    "results = []\n",
    "\n",
    "for model_name, size_gb in models.items():\n",
    "    print(f\"\\n{model_name} (Original: {size_gb} GB):\")\n",
    "    for bits in quantization_levels:\n",
    "        q_size, ratio = simulate_quantization(size_gb, bits)\n",
    "        print(f\"  {bits}-bit: {q_size:.1f} GB (compression: {ratio:.1f}x)\")\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Bits': bits,\n",
    "            'Size_GB': q_size,\n",
    "            'Compression': ratio\n",
    "        })\n",
    "\n",
    "# Visualize quantization impact\n",
    "df_quant = pd.DataFrame(results)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model in models.keys():\n",
    "    model_data = df_quant[df_quant['Model'] == model]\n",
    "    ax.plot(model_data['Bits'], model_data['Size_GB'], \n",
    "            marker='o', linewidth=2, label=model)\n",
    "\n",
    "ax.set_xlabel('Quantization Bits')\n",
    "ax.set_ylabel('Model Size (GB)')\n",
    "ax.set_title('Impact of Quantization on Model Size')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(quantization_levels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comparing Architectures\n",
    "\n",
    "Compare encoder-only, decoder-only, and encoder-decoder architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture comparison\n",
    "architectures = {\n",
    "    'Encoder-only (BERT)': {\n",
    "        'strengths': ['Bidirectional context', 'Great for classification', 'Understanding tasks'],\n",
    "        'weaknesses': ['Cannot generate text naturally', 'Requires task-specific heads'],\n",
    "        'use_cases': ['Sentiment analysis', 'NER', 'Question answering']\n",
    "    },\n",
    "    'Decoder-only (GPT)': {\n",
    "        'strengths': ['Natural text generation', 'Few-shot learning', 'Scales well'],\n",
    "        'weaknesses': ['Unidirectional context', 'Less efficient for classification'],\n",
    "        'use_cases': ['Text generation', 'Code completion', 'Creative writing']\n",
    "    },\n",
    "    'Encoder-Decoder (T5)': {\n",
    "        'strengths': ['Flexible input/output', 'Good for seq2seq', 'Unified framework'],\n",
    "        'weaknesses': ['More parameters needed', 'Complex architecture'],\n",
    "        'use_cases': ['Translation', 'Summarization', 'Question generation']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 8))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E77E']\n",
    "\n",
    "for idx, (arch_name, arch_info) in enumerate(architectures.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create simple architecture diagram\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(5, 9, arch_name, ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Strengths\n",
    "    ax.text(5, 7.5, 'Strengths:', ha='center', fontsize=10, fontweight='bold')\n",
    "    for i, strength in enumerate(arch_info['strengths']):\n",
    "        ax.text(5, 7 - i*0.5, f'• {strength}', ha='center', fontsize=9, color='green')\n",
    "    \n",
    "    # Weaknesses\n",
    "    ax.text(5, 5, 'Weaknesses:', ha='center', fontsize=10, fontweight='bold')\n",
    "    for i, weakness in enumerate(arch_info['weaknesses']):\n",
    "        ax.text(5, 4.5 - i*0.5, f'• {weakness}', ha='center', fontsize=9, color='red')\n",
    "    \n",
    "    # Use cases\n",
    "    ax.text(5, 2.5, 'Best for:', ha='center', fontsize=10, fontweight='bold')\n",
    "    for i, use_case in enumerate(arch_info['use_cases']):\n",
    "        ax.text(5, 2 - i*0.5, f'• {use_case}', ha='center', fontsize=9, color='blue')\n",
    "    \n",
    "    # Add colored border\n",
    "    rect = plt.Rectangle((0.5, 0.5), 9, 9, \n",
    "                         fill=False, edgecolor=colors[idx], linewidth=3)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.suptitle('Transformer Architecture Comparison', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Implementing a Simple MoE Layer\n",
    "\n",
    "Build a basic Mixture of Experts layer to understand sparse models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMoE(nn.Module):\n",
    "    \"\"\"Simple Mixture of Experts layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, n_experts=4, expert_capacity=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_experts = n_experts\n",
    "        self.expert_capacity = expert_capacity\n",
    "        \n",
    "        # Router network\n",
    "        self.router = nn.Linear(d_model, n_experts)\n",
    "        \n",
    "        # Expert networks (simple FFN)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_model * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model * 4, d_model)\n",
    "            ) for _ in range(n_experts)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Flatten for routing\n",
    "        x_flat = x.view(-1, d_model)\n",
    "        \n",
    "        # Compute router probabilities\n",
    "        router_logits = self.router(x_flat)\n",
    "        router_probs = torch.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts per token\n",
    "        _, selected_experts = torch.topk(router_probs, k=1, dim=-1)\n",
    "        \n",
    "        # Route tokens to experts\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        for expert_id in range(self.n_experts):\n",
    "            # Get tokens for this expert\n",
    "            expert_mask = (selected_experts.squeeze() == expert_id)\n",
    "            if expert_mask.any():\n",
    "                expert_input = x_flat[expert_mask]\n",
    "                expert_output = self.experts[expert_id](expert_input)\n",
    "                output[expert_mask] = expert_output\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Return output and routing info for visualization\n",
    "        return output, router_probs.view(batch_size, seq_len, -1)\n",
    "\n",
    "# Test MoE layer\n",
    "moe = SimpleMoE(d_model=256, n_experts=4)\n",
    "test_input = torch.randn(2, 10, 256)  # batch=2, seq=10, d_model=256\n",
    "\n",
    "output, routing = moe(test_input)\n",
    "\n",
    "print(f\"MoE Layer Test:\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Routing shape: {routing.shape}\")\n",
    "\n",
    "# Visualize routing decisions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Routing probabilities heatmap\n",
    "routing_viz = routing[0].detach().cpu().numpy()  # First batch\n",
    "im1 = axes[0].imshow(routing_viz.T, aspect='auto', cmap='YlOrRd')\n",
    "axes[0].set_xlabel('Token Position')\n",
    "axes[0].set_ylabel('Expert ID')\n",
    "axes[0].set_title('Expert Routing Probabilities')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Expert load distribution\n",
    "expert_loads = routing.mean(dim=(0, 1)).detach().cpu().numpy()\n",
    "axes[1].bar(range(len(expert_loads)), expert_loads, color=['#FF6B6B', '#4ECDC4', '#95E77E', '#FFE66D'])\n",
    "axes[1].set_xlabel('Expert ID')\n",
    "axes[1].set_ylabel('Average Load')\n",
    "axes[1].set_title('Expert Load Balancing')\n",
    "axes[1].set_xticks(range(len(expert_loads)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExpert utilization:\")\n",
    "for i, load in enumerate(expert_loads):\n",
    "    print(f\"  Expert {i}: {load:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercise - Building a Multi-Task Model\n",
    "\n",
    "Combine what we've learned to build a simple multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create a multi-task prompt handler\n",
    "class MultiTaskPromptHandler:\n",
    "    \"\"\"Handle multiple NLP tasks with appropriate prompting.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def process_task(self, task_type, input_text, few_shot_examples=None):\n",
    "        \"\"\"Process different task types with appropriate prompting.\"\"\"\n",
    "        \n",
    "        # Task-specific prompt templates\n",
    "        templates = {\n",
    "            'classification': 'Classify the following text:\\n{input}\\nCategory:',\n",
    "            'generation': 'Continue the following text:\\n{input}\\n',\n",
    "            'qa': 'Answer the question based on the context.\\nContext: {context}\\nQuestion: {question}\\nAnswer:',\n",
    "            'summarization': 'Summarize the following text:\\n{input}\\nSummary:',\n",
    "            'translation': 'Translate the following to {target_lang}:\\n{input}\\nTranslation:'\n",
    "        }\n",
    "        \n",
    "        # Build prompt\n",
    "        if task_type in templates:\n",
    "            if task_type == 'qa':\n",
    "                # Special handling for QA\n",
    "                context, question = input_text.split('\\n', 1)\n",
    "                prompt = templates[task_type].format(context=context, question=question)\n",
    "            else:\n",
    "                prompt = templates[task_type].format(input=input_text)\n",
    "                \n",
    "            # Add few-shot examples if provided\n",
    "            if few_shot_examples:\n",
    "                examples_str = '\\n\\n'.join(few_shot_examples)\n",
    "                prompt = f\"{examples_str}\\n\\n{prompt}\"\n",
    "                \n",
    "            return prompt\n",
    "        else:\n",
    "            return f\"Unknown task type: {task_type}\"\n",
    "\n",
    "# Test the multi-task handler\n",
    "handler = MultiTaskPromptHandler(gpt_model, gpt_tokenizer)\n",
    "\n",
    "# Test different tasks\n",
    "test_cases = [\n",
    "    ('classification', 'The new smartphone has an incredible camera and battery life.'),\n",
    "    ('summarization', 'Artificial intelligence has made remarkable progress in recent years. '\n",
    "                      'Deep learning models now excel at tasks like image recognition, '\n",
    "                      'natural language processing, and game playing. These advances '\n",
    "                      'have led to practical applications in healthcare, finance, and transportation.'),\n",
    "    ('generation', 'Once upon a time in a distant galaxy,')\n",
    "]\n",
    "\n",
    "print(\"Multi-Task Model Testing:\\n\" + \"=\"*50)\n",
    "for task_type, input_text in test_cases:\n",
    "    print(f\"\\nTask: {task_type}\")\n",
    "    prompt = handler.process_task(task_type, input_text)\n",
    "    print(f\"Prompt: {prompt[:100]}...\")\n",
    "    \n",
    "    # Generate response\n",
    "    if 't5' in task_type:\n",
    "        response = t5_generate(t5_model, t5_tokenizer, prompt, max_length=50)\n",
    "    else:\n",
    "        response = gpt_generate(gpt_model, gpt_tokenizer, prompt, max_new_tokens=30)\n",
    "    \n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Scaling Analysis**: \n",
    "   - Load models of different sizes (GPT-2 small, medium, large) and compare their performance on the same tasks\n",
    "   - Plot the relationship between model size and task performance\n",
    "\n",
    "2. **Few-Shot Learning**:\n",
    "   - Create a custom few-shot learning task with your own examples\n",
    "   - Test how the number of examples (0, 1, 5, 10) affects performance\n",
    "\n",
    "3. **T5 Fine-tuning**:\n",
    "   - Fine-tune T5-small on a specific task using the datasets library\n",
    "   - Compare zero-shot vs fine-tuned performance\n",
    "\n",
    "4. **MoE Implementation**:\n",
    "   - Extend the SimpleMoE class to support top-k routing (k>1)\n",
    "   - Implement load balancing loss to ensure even expert utilization\n",
    "\n",
    "5. **Efficiency Optimization**:\n",
    "   - Implement KV caching for faster autoregressive generation\n",
    "   - Measure the speed improvement with and without caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we explored:\n",
    "- T5's unified text-to-text framework and span corruption training\n",
    "- GPT-style few-shot learning and emergent abilities\n",
    "- Scaling laws and their implications for model performance\n",
    "- Efficient inference techniques including quantization and MoE\n",
    "- Practical implementation of advanced transformer concepts\n",
    "\n",
    "Key takeaways:\n",
    "1. Scale brings emergence - larger models show qualitatively different behaviors\n",
    "2. Different architectures excel at different tasks\n",
    "3. Efficiency techniques are crucial for deploying large models\n",
    "4. Few-shot learning reduces the need for task-specific fine-tuning\n",
    "5. The field is rapidly evolving with new techniques and models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}