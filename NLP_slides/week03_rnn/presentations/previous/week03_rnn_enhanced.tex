\documentclass[8pt,aspectratio=169,8pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{subfigure}
\usepackage{hyperref}

% Theme settings
\usetheme{Frankfurt}
\usecolortheme{seahorse}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

% Custom colors
\definecolor{darkblue}{RGB}{0,51,102}
\definecolor{lightblue}{RGB}{173,216,230}
\definecolor{codegreen}{RGB}{0,128,0}
\definecolor{codegray}{RGB}{150,150,150}
\definecolor{codepurple}{RGB}{128,0,128}
\definecolor{backcolor}{RGB}{245,245,245}

% Code listing settings
\lstset{
    backgroundcolor=\color{backcolor},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    showstringspaces=false,
    frame=single,
    numbers=left,
    language=Python
}

% Include slide layouts
\input{../slide_layouts.tex}

\title[Week 3: RNNs]{Natural Language Processing Course}
\subtitle{Week 3: Recurrent Neural Networks (RNNs)}
\author{Joerg R. Osterrieder \\ \url{www.joergosterrieder.com}}
\date{}

\begin{document}

% Title page
\begin{frame}
    \titlepage
\end{frame}

\section{Week 3: Recurrent Neural Networks}

% Title slide
\begin{frame}
    \centering
    \vspace{2cm}
    {\Large \textbf{Week 3}}\\
    \vspace{0.5cm}
    {\huge \textbf{Recurrent Neural Networks}}\\
    \vspace{1cm}
    {\large Teaching Computers to Remember}
\end{frame}

% Motivation: The problem with context
\begin{frame}[t]{Why Your Voice Assistant Sometimes Fails}
    \textbf{You:} "Set a timer for 10 minutes"
    
    \textbf{Alexa:} "Timer set for 10 minutes"
    
    \textbf{You:} "Actually, make it 15"
    
    \textbf{Alexa:} "I'm not sure what you want me to make"
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{red!20}{
        \parbox{0.8\textwidth}{
            \centering
            Word embeddings don't remember what came before!
        }
    }
    \end{center}
    
    \vspace{0.5em}
    \textbf{The problem:} Understanding "it" requires remembering "timer"
    
    \textbf{The solution:} Networks that maintain memory of past inputs
\end{frame}

% Real-world applications
\begin{frame}[t]{RNNs Power Sequential Understanding Everywhere}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Voice Assistants (2024):}
            \begin{itemize}
                \item Siri: LSTM for complex commands\footnotemark
                \item Google: RNN-T model (450MB)\footnotemark
                \item Context maintained across turns
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Text Generation:}
            \begin{itemize}
                \item Gmail Smart Compose
                \item Code completion (before Copilot)
                \item Character-by-character prediction
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Still Best For:}
            \begin{itemize}
                \item Stock price prediction\footnotemark
                \item Speech recognition on phones
                \item Music generation
                \item Energy demand forecasting
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Key Advantage:}
            
            Processes sequences step-by-step, just like humans read!
        \end{column}
    \end{columns}
    
    \footnotetext[1]{Apple's on-device processing}
    \footnotetext[2]{Gboard's efficient on-device model}
    \footnotetext[3]{LSTMs dominate financial forecasting in 2024}
\end{frame}

% Learning objectives
\begin{frame}[t]{Week 3: What You'll Master}
    \textbf{By the end of this week, you will:}
    \begin{itemize}
        \item \textbf{Understand} why order matters in language
        \item \textbf{Build} intuition for how RNNs maintain memory
        \item \textbf{Implement} an LSTM from scratch
        \item \textbf{Solve} the vanishing gradient problem
        \item \textbf{Create} a text generator that remembers context
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            \textbf{Core Insight:} Process sequences like humans do - one step at a time, remembering what came before
        }
    }
    \end{center}
\end{frame}

% The order matters example
\begin{frame}[t]{Why Order Matters: A Simple Example}
    \textbf{Same words, different order, different meaning:}
    
    \vspace{0.5em}
    \begin{enumerate}
        \item "Dog bites man" \hspace{2cm} (Not news)
        \item "Man bites dog" \hspace{2cm} (Front page news!)
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Word embeddings alone can't distinguish these!}
    
    Both have same word vectors: \{dog, bites, man\}
    
    \vspace{0.5em}
    \textbf{More examples where order is crucial:}
    \begin{itemize}
        \item "not bad" vs "bad, not good"
        \item "can you?" vs "you can"
        \item "barely passed" vs "passed barely" (different emphasis)
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            Language is fundamentally sequential - we need models that process it that way
        }
    }
    \end{center}
\end{frame}

% RNN intuition building
\begin{frame}[t]{The Brilliant Idea: Networks with Memory}
    \textbf{How humans read:}
    
    "The movie was really..."
    
    \begin{itemize}
        \item Read "The" $\rightarrow$ remember it
        \item Read "movie" $\rightarrow$ remember "The movie"
        \item Read "was" $\rightarrow$ remember "The movie was"
        \item Read "really" $\rightarrow$ expect adjective next
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{RNN does exactly this:}\footnotemark
    \begin{enumerate}
        \item Process one word at a time
        \item Maintain a "hidden state" (memory)
        \item Update memory with each new word
        \item Use memory to predict next word
    \end{enumerate}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            Hidden state = What the network remembers so far
        }
    }
    \end{center}
    
    \footnotetext{Rumelhart, Hinton \& Williams (1986). "Learning representations by back-propagating errors", Nature}
\end{frame}

% Visual RNN unfolding
\begin{frame}[t]{Visualizing RNNs: Unfolding Through Time}
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/rnn_unfolding.pdf}
    
    \vspace{0.5em}
    \textbf{Key insight: Same weights used at every step!}
    \begin{itemize}
        \item This is why it can handle any sequence length
        \item But also why training is tricky (gradient problems)
    \end{itemize}
\end{frame}

% Simple RNN math
\conceptslide{RNN Mathematics: Surprisingly Simple!}{
    \textbf{Just two equations:}
    
    \vspace{0.5em}
    \eqbox{
        $\begin{aligned}
        h_t &= \tanh(W_h h_{t-1} + W_x x_t + b_h) \\
        y_t &= W_y h_t + b_y
        \end{aligned}$
    }
    
    \vspace{0.5em}
    Where:
    \begin{itemize}
        \item $h_t$: Hidden state (memory) at time $t$
        \item $x_t$: Input word embedding at time $t$
        \item $y_t$: Output prediction at time $t$
        \item $W$: Weight matrices (shared across time!)
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{In plain English:}
    
    New memory = function(old memory + new input)
}{
    % Could add a simple diagram here
}

% Implementation
\begin{frame}[fragile]{Building an RNN: Complete Implementation}
    \begin{columns}[T]
        \column{0.55\textwidth}
\begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        """Initialize RNN with typical hidden size 256"""
        super().__init__()
        self.hidden_size = hidden_size
        
        # Learnable parameters
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.h2o = nn.Linear(hidden_size, output_size)
        self.tanh = nn.Tanh()
        
    def forward(self, input, hidden):
        """Process one time step"""
        # Combine input and previous hidden state
        combined = torch.cat((input, hidden), 1)
        
        # Update hidden state (memory)
        hidden = self.tanh(self.i2h(combined))
        
        # Generate output
        output = self.h2o(hidden)
        
        return output, hidden
    
    def init_hidden(self, batch_size):
        """Start with blank memory"""
        return torch.zeros(batch_size, self.hidden_size)
\end{lstlisting}
\column{0.43\textwidth}
        \codeexplanation{
            \textbf{Design Choices:}
            \begin{itemize}
                \item Hidden size typically 128-512\footnotemark
                \item Tanh keeps values in [-1, 1]
                \item Same weights for all time steps
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Usage Pattern:}
            \begin{small}
            \texttt{hidden = rnn.init\_hidden(32)}\\
            \texttt{for word in sentence:}\\
            \texttt{~~~~out, hidden = rnn(word, hidden)}
            \end{small}
            
            \footnotetext{256 is memory-bandwidth optimal}
        }
    \end{columns}
\end{frame}

% The vanishing gradient problem
\begin{frame}[t]{The Fatal Flaw: Vanishing Gradients}
    \textbf{Try to learn from this sentence:}
    
    "The student who the professor who won the Nobel Prize taught {\color{red}was} brilliant"
    
    \vspace{0.5em}
    \textbf{Problem:} "was" agrees with "student" (15 words back!)
    
    \vspace{0.5em}
    \textbf{What happens during training:}\footnotemark
    \begin{itemize}
        \item Gradient flows backward through time
        \item Gets multiplied by weights at each step
        \item After 10-15 steps: gradient $\approx$ 0
        \item Network can't learn long dependencies!
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \includegraphics[width=0.8\textwidth]{../figures/vanishing_gradient.pdf}
    \end{center}
    
    \footnotetext{Hochreiter (1991) thesis; Bengio et al. (1994) "Learning long-term dependencies with gradient descent is difficult"}
\end{frame}

% LSTM solution
\begin{frame}[t]{The LSTM Solution: Gated Memory}
    \textbf{The breakthrough (1997):}\footnotemark Add gates to control memory!
    
    \vspace{0.5em}
    \textbf{Three gates, like a smart filing system:}
    \begin{enumerate}
        \item \textbf{Forget Gate:} What to throw away
        \item \textbf{Input Gate:} What new info to store
        \item \textbf{Output Gate:} What to use right now
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Analogy: Reading a mystery novel}
    \begin{itemize}
        \item See new character $\rightarrow$ Store in memory (input gate)
        \item Character becomes irrelevant $\rightarrow$ Forget them (forget gate)
        \item Need to solve mystery $\rightarrow$ Recall important clues (output gate)
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{green!20}{
        \parbox{0.8\textwidth}{
            \centering
            LSTMs can remember for 100+ steps (vs 10-15 for vanilla RNNs)
        }
    }
    \end{center}
    
    \footnotetext{Hochreiter \& Schmidhuber (1997). "Long Short-Term Memory", Neural Computation}
\end{frame}

% LSTM implementation
\begin{frame}[fragile]{LSTM Implementation: The Gated Architecture}
    \begin{columns}[T]
     \column{0.55\textwidth}   
\begin{lstlisting}[language=Python,basicstyle=\ttfamily\tiny]
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        """LSTM with typical hidden size 256-512"""
        super().__init__()
        self.hidden_size = hidden_size
        
        # Gates
        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.candidate_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)
        
        # Output projection
        self.h2o = nn.Linear(hidden_size, output_size)
        
    def forward(self, input, hidden, cell):
        """Process one time step with gated memory"""
        combined = torch.cat((input, hidden), 1)
        
        # Forget gate: what to discard from memory
        f_gate = torch.sigmoid(self.forget_gate(combined))
        
        # Input gate: what new info to store
        i_gate = torch.sigmoid(self.input_gate(combined))
        candidate = torch.tanh(self.candidate_gate(combined))
        
        # Update cell state (long-term memory)
        cell = f_gate * cell + i_gate * candidate
        
        # Output gate: what to output based on memory
        o_gate = torch.sigmoid(self.output_gate(combined))
        hidden = o_gate * torch.tanh(cell)
        
        output = self.h2o(hidden)
        return output, hidden, cell
\end{lstlisting}
        \column{0.45\textwidth}
        \codeexplanation{
            \textbf{Why Gates Work:}
            \begin{itemize}
                \item Sigmoid: 0-1 range (percentage)
                \item Multiplication: gating mechanism
                \item Addition: gradient highway
            \end{itemize}
            
            \vspace{0.3em}
            \textbf{Memory Management:}
            \begin{itemize}
                \item \texttt{cell}: Long-term memory
                \item \texttt{hidden}: Working memory
                \item Gates control information flow
            \end{itemize}
            
            \vspace{0.3em}
            \textbf{Training Benefits:}
            \begin{itemize}
                \item Gradients flow through addition
                \item Can learn 100+ step dependencies
                \item Solves vanishing gradient!
            \end{itemize}
        }
    \end{columns}
\end{frame}

% Real performance comparison
\resultslide{RNN vs LSTM: The Difference is Dramatic}{
    \centering
    \includegraphics[width=0.65\textwidth]{../figures/rnn_vs_lstm_performance.pdf}
}{
    \begin{itemize}
        \item Vanilla RNN: Forgets after ~10 steps
        \item LSTM: Maintains memory for 100+ steps
        \item Real impact: Can model entire paragraphs, not just phrases
        \item Training time: 2-3x slower but worth it
    \end{itemize}
}

% Where RNNs still win
\begin{frame}[t]{RNNs vs Transformers: When Sequential Wins (2024)}
    \textbf{Despite transformer dominance, RNNs still excel at:}
    
    \vspace{0.5em}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{1. Resource-Constrained:}
            \begin{itemize}
                \item Google's RNN-T: 450MB\footnotemark
                \item Transformer equivalent: 2-5GB
                \item Perfect for phones/IoT
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{2. Streaming/Real-time:}
            \begin{itemize}
                \item Process as data arrives
                \item No need to see entire sequence
                \item Live transcription, translation
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{3. Time Series:}
            \begin{itemize}
                \item Stock prediction\footnotemark
                \item Energy demand forecasting
                \item ES-RNN won M4 competition
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{4. Truly Sequential:}
            \begin{itemize}
                \item Music generation
                \item Handwriting synthesis
                \item Robot control sequences
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            Rule: Use RNNs when order truly matters and resources are limited
        }
    }
    \end{center}
    
    \footnotetext[1]{Gboard on-device speech recognition}
    \footnotetext[2]{LSTMs still dominate financial forecasting in 2024}
\end{frame}

% Common pitfalls
\begin{frame}[t]{Common RNN Pitfalls and Solutions}
    \textbf{1. Exploding Gradients}
    \begin{itemize}
        \item Problem: Gradients grow exponentially
        \item Solution: Gradient clipping (max norm = 5)
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{2. Exposure Bias}\footnotemark
    \begin{itemize}
        \item Problem: Train with truth, test with predictions
        \item Solution: Scheduled sampling (mix both)
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{3. Slow Training}
    \begin{itemize}
        \item Problem: Can't parallelize across time
        \item Solution: Truncated backprop, smaller sequences
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Real Example - Text Generation:}
    \begin{itemize}
        \item Without fixes: "The the the the..."
        \item With fixes: "The movie was really entertaining"
    \end{itemize}
    
    \footnotetext{Major cause of repetition and hallucination in generated text}
\end{frame}

% Exercise
\begin{frame}[t]{Week 3 Exercise: Build a Context-Aware Chatbot}
    \textbf{Your Mission:} Create a chatbot that remembers conversation context
    
    \vspace{0.5em}
    \textbf{Example Conversation:}
    \begin{itemize}
        \item User: "My name is Alice"
        \item Bot: "Nice to meet you, Alice!"
        \item User: "What's my name?"
        \item Bot: "Your name is Alice"
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Implementation Steps:}
    \begin{enumerate}
        \item Implement LSTM-based encoder
        \item Maintain conversation state
        \item Generate contextual responses
        \item Handle 5-10 turn conversations
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Bonus Challenges:}
    \begin{itemize}
        \item Compare RNN vs LSTM memory retention
        \item Visualize hidden states over conversation
        \item Implement attention to see what it remembers
        \item Try different hidden sizes (128, 256, 512)
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{You'll discover:} Why Siri sometimes forgets context mid-conversation!
\end{frame}

% Summary
\begin{frame}[t]{Key Takeaways: Sequential Processing Matters}
    \textbf{What we learned:}
    \begin{itemize}
        \item Language is inherently sequential - order matters!
        \item RNNs process sequences step-by-step with memory
        \item Vanilla RNNs suffer from vanishing gradients (~10 steps)
        \item LSTMs use gates to remember for 100+ steps
        \item Still best for resource-constrained and streaming applications
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{The evolution:}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            N-grams (no memory) $\rightarrow$ Word2Vec (no order) $\rightarrow$ RNNs (sequential memory)
        }
    }
    \end{center}
    
    \vspace{0.5em}
    \textbf{Next week: Sequence-to-Sequence}
    
    How do we use RNNs for translation, where input and output lengths differ?
\end{frame}

% References
\begin{frame}[t]{References and Further Reading}
    \textbf{Foundational Papers:}
    \begin{itemize}
        \item Rumelhart et al. (1986). "Learning representations by back-propagating errors", Nature
        \item Hochreiter \& Schmidhuber (1997). "Long Short-Term Memory", Neural Computation
        \item Bengio et al. (1994). "Learning long-term dependencies with gradient descent is difficult"
    \end{itemize}
    
    \textbf{Modern Applications:}
    \begin{itemize}
        \item Google's RNN-T for on-device speech (2024)
        \item ES-RNN winning M4 forecasting competition
        \item Financial time series with LSTMs
    \end{itemize}
    
    \textbf{Recommended Resources:}
    \begin{itemize}
        \item Colah's Blog: "Understanding LSTM Networks" (visual guide)
        \item Karpathy's "The Unreasonable Effectiveness of RNNs"
        \item PyTorch RNN tutorial with Shakespeare generation
    \end{itemize}
\end{frame}
\end{document}
