\documentclass[8pt,aspectratio=169,8pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{subfigure}
\usepackage{hyperref}

% Theme settings
\usetheme{Frankfurt}
\usecolortheme{seahorse}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

% Custom colors
\definecolor{darkblue}{RGB}{0,51,102}
\definecolor{lightblue}{RGB}{173,216,230}
\definecolor{codegreen}{RGB}{0,128,0}
\definecolor{codegray}{RGB}{150,150,150}
\definecolor{codepurple}{RGB}{128,0,128}
\definecolor{backcolor}{RGB}{245,245,245}

% Code listing settings
\lstset{
    backgroundcolor=\color{backcolor},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    showstringspaces=false,
    frame=single,
    numbers=left,
    language=Python
}

% Include slide layouts
\input{../slide_layouts.tex}

\title[Week 8: Tokenization]{Natural Language Processing Course}
\subtitle{Week 8: Tokenization and Subword Models}
\author{Joerg R. Osterrieder \\ \url{www.joergosterrieder.com}}
\date{}

\begin{document}

% Title page
\begin{frame}
    \titlepage
\end{frame}

\section{Week 8: Tokenization and Subword Models}

% Title slide
\begin{frame}
    \centering
    \vspace{2cm}
    {\Large \textbf{Week 8}}\\
    \vspace{0.5cm}
    {\huge \textbf{Tokenization}}\\
    \vspace{1cm}
    {\large The Hidden Foundation of Every LLM}
\end{frame}

% Motivation: The vocabulary explosion
\begin{frame}[t]{The Word That Broke Google Translate}
    \textbf{2016: A user typed "Pneumonoultramicroscopicsilicovolcanoconiosis"}
    
    \vspace{0.5em}
    Result: System crashed.\footnotemark
    
    \vspace{0.5em}
    \textbf{The problem:}
    \begin{itemize}
        \item English: 170,000 words in current use
        \item Medical terms: 100,000+ additional
        \item New words daily: "COVID-19", "cryptocurrency", "mansplaining"
        \item Misspellings: "recieve", "definately", "occured"
        \item Other languages: 7,000+ languages worldwide!
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{red!20}{
        \parbox{0.8\textwidth}{
            \centering
            Can't have a token for every possible word - memory explosion!
        }
    }
    \end{center}
    
    \vspace{0.5em}
    \textbf{The dilemma:} How do you handle infinite vocabulary with finite memory?
    
    \footnotetext{Simplified example - real systems had various OOV issues}
\end{frame}

% The tokenization spectrum
\begin{frame}[t]{The Tokenization Spectrum: Characters vs Words}
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/tokenization_spectrum.pdf}
    
    \vspace{0.5em}
    \textbf{The Goldilocks problem:}
    \begin{itemize}
        \item Characters: Too fine-grained (sequences too long)
        \item Words: Too coarse (vocabulary too large)
        \item Subwords: Just right!
    \end{itemize}
\end{frame}

% Real-world impact
\begin{frame}[t]{Tokenization Powers Every Modern LLM (2024)}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Model Vocabularies:}
            \begin{itemize}
                \item GPT-2: 50,257 tokens
                \item GPT-3/4: 50,257 tokens
                \item BERT: 30,522 tokens
                \item T5: 32,000 tokens
                \item LLaMA: 32,000 tokens
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Why These Numbers?}
            \begin{itemize}
                \item Power of 2 for efficiency
                \item Covers 99.9\% of text
                \item Balances sequence length
                \item Works across languages
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Tokenization Methods:}
            \begin{itemize}
                \item BPE: GPT family\footnotemark
                \item WordPiece: BERT family
                \item SentencePiece: T5, mT5
                \item Unigram: Some Japanese models
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Critical Impact:}
            \begin{itemize}
                \item Wrong tokenization → 50\% performance drop
                \item Affects prompt cost (GPT-4: \$0.01/1K tokens)
                \item Determines max context length
                \item Controls multilingual ability
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            Tokenization is the most important choice you've never heard of
        }
    }
    \end{center}
    
    \footnotetext{Byte-level BPE specifically for GPT-2 onwards}
\end{frame}

% Learning objectives
\begin{frame}[t]{Week 8: What You'll Master}
    \textbf{By the end of this week, you will:}
    \begin{itemize}
        \item \textbf{Understand} why subword tokenization dominates
        \item \textbf{Implement} Byte Pair Encoding from scratch
        \item \textbf{Master} the trade-offs in vocabulary design
        \item \textbf{Analyze} tokenization's impact on different languages
        \item \textbf{Build} your own tokenizer for any domain
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            \textbf{Core Insight:} Break words into learnable pieces
        }
    }
    \end{center}
\end{frame}

% Character vs Word tokenization
\begin{frame}[t]{The Two Extremes: Characters vs Words}
    \textbf{Example sentence:} "The quick brown fox jumps"
    
    \vspace{0.5em}
    \textbf{Character tokenization:}
    \begin{itemize}
        \item Tokens: [T, h, e, \_, q, u, i, c, k, \_, b, r, o, w, n, \_, f, o, x, \_, j, u, m, p, s]
        \item Length: 25 tokens
        \item Vocabulary: ~100 (all ASCII)
        \item X Handles any text
        \item X Sequences too long
        \item X Model must learn spelling
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Word tokenization:}
    \begin{itemize}
        \item Tokens: [The, quick, brown, fox, jumps]
        \item Length: 5 tokens
        \item Vocabulary: 170,000+ (English)
        \item X Efficient sequences
        \item X Out-of-vocabulary words
        \item X Can't handle typos
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            Neither extreme works well - we need a middle ground
        }
    }
    \end{center}
\end{frame}

% BPE algorithm introduction
\begin{frame}[t]{Byte Pair Encoding: Learning to Compress}
    \textbf{The brilliant idea (1994):}\footnotemark Compress text by merging frequent pairs
    
    \vspace{0.5em}
    \textbf{Example: "low lower lowest"}
    
    \begin{enumerate}
        \item Start with characters: l o w \_ l o w e r \_ l o w e s t
        \item Count pairs: (l,o)=3, (o,w)=3, (w,e)=2, (e,r)=1...
        \item Merge most frequent: "lo" 
        \item New: lo w \_ lo w e r \_ lo w e s t
        \item Repeat: merge "low"
        \item Final: low \_ low er \_ low est
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Result:} Learned meaningful subwords!
    \begin{itemize}
        \item "low" = common root
        \item "er" = comparative suffix  
        \item "est" = superlative suffix
    \end{itemize}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{green!20}{
        \parbox{0.8\textwidth}{
            \centering
            BPE discovers linguistic structure automatically!
        }
    }
    \end{center}
    
    \footnotetext{Gage (1994) for compression; Sennrich et al. (2016) for NMT}
\end{frame}

% BPE visualization
\begin{frame}[t]{BPE in Action: Building a Vocabulary}
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/bpe_visualization.pdf}
    
    \vspace{0.5em}
    \textbf{Key insights:}
    \begin{itemize}
        \item Common words stay intact ("the", "and")
        \item Rare words split into pieces ("un-believ-able")
        \item Morphology emerges naturally
        \item Works for any language!
    \end{itemize}
\end{frame}

% BPE implementation
\begin{frame}[fragile]{Implementing BPE: The Core Algorithm}
    \begin{columns}[T]
        \column{0.55\textwidth}

\begin{lstlisting}[language=Python,basicstyle=\ttfamily\tiny]
import re
from collections import defaultdict, Counter

class BytePairEncoding:
    def __init__(self, vocab_size):
        """Initialize BPE tokenizer"""
        self.vocab_size = vocab_size
        self.word_tokenizer = re.compile(r'\w+|[^\w\s]')
        self.vocab = {}
        self.merges = []
        
    def get_stats(self, words):
        """Count frequency of adjacent pairs"""
        pairs = defaultdict(int)
        for word, freq in words.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i + 1]] += freq
        return pairs
    
    def merge_vocab(self, pair, words):
        """Merge most frequent pair"""
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        new_words = {}
        for word in words:
            new_word = word.replace(bigram, replacement)
            new_words[new_word] = words[word]
        return new_words
    
    def train(self, text, num_merges):
        """Train BPE on text corpus"""
        # Split into words
        words = self.word_tokenizer.findall(text.lower())
        
        # Initialize with character-level tokens
        word_freq = Counter(words)
        words = {' '.join(word) + ' </w>': freq 
                for word, freq in word_freq.items()}
        
        # Learn merges
        for i in range(num_merges):
            pairs = self.get_stats(words)
            if not pairs:
                break
                
            best_pair = max(pairs, key=pairs.get)
            self.merges.append(best_pair)
            words = self.merge_vocab(best_pair, words)
            
            if i % 100 == 0:
                print(f"Merge {i}: {best_pair} -> {''.join(best_pair)}")
                
        # Extract vocabulary
        for word, freq in words.items():
            for subword in word.split():
                self.vocab[subword] = self.vocab.get(subword, 0) + freq
\end{lstlisting}
        \column{0.43\textwidth}

        \codeexplanation{
            \textbf{Algorithm Steps:}
            \begin{itemize}
                \item Start with character vocabulary
                \item Count all adjacent pairs
                \item Merge most frequent pair
                \item Repeat until vocab size reached
            \end{itemize}
            
            \vspace{0.3em}
            \textbf{Design Choices:}
            \begin{itemize}
                \item End-of-word token </w>
                \item Preserves word boundaries
                \item Case normalization optional
                \item Typically 10K-50K merges
            \end{itemize}
            
            \vspace{0.3em}
            \textbf{Complexity:}
            \begin{itemize}
                \item Training: O(NM) for N words, M merges
                \item Encoding: O(L²) for length L
                \item Memory: O(V) for vocabulary V
            \end{itemize}
        }
    \end{columns}
\end{frame}

% Tokenizer comparison
\begin{frame}[t]{Tokenizer Comparison: Real Examples}
    \textbf{Text:} "Tokenization is surprisingly important for performance"
    
    \vspace{0.5em}
    \begin{small}
    \textbf{GPT-2 (BPE):}
    \texttt{["Token", "ization", " is", " surprisingly", " important", " for", " performance"]}
    
    7 tokens
    
    \vspace{0.5em}
    \textbf{BERT (WordPiece):}
    \texttt{["token", "\#\#ization", "is", "surprisingly", "important", "for", "performance"]}
    
    7 tokens (note: \#\# indicates continuation)
    
    \vspace{0.5em}
    \textbf{Character-level:}
    \texttt{["T", "o", "k", "e", "n", "i", "z", "a", "t", "i", "o", "n", " ", ...]}
    
    47 tokens!
    \end{small}
    
    \vspace{0.5em}
    \textbf{Impact on rare words:}
    
    "Pneumonoultramicroscopicsilicovolcanoconiosis" →
    \begin{itemize}
        \item BPE: ["P", "neum", "ono", "ult", "ram", "ic", "ros", "cop", "ic", "sil", "ico", "vol", "can", "oc", "on", "ios", "is"]
        \item Still handles it! (17 subwords vs 45 characters)
    \end{itemize}
\end{frame}

% Language-specific challenges
\begin{frame}[t]{Tokenization Across Languages}
    \centering
    \includegraphics[width=0.75\textwidth]{../figures/multilingual_tokenization.pdf}
    
    \vspace{0.5em}
    \textbf{Key challenges:}
    \begin{itemize}
        \item Chinese/Japanese: No spaces between words
        \item Arabic/Hebrew: Right-to-left scripts
        \item Korean: Syllable blocks (hangul)
        \item Code-switching: Multiple languages in one text
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Solution: SentencePiece}
    \begin{itemize}
        \item Treats input as raw byte stream
        \item No pre-tokenization needed
        \item Language-agnostic
        \item Used in multilingual models (mT5, XLM-R)
    \end{itemize}
\end{frame}

% Tokenization effects
\resultslide{Tokenization's Hidden Impact}{
    \centering
    \includegraphics[width=0.65\textwidth]{../figures/tokenization_impact.pdf}
}{
    \begin{itemize}
        \item Wrong tokenizer: 50\% performance drop
        \item Prompt length varies 3x across tokenizers
        \item Some languages need 10x more tokens
        \item Affects: cost, speed, max context
        \item Can't change tokenizer after training!
    \end{itemize}
}

% Modern tokenizers
\begin{frame}[t]{State-of-the-Art Tokenizers (2024)}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Byte-level BPE (GPT family):}
            \begin{itemize}
                \item Handles ANY text (even emojis)
                \item No unknown tokens
                \item Consistent across languages
                \item Used by: GPT-2/3/4, RoBERTa
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{SentencePiece (T5, mT5):}
            \begin{itemize}
                \item Language independent
                \item Reversible tokenization
                \item BPE or Unigram options
                \item Handles 100+ languages
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Recent Innovations:}
            \begin{itemize}
                \item \textbf{CANINE:} Character-level (2021)
                \item \textbf{ByT5:} Byte-level tokens
                \item \textbf{PIXEL:} Image-based text
                \item \textbf{Charformer:} Learnable tokenization
            \end{itemize}
            
            \vspace{0.5em}
            \textbf{Tokenizer Selection:}
            \begin{itemize}
                \item English only: BPE (30-50K)
                \item Multilingual: SentencePiece
                \item Code: Include special tokens
                \item Domain-specific: Custom training
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \begin{center}
    \colorbox{yellow!20}{
        \parbox{0.8\textwidth}{
            \centering
            2024 trend: Larger vocabularies (100K+) for better multilingual support
        }
    }
    \end{center}
\end{frame}

% Practical considerations
\begin{frame}[t]{Tokenization Best Practices}
    \textbf{1. Vocabulary Size Trade-offs:}
    \begin{itemize}
        \item Small (8K): Long sequences, better generalization
        \item Medium (32K): Standard choice, balanced
        \item Large (100K+): Shorter sequences, more memory
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{2. Special Tokens:}
    \begin{itemize}
        \item [PAD], [UNK], [CLS], [SEP], [MASK]
        \item Domain tokens: [CODE], [MATH], [URL]
        \item Control tokens: [INST], [/INST]
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{3. Common Pitfalls:}
    \begin{itemize}
        \item Tokenizer/model mismatch (50\% accuracy drop!)
        \item Forgetting to handle special characters
        \item Not preserving whitespace information
        \item Case sensitivity mismatches
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Real Example - GPT costs:}
    \begin{itemize}
        \item "Hello world" = 2 tokens = \$0.00002
        \item "Chinese text" = 4 tokens = \$0.00004 (2x cost!)
        \item Emoji = 1-3 tokens depending on tokenizer
    \end{itemize}
\end{frame}

% Exercise
\begin{frame}[t]{Week 8 Exercise: Build Your Domain Tokenizer}
    \textbf{Your Mission:} Create optimal tokenizer for your domain
    
    \vspace{0.5em}
    \textbf{Part 1: Analyze Tokenization Impact}
    \begin{itemize}
        \item Compare GPT-2, BERT, T5 tokenizers
        \item Tokenize: English, code, multilingual text
        \item Measure compression rates
        \item Calculate cost differences
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Part 2: Train Custom BPE}
    \begin{itemize}
        \item Choose domain: medical, legal, code, etc.
        \item Collect domain corpus (10MB+)
        \item Train BPE with different vocab sizes
        \item Compare with general tokenizers
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Part 3: Optimization Experiments}
    \begin{itemize}
        \item Test character vs subword vs word
        \item Measure model performance impact
        \item Analyze out-of-vocabulary rates
        \item Optimize for your use case
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{You'll discover:} Why tokenization is make-or-break for LLMs!
\end{frame}

% Summary
\begin{frame}[t]{Key Takeaways: The Foundation of Language Models}
    \textbf{What we learned:}
    \begin{itemize}
        \item Tokenization solves the infinite vocabulary problem
        \item Subwords balance efficiency and coverage
        \item BPE learns meaningful units automatically
        \item Different tokenizers for different needs
        \item Critical impact on cost and performance
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{The evolution:}
    \begin{center}
    \colorbox{lightblue!30}{
        \parbox{0.8\textwidth}{
            \centering
            Characters → Words → Subwords → Learned tokenization
        }
    }
    \end{center}
    
    \vspace{0.5em}
    \textbf{Why it matters:}
    \begin{itemize}
        \item Determines model capabilities
        \item Affects all downstream tasks
        \item Can't fix after training!
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Next week: Decoding Strategies}
    
    How do we generate coherent text from token probabilities?
\end{frame}

% References
\begin{frame}[t]{References and Further Reading}
    \textbf{Foundational Papers:}
    \begin{itemize}
        \item Sennrich et al. (2016). "Neural Machine Translation of Rare Words with Subword Units"
        \item Kudo \& Richardson (2018). "SentencePiece: Language-independent subword tokenizer"
        \item Schuster \& Nakajima (2012). "Japanese and Korean voice search" (WordPiece)
    \end{itemize}
    
    \textbf{Recent Advances:}
    \begin{itemize}
        \item Clark et al. (2021). "CANINE: Character-level transformers"
        \item Xue et al. (2021). "ByT5: Token-free transformers"
        \item Tay et al. (2021). "Charformer: Fast character transformers"
    \end{itemize}
    
    \textbf{Practical Resources:}
    \begin{itemize}
        \item Hugging Face Tokenizers library
        \item Google SentencePiece
        \item OpenAI tiktoken library
    \end{itemize}
\end{frame}
\end{document}
