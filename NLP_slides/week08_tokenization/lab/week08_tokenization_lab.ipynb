{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 8 Lab: Tokenization & Vocabulary\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different tokenization strategies (word, character, subword)\n",
        "- Implement Byte-Pair Encoding (BPE) from scratch\n",
        "- Compare tokenizers from popular NLP libraries\n",
        "- Analyze vocabulary efficiency and coverage\n",
        "\n",
        "## Prerequisites\n",
        "```bash\n",
        "pip install transformers torch numpy matplotlib sentencepiece\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "# Setup\n",
        "print('Week 8: Tokenization & Vocabulary')\n",
        "print('=' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Basic Tokenization Strategies\n",
        "\n",
        "Let's compare three fundamental approaches:\n",
        "1. **Word-level**: Split on whitespace/punctuation\n",
        "2. **Character-level**: Each character is a token\n",
        "3. **Subword-level**: Balance between word and character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text for tokenization\n",
        "sample_text = \"\"\"Natural language processing enables computers to understand human language.\n",
        "Tokenization is the first step in most NLP pipelines.\n",
        "Different tokenization strategies have different tradeoffs.\"\"\"\n",
        "\n",
        "# Word-level tokenization\n",
        "def word_tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Character-level tokenization\n",
        "def char_tokenize(text):\n",
        "    return list(text)\n",
        "\n",
        "word_tokens = word_tokenize(sample_text)\n",
        "char_tokens = char_tokenize(sample_text)\n",
        "\n",
        "print(f\"Original text length: {len(sample_text)} characters\")\n",
        "print(f\"\\nWord tokens ({len(word_tokens)} tokens):\")\n",
        "print(word_tokens[:15], '...')\n",
        "print(f\"\\nCharacter tokens ({len(char_tokens)} tokens):\")\n",
        "print(char_tokens[:30], '...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vocabulary analysis\n",
        "word_vocab = set(word_tokens)\n",
        "char_vocab = set(char_tokens)\n",
        "\n",
        "print(\"Vocabulary Comparison:\")\n",
        "print(f\"  Word vocabulary size: {len(word_vocab)}\")\n",
        "print(f\"  Character vocabulary size: {len(char_vocab)}\")\n",
        "print(f\"\\nCompression ratio (tokens/chars):\")\n",
        "print(f\"  Word: {len(word_tokens)/len(sample_text):.3f}\")\n",
        "print(f\"  Character: {len(char_tokens)/len(sample_text):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Byte-Pair Encoding (BPE) from Scratch\n",
        "\n",
        "BPE is the foundation of modern subword tokenization. Let's implement it step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleBPE:\n",
        "    \"\"\"Simple Byte-Pair Encoding implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_merges=100):\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = {}  # (pair) -> merged token\n",
        "        self.vocab = set()\n",
        "    \n",
        "    def get_stats(self, vocab):\n",
        "        \"\"\"Count frequency of adjacent pairs\"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i+1])] += freq\n",
        "        return pairs\n",
        "    \n",
        "    def merge_vocab(self, pair, vocab):\n",
        "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
        "        new_vocab = {}\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        \n",
        "        for word in vocab:\n",
        "            new_word = word.replace(bigram, replacement)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "        return new_vocab\n",
        "    \n",
        "    def fit(self, text):\n",
        "        \"\"\"Learn BPE merges from text\"\"\"\n",
        "        # Initialize vocabulary with character-level tokens\n",
        "        words = text.lower().split()\n",
        "        word_freqs = Counter(words)\n",
        "        \n",
        "        # Add end-of-word marker and split into characters\n",
        "        vocab = {' '.join(list(word) + ['</w>']): freq \n",
        "                 for word, freq in word_freqs.items()}\n",
        "        \n",
        "        print(f\"Initial vocabulary: {len(set(' '.join(vocab.keys()).split()))} tokens\")\n",
        "        \n",
        "        # Iteratively merge most frequent pairs\n",
        "        for i in range(self.num_merges):\n",
        "            pairs = self.get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "            \n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            vocab = self.merge_vocab(best_pair, vocab)\n",
        "            self.merges[best_pair] = ''.join(best_pair)\n",
        "            \n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"Merge {i+1}: {best_pair} -> {''.join(best_pair)} (freq: {pairs[best_pair]})\")\n",
        "        \n",
        "        # Build final vocabulary\n",
        "        self.vocab = set(' '.join(vocab.keys()).split())\n",
        "        print(f\"\\nFinal vocabulary size: {len(self.vocab)} tokens\")\n",
        "        return self\n",
        "    \n",
        "    def tokenize(self, word):\n",
        "        \"\"\"Tokenize a single word using learned merges\"\"\"\n",
        "        word = list(word.lower()) + ['</w>']\n",
        "        \n",
        "        while len(word) > 1:\n",
        "            pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
        "            \n",
        "            # Find first pair that can be merged\n",
        "            mergeable = [p for p in pairs if p in self.merges]\n",
        "            if not mergeable:\n",
        "                break\n",
        "            \n",
        "            # Merge first valid pair\n",
        "            pair = mergeable[0]\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
        "                    new_word.append(self.merges[pair])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            word = new_word\n",
        "        \n",
        "        return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train BPE on sample text\n",
        "training_text = \"\"\"the cat sat on the mat\n",
        "the dog ran in the park\n",
        "cats and dogs are popular pets\n",
        "running and sitting are activities\n",
        "the quick brown fox jumps over the lazy dog\"\"\"\n",
        "\n",
        "bpe = SimpleBPE(num_merges=50)\n",
        "bpe.fit(training_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tokenization\n",
        "test_words = ['cat', 'cats', 'running', 'unknown', 'tokenization']\n",
        "\n",
        "print(\"BPE Tokenization Results:\")\n",
        "print(\"-\" * 40)\n",
        "for word in test_words:\n",
        "    tokens = bpe.tokenize(word)\n",
        "    print(f\"{word:15} -> {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Using HuggingFace Tokenizers\n",
        "\n",
        "Let's compare our implementation with production tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load different tokenizers\n",
        "tokenizers = {\n",
        "    'BERT': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
        "    'GPT-2': AutoTokenizer.from_pretrained('gpt2'),\n",
        "    'T5': AutoTokenizer.from_pretrained('t5-small'),\n",
        "}\n",
        "\n",
        "test_sentence = \"Tokenization is fundamental to natural language processing.\"\n",
        "\n",
        "print(f\"Test sentence: '{test_sentence}'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    tokens = tokenizer.tokenize(test_sentence)\n",
        "    ids = tokenizer.encode(test_sentence)\n",
        "    print(f\"\\n{name} ({tokenizer.__class__.__name__}):\")\n",
        "    print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
        "    print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
        "    print(f\"  Token IDs: {ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize tokenization differences\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "sentences = [\n",
        "    \"Hello world\",\n",
        "    \"Tokenization is important\",\n",
        "    \"Supercalifragilisticexpialidocious\"\n",
        "]\n",
        "\n",
        "for ax, sentence in zip(axes, sentences):\n",
        "    token_counts = [len(tok.tokenize(sentence)) for tok in tokenizers.values()]\n",
        "    bars = ax.bar(tokenizers.keys(), token_counts, color=['#3333B2', '#FF7F0E', '#2CA02C'])\n",
        "    ax.set_title(f'\"{sentence}\"', fontsize=10)\n",
        "    ax.set_ylabel('Number of tokens')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, count in zip(bars, token_counts):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                str(count), ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Token Count Comparison Across Tokenizers', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Vocabulary Analysis\n",
        "\n",
        "Let's analyze how different tokenizers handle various text types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze tokenization of different text types\n",
        "test_cases = {\n",
        "    'Regular English': 'The quick brown fox jumps over the lazy dog.',\n",
        "    'Technical': 'The API endpoint returns JSON with UTF-8 encoding.',\n",
        "    'Numbers': 'In 2024, the model achieved 99.5% accuracy on 10,000 samples.',\n",
        "    'Code-like': 'def calculate_loss(y_pred, y_true): return mse(y_pred, y_true)',\n",
        "    'Rare words': 'Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease.',\n",
        "}\n",
        "\n",
        "results = []\n",
        "for case_name, text in test_cases.items():\n",
        "    row = {'Case': case_name, 'Text': text[:30] + '...'}\n",
        "    for tok_name, tokenizer in tokenizers.items():\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        row[f'{tok_name}_tokens'] = len(tokens)\n",
        "        row[f'{tok_name}_ratio'] = len(tokens) / len(text.split())\n",
        "    results.append(row)\n",
        "\n",
        "# Display results\n",
        "print(\"Tokenization Analysis\")\n",
        "print(\"=\" * 80)\n",
        "for r in results:\n",
        "    print(f\"\\n{r['Case']}:\")\n",
        "    print(f\"  Text: {r['Text']}\")\n",
        "    for tok_name in tokenizers.keys():\n",
        "        print(f\"  {tok_name}: {r[f'{tok_name}_tokens']} tokens (ratio: {r[f'{tok_name}_ratio']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Special Tokens and Vocabulary\n",
        "\n",
        "Understanding special tokens used by different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Special Tokens Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    special = {\n",
        "        'PAD': getattr(tokenizer, 'pad_token', None),\n",
        "        'UNK': getattr(tokenizer, 'unk_token', None),\n",
        "        'CLS': getattr(tokenizer, 'cls_token', None),\n",
        "        'SEP': getattr(tokenizer, 'sep_token', None),\n",
        "        'MASK': getattr(tokenizer, 'mask_token', None),\n",
        "        'BOS': getattr(tokenizer, 'bos_token', None),\n",
        "        'EOS': getattr(tokenizer, 'eos_token', None),\n",
        "    }\n",
        "    for token_type, token in special.items():\n",
        "        if token:\n",
        "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "            print(f\"  {token_type}: '{token}' (ID: {token_id})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Extend BPE**: Modify the SimpleBPE class to handle punctuation properly\n",
        "2. **Vocabulary Analysis**: Calculate the percentage of unknown tokens for each tokenizer on a corpus\n",
        "3. **Compression Ratio**: Compare compression ratios across different languages\n",
        "4. **Custom Tokenizer**: Train a SentencePiece tokenizer on a custom corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Calculate unknown token percentage\n",
        "def calculate_unk_percentage(tokenizer, text):\n",
        "    \"\"\"Calculate percentage of unknown tokens\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    unk_token = tokenizer.unk_token\n",
        "    if unk_token:\n",
        "        unk_count = tokens.count(unk_token)\n",
        "        return 100 * unk_count / len(tokens) if tokens else 0\n",
        "    return 0\n",
        "\n",
        "# Test with rare words\n",
        "rare_text = \"The xyzzy plugh was discovered in the colossal cave.\"\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    unk_pct = calculate_unk_percentage(tokenizer, rare_text)\n",
        "    print(f\"{name}: {unk_pct:.1f}% unknown tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, we explored:\n",
        "\n",
        "1. **Basic tokenization strategies**: Word, character, and subword approaches\n",
        "2. **BPE algorithm**: Implemented from scratch to understand merge operations\n",
        "3. **Production tokenizers**: Compared BERT (WordPiece), GPT-2 (BPE), and T5 (SentencePiece)\n",
        "4. **Vocabulary analysis**: Understood how tokenizers handle different text types\n",
        "5. **Special tokens**: Learned about model-specific tokens (PAD, UNK, CLS, etc.)\n",
        "\n",
        "**Key Takeaways**:\n",
        "- Subword tokenization balances vocabulary size with sequence length\n",
        "- Different models use different tokenization algorithms\n",
        "- Tokenization choices significantly impact model performance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
