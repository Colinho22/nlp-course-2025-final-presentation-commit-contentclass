% Section 9: Applications, Variants, and Practical Implementation

\begin{frame}[t]{GRU: Simplified LSTM Alternative}
\begin{center}
\textbf{Gated Recurrent Unit - Fewer Gates, Similar Performance}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Key Idea:} Simplify LSTM architecture

\vspace{3mm}
\textbf{Main Differences from LSTM:}
\begin{itemize}
\item Only 2 gates (vs 3 in LSTM)
\item No separate cell state
\item Fewer parameters (25\% reduction)
\item Faster training
\item Often comparable performance
\item Popular for quick experiments
\end{itemize}

\vspace{3mm}
\textbf{GRU Equations:}
\begin{align*}
z_t &= \sigma(W_z [h_{t-1}, x_t]) \quad \text{(update gate)} \\
r_t &= \sigma(W_r [h_{t-1}, x_t]) \quad \text{(reset gate)} \\
\tilde{h}_t &= \tanh(W[\textcolor{blue}{r_t \odot h_{t-1}}, x_t]) \\
h_t &= (1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{align*}

\vspace{3mm}
\textbf{How It Works:}
\begin{itemize}
\item \textbf{Update gate} $z_t$: How much to update
\item \textbf{Reset gate} $r_t$: How much past to forget
\item Single state $h_t$ (no separate $C_t$)
\item Simpler but still effective
\end{itemize}

\column{0.48\textwidth}
\textbf{When to Use GRU:}

\vspace{3mm}
\textbf{Advantages:}
\begin{itemize}
\item Faster to train (fewer parameters)
\item Less memory usage
\item Good starting point
\item Often matches LSTM performance
\item Easier to tune
\end{itemize}

\vspace{3mm}
\textbf{Choose GRU if:}
\begin{itemize}
\item Limited compute budget
\item Quick experimentation needed
\item Dataset not huge
\item Speed more important than last 1\% accuracy
\end{itemize}

\vspace{3mm}
\textbf{Choose LSTM if:}
\begin{itemize}
\item Need maximum performance
\item Very long sequences (100+ steps)
\item Complex dependencies
\item Have computational resources
\end{itemize}

\vspace{3mm}
\textbf{Empirical Comparison:}

\begin{itemize}
\item \textbf{Penn Treebank}: GRU 78.1 vs LSTM 78.4 perplexity
\item \textbf{IMDB sentiment}: GRU 89.2\% vs LSTM 89.5\% accuracy
\item \textbf{Training time}: GRU 30\% faster
\item \textbf{Memory}: GRU uses 25\% less
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}GRU: Often the pragmatic choice for sequence modeling}

\end{columns}
\end{frame}

\begin{frame}[t]{Bidirectional LSTM: Using Future Context}
\begin{center}
\textbf{Processing Sequences in Both Directions}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Key Idea:} See future context too

\vspace{3mm}
\textbf{Architecture:}
\begin{itemize}
\item Two LSTMs running simultaneously
\item Forward LSTM: $\overrightarrow{h}_t$ (left-to-right)
\item Backward LSTM: $\overleftarrow{h}_t$ (right-to-left)
\item Concatenate outputs at each step
\end{itemize}

$$h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]$$

\vspace{3mm}
\textbf{Example:} ``The cat sat on the \_\_\_''

\begin{itemize}
\item \textbf{Forward}: Sees ``The cat sat on the''
\item \textbf{Backward}: Sees full sentence from end
\item \textbf{Combined}: Full context for each word
\end{itemize}

\vspace{3mm}
\textbf{Advantages:}
\begin{itemize}
\item Complete sentence context
\item Better for classification
\item Improved accuracy (5-15\% typical)
\item Bidirectional dependencies captured
\item More robust predictions
\end{itemize}

\column{0.48\textwidth}
\textbf{Limitations:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Cannot generate}: Needs full input first
\item \textbf{2x slower}: Two LSTMs to run
\item \textbf{2x memory}: Double hidden states
\item \textbf{No real-time}: Must wait for complete sequence
\end{itemize}

\vspace{3mm}
\textbf{Use Cases:}

\vspace{3mm}
\textbf{Perfect for:}
\begin{itemize}
\item Sentence classification (sentiment, intent)
\item Named entity recognition (NER)
\item Part-of-speech (POS) tagging
\item Question answering (full question available)
\item Any classification with full sequence
\end{itemize}

\vspace{3mm}
\textbf{NOT for:}
\begin{itemize}
\item Text generation (no future context)
\item Real-time prediction (autocomplete)
\item Streaming data processing
\item Online learning scenarios
\end{itemize}

\vspace{3mm}
\textbf{Modern Connection:}
\begin{itemize}
\item BERT uses bidirectional context
\item Transformer encoder = bidirectional
\item Key insight: both directions matter
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}Bidirectional = classification workhorse}

\end{columns}
\end{frame}

\begin{frame}[t]{Architectural Extensions: Stacked, Attention, Peephole}
\begin{center}
\textbf{Advanced LSTM Configurations}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Stacked/Deep LSTMs:}

\vspace{3mm}
\begin{itemize}
\item Multiple LSTM layers stacked vertically
\item Layer 1 output becomes Layer 2 input
\item Hierarchical feature learning
\item 2-4 layers typical (diminishing returns after 4)
\end{itemize}

\vspace{3mm}
\textbf{Why Stacking Works:}
\begin{itemize}
\item \textbf{Layer 1}: Low-level patterns
\item \textbf{Layer 2}: Mid-level features
\item \textbf{Layer 3}: High-level abstractions
\item Similar to deep CNNs
\end{itemize}

\vspace{3mm}
\textbf{Attention Mechanism:}

\vspace{3mm}
\textbf{Problem:} Encoder-decoder bottleneck
\begin{itemize}
\item Single vector must encode entire source
\item Information loss for long sequences
\end{itemize}

\vspace{3mm}
\textbf{Solution:} Weighted combination
\begin{itemize}
\item Keep ALL encoder hidden states
\item Decoder ``attends'' to relevant parts
\item Dynamic focus at each decoding step
\item Dramatically improved translation quality
\end{itemize}

\column{0.48\textwidth}
\textbf{Attention Equations:}

$$\alpha_{t,i} = \frac{\exp(\text{score}(h_t, h_i))}{\sum_j \exp(\text{score}(h_t, h_j))}$$

$$c_t = \sum_i \alpha_{t,i} h_i$$

\begin{itemize}
\item $\alpha_{t,i}$ = attention weights
\item $c_t$ = context vector
\item Led directly to Transformers
\end{itemize}

\vspace{3mm}
\textbf{Peephole Connections:}

\begin{itemize}
\item Gates see cell state directly
\item $f_t = \sigma(W_f [h_{t-1}, x_t, C_{t-1}])$
\item Better timing information
\item Marginal improvements (1-2\%)
\item Rarely used in practice
\item More parameters, minimal gain
\end{itemize}

\vspace{3mm}
\textbf{Which Extensions Matter:}

\begin{itemize}
\item \textbf{Stacking}: Yes, use 2-3 layers
\item \textbf{Attention}: Yes, revolutionary
\item \textbf{Peephole}: No, skip it
\item \textbf{Bidirectional}: Yes, for classification
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}Attention was the key insight leading to Transformers}

\end{columns}
\end{frame}

\begin{frame}[t]{Modern Context: Where LSTMs Fit (2020-2025)}
\begin{center}
\textbf{LSTMs in the Transformer Era}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Transformers Dominate NLP:}

\vspace{3mm}
\begin{itemize}
\item \textbf{BERT} (2018): Bidirectional pretraining
\item \textbf{GPT} (2018+): Autoregressive generation
\item \textbf{T5, BART} (2019): Unified text-to-text
\item \textbf{ChatGPT} (2022): Massive scale
\end{itemize}

\vspace{3mm}
\textbf{Why Transformers Won NLP:}
\begin{itemize}
\item \textbf{Parallel training}: All positions at once
\item \textbf{Self-attention}: Direct connections
\item \textbf{Scalability}: 100B+ parameters
\item \textbf{Pre-training}: Transfer learning
\item \textbf{Long context}: 128K+ tokens
\end{itemize}

\vspace{3mm}
\textbf{Where LSTMs Still Excel:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Time series forecasting}
  \begin{itemize}
  \item Stock prices, weather, energy
  \item Still state-of-the-art
  \end{itemize}
\item \textbf{Mobile/Edge deployment}
  \begin{itemize}
  \item 10-100x smaller than Transformers
  \item Real-time on device
  \end{itemize}
\item \textbf{Streaming data}
  \begin{itemize}
  \item Process one token at a time
  \item Constant memory
  \end{itemize}
\end{itemize}

\column{0.48\textwidth}
\textbf{Hybrid Architectures:}

\vspace{3mm}
\begin{itemize}
\item \textbf{CNN + LSTM}: Video/audio analysis
  \begin{itemize}
  \item CNN extracts spatial features
  \item LSTM models temporal dynamics
  \end{itemize}
\item \textbf{LSTM + Attention}: Improved seq2seq
  \begin{itemize}
  \item Encoder: Bidirectional LSTM
  \item Decoder: LSTM + attention
  \item Pre-Transformer standard
  \end{itemize}
\item \textbf{Transformer + LSTM}: Hybrid models
  \begin{itemize}
  \item Transformer encoder
  \item LSTM decoder (faster inference)
  \end{itemize}
\end{itemize}

\vspace{3mm}
\textbf{Research Directions:}

\begin{itemize}
\item \textbf{Linear RNNs}: O(1) memory complexity
\item \textbf{State Space Models}: S4, Mamba
\item \textbf{Efficient Transformers}: Linformer, Performer
\item \textbf{Hardware optimization}: Custom LSTM chips
\item \textbf{Continual learning}: Streaming updates
\end{itemize}

\vspace{3mm}
\textbf{Bottom Line (2025):}

\begin{itemize}
\item \textbf{NLP}: Use Transformers
\item \textbf{Time series}: Use LSTMs
\item \textbf{Mobile}: Use LSTMs
\item \textbf{Learning}: Understand both
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}LSTMs remain relevant for efficiency-critical applications}

\end{columns}
\end{frame}

\begin{frame}[t]{Model Comparison: When to Use What}
\begin{center}
\textbf{Decision Framework for Sequence Modeling}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.55\textwidth}
\includegraphics[width=\textwidth]{../figures/model_comparison_table.pdf}

\vspace{3mm}
\textbf{Decision Tree:}

\vspace{2mm}
\textbf{Question 1:} Need bidirectional context?
\begin{itemize}
\item Yes: Bidirectional LSTM or Transformer
\item No: Continue to Q2
\end{itemize}

\vspace{2mm}
\textbf{Question 2:} Sequence length?
\begin{itemize}
\item $<$ 100 tokens: LSTM or GRU
\item $>$ 100 tokens: Transformer
\end{itemize}

\vspace{2mm}
\textbf{Question 3:} Inference speed critical?
\begin{itemize}
\item Yes: GRU or small Transformer
\item No: Full LSTM or Transformer
\end{itemize}

\column{0.42\textwidth}
\textbf{Practical Recommendations:}

\vspace{3mm}
\textbf{Use LSTM when:}
\begin{itemize}
\item Time series prediction
\item Sequential generation (char-by-char)
\item Mobile/edge deployment
\item Limited compute budget
\item Variable-length sequences
\end{itemize}

\vspace{3mm}
\textbf{Use GRU when:}
\begin{itemize}
\item Faster training needed
\item Similar to LSTM use cases
\item Slightly less memory
\item Often first try
\end{itemize}

\vspace{3mm}
\textbf{Use Transformer when:}
\begin{itemize}
\item Long sequences ($>$100 tokens)
\item Parallel processing available
\item State-of-the-art performance needed
\item Pre-trained models exist (BERT, GPT)
\item Most modern NLP tasks
\end{itemize}

\vspace{3mm}
\textbf{Hybrid Approaches:}
\begin{itemize}
\item CNN + LSTM for video/audio
\item LSTM + Attention for seq2seq
\item Transformer encoder + LSTM decoder
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}Transformers won NLP, but LSTMs remain relevant elsewhere}

\end{columns}
\end{frame}

\begin{frame}[t]{Attention Mechanism: Bridge to Transformers}
\begin{center}
\textbf{The Idea That Changed Everything}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{The Problem with LSTMs:}

\vspace{3mm}
In sequence-to-sequence tasks:
\begin{itemize}
\item Encoder compresses entire source into one vector
\item Decoder must use this single vector
\item Bottleneck for long sequences
\item Information loss
\end{itemize}

\vspace{3mm}
\textbf{The Attention Solution:}

\vspace{3mm}
Instead of single vector:
\begin{itemize}
\item Keep all encoder hidden states
\item Decoder ``attends'' to relevant parts
\item Weighted combination at each step
\item Dynamic focus
\end{itemize}

\vspace{3mm}
\textbf{Attention Score:}
$$\alpha_{t,i} = \frac{\exp(\text{score}(h_t, h_i))}{\sum_j \exp(\text{score}(h_t, h_j))}$$

\vspace{3mm}
\textbf{Context Vector:}
$$c_t = \sum_i \alpha_{t,i} h_i$$

\column{0.48\textwidth}
\textbf{Example: Translation}

\vspace{3mm}
English: ``The cat sat on the mat''

French: ``Le chat \_\_\_''

\vspace{3mm}
When generating ``assis'' (sat):
\begin{itemize}
\item High attention on ``sat'' (0.7)
\item Medium attention on ``cat'' (0.2)
\item Low attention on ``the'', ``on'' (0.05 each)
\item Almost zero on ``mat'' (0.01)
\end{itemize}

\vspace{3mm}
\textbf{Why It Matters:}

\begin{itemize}
\item Solves long sequence problem
\item Interpretable (visualize attention)
\item State-of-the-art 2015-2017
\item Direct path to Transformers
\end{itemize}

\vspace{3mm}
\textbf{Transformers (2017):}

\begin{itemize}
\item \textbf{Idea:} Use \textit{only} attention
\item Remove LSTM entirely
\item Fully parallel (no recurrence)
\item 100x faster training
\item Enabled GPT, BERT, modern NLP
\end{itemize}

\vspace{3mm}
\textbf{The Evolution:}
$$\text{LSTM} \rightarrow \text{LSTM+Attention} \rightarrow \text{Transformer}$$

\vspace{3mm}
{\footnotesize\color{annotGray}LSTMs taught us what we needed, then we moved beyond them}

\end{columns}
\end{frame}

\begin{frame}[t]{NLP Applications: Translation, Understanding, Generation}
\begin{center}
\textbf{How LSTMs Transformed Natural Language Processing}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Machine Translation:}

\vspace{3mm}
\textbf{The Breakthrough (2016):}
\begin{itemize}
\item Google Translate switches to LSTM
\item Translation errors dropped 60\% overnight
\item First human-quality translations
\item 100+ languages supported
\item Newspaper: ``AI achieved breakthrough''
\end{itemize}

\vspace{3mm}
\textbf{How It Works:}
\begin{itemize}
\item Encoder LSTM: Read source sentence
\item Decoder LSTM: Generate translation
\item Attention mechanism: Focus on relevant parts
\item Beam search: Find best translation
\end{itemize}

\vspace{3mm}
\textbf{Impact:}
\begin{itemize}
\item Billions of translations daily
\item Cross-lingual communication enabled
\item Business globalization accelerated
\item Foundation for GPT/BERT
\end{itemize}

\column{0.48\textwidth}
\textbf{Text Understanding:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Sentiment analysis}: Product reviews, social media
\item \textbf{Named entity recognition}: Extract names, locations
\item \textbf{Question answering}: Early chatbot systems
\item \textbf{Document classification}: Topic categorization
\item \textbf{Intent detection}: Virtual assistants
\item \textbf{Relation extraction}: Knowledge graphs
\end{itemize}

\vspace{3mm}
\textbf{Text Generation:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Story writing}: Character-level models
\item \textbf{Code completion}: Early IDE assistants
\item \textbf{Email auto-complete}: Gmail Smart Compose
\item \textbf{Dialogue systems}: Conversational AI
\item \textbf{Summarization}: Abstractive summaries
\item \textbf{Poetry generation}: Creative applications
\end{itemize}

\vspace{3mm}
\textbf{Why LSTMs Worked:}
\begin{itemize}
\item Long-range dependencies captured
\item Grammatical structure learned
\item Context-aware predictions
\item Sequential nature preserved
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}LSTMs dominated NLP from 2015-2018 before Transformers}

\end{columns}
\end{frame}

\begin{frame}[t]{Speech, Audio, and Vision Applications}
\begin{center}
\textbf{Beyond Text: Multimodal Sequence Modeling}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Speech Recognition:}

\vspace{3mm}
\textbf{The Revolution:}
\begin{itemize}
\item Siri, Alexa, Google Assistant
\item Word error rate halved (2015-2017)
\item Real-time transcription enabled
\item Accent-independent understanding
\item Background noise robustness
\end{itemize}

\vspace{3mm}
\textbf{Architecture:}
\begin{itemize}
\item Bidirectional LSTM (see full context)
\item CTC loss (alignment-free training)
\item Multi-layer stacking (hierarchical features)
\item Combined with CNN for spectrograms
\end{itemize}

\vspace{3mm}
\textbf{Audio Applications:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Speech synthesis}: WaveNet + LSTM (TTS)
\item \textbf{Music generation}: Compose melodies
\item \textbf{Audio classification}: Sound event detection
\item \textbf{Voice biometrics}: Speaker identification
\item \textbf{Audio enhancement}: Noise removal
\end{itemize}

\column{0.48\textwidth}
\textbf{Computer Vision:}

\vspace{3mm}
\textbf{Video Understanding:}
\begin{itemize}
\item \textbf{Video captioning}: Describe video content
\item \textbf{Action recognition}: Identify activities
\item \textbf{Video prediction}: Forecast future frames
\item \textbf{Temporal modeling}: Frame relationships
\item \textbf{Event detection}: Anomaly in surveillance
\end{itemize}

\vspace{3mm}
\textbf{Architecture: CNN+LSTM}
\begin{itemize}
\item CNN: Extract spatial features per frame
\item LSTM: Model temporal dynamics
\item Attention: Focus on important frames
\item End-to-end training possible
\end{itemize}

\vspace{3mm}
\textbf{Image-Text Tasks:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Image captioning}: CNN encoder + LSTM decoder
\item \textbf{Visual question answering}: Combined reasoning
\item \textbf{Scene understanding}: Spatial relationships
\item \textbf{Dense captioning}: Multiple region descriptions
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}LSTMs bridge spatial (CNN) and temporal processing}

\end{columns}
\end{frame}

\begin{frame}[t]{Time Series Forecasting Applications}
\begin{center}
\textbf{Where LSTMs Excel at Temporal Prediction}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Financial Forecasting:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Stock prediction}: Price movements, volatility
\item \textbf{Trading algorithms}: Automated strategies
\item \textbf{Risk assessment}: Portfolio optimization
\item \textbf{Market trends}: Pattern recognition
\item \textbf{Fraud detection}: Transaction anomalies
\end{itemize}

\vspace{3mm}
\textbf{Why LSTMs Work Here:}
\begin{itemize}
\item Capture long-term trends
\item Handle non-stationary data
\item Model complex dependencies
\item Real-time prediction capability
\end{itemize}

\vspace{3mm}
\textbf{Infrastructure Monitoring:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Weather forecasting}: Temperature, precipitation
\item \textbf{Energy consumption}: Load prediction, optimization
\item \textbf{Traffic prediction}: Route planning, congestion
\item \textbf{Supply chain}: Demand forecasting
\item \textbf{Utilities}: Water, gas consumption
\end{itemize}

\column{0.48\textwidth}
\textbf{Industrial Applications:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Anomaly detection}: Equipment monitoring
\item \textbf{Predictive maintenance}: Failure prediction
\item \textbf{Quality control}: Defect detection
\item \textbf{Sensor data analysis}: IoT systems
\item \textbf{Process optimization}: Manufacturing
\end{itemize}

\vspace{3mm}
\textbf{Key Advantages:}
\begin{itemize}
\item Sequential data naturally handled
\item Variable-length sequences
\item Missing data tolerance
\item Multi-variate time series
\item Uncertainty quantification
\end{itemize}

\vspace{3mm}
\textbf{Comparison with Alternatives:}

\begin{itemize}
\item \textbf{vs ARIMA}: Better for non-linear patterns
\item \textbf{vs Prophet}: More flexible, complex patterns
\item \textbf{vs Transformers}: More efficient for long series
\item \textbf{vs Simple RNN}: Much better long-term memory
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}Time series remains LSTM's strongest domain in 2025}

\end{columns}
\end{frame}

\begin{frame}[t]{Healthcare Applications and LSTM Impact}
\begin{center}
\textbf{Medical AI and Historical Impact}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Healthcare Applications:}

\vspace{3mm}
\textbf{Patient Monitoring:}
\begin{itemize}
\item ICU vital signs time series
\item Early warning systems
\item Sepsis prediction
\item Mortality risk assessment
\end{itemize}

\vspace{3mm}
\textbf{Clinical Analysis:}
\begin{itemize}
\item \textbf{ECG analysis}: Arrhythmia detection
\item \textbf{Disease progression}: Trajectory modeling
\item \textbf{Medical imaging}: Temporal scan sequences
\item \textbf{EEG analysis}: Seizure prediction
\end{itemize}

\vspace{3mm}
\textbf{Research Applications:}
\begin{itemize}
\item \textbf{Drug discovery}: Molecular sequences
\item \textbf{Protein folding}: Structure prediction
\item \textbf{Clinical notes}: Information extraction
\item \textbf{Treatment response}: Outcome prediction
\end{itemize}

\column{0.48\textwidth}
\textbf{Impact Statistics:}

\vspace{3mm}
\textbf{Research Impact:}
\begin{itemize}
\item \textbf{50,000+ papers}: Citing LSTM architecture
\item \textbf{Hochreiter \& Schmidhuber (1997)}: Original paper
\item \textbf{Foundation}: Enabled modern sequence models
\item \textbf{Turing Award} consideration-worthy
\end{itemize}

\vspace{3mm}
\textbf{Industry Impact:}
\begin{itemize}
\item \textbf{Google Translate}: 60\% error reduction (2016)
\item \textbf{Speech recognition}: Word error rate halved
\item \textbf{Billions of users}: Daily interactions
\item \textbf{Economic value}: Tens of billions USD
\end{itemize}

\vspace{3mm}
\textbf{Current Status (2025):}

\begin{itemize}
\item \textbf{NLP}: Transformers dominate
\item \textbf{Speech}: Hybrid LSTM+Transformer
\item \textbf{Time series}: LSTM still state-of-the-art
\item \textbf{Mobile/Edge}: Preferred for efficiency
\item \textbf{Streaming}: Real-time sequential processing
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}LSTMs revolutionized sequence modeling (2015-2018)}

\end{columns}
\end{frame}

\begin{frame}[fragile,t]{Implementation: PyTorch Code}
\begin{center}
\textbf{Building LSTM Models in Practice}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Model Definition:}

\begin{lstlisting}[language=Python,basicstyle=\tiny\ttfamily]
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, vocab_size,
                 embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(
            vocab_size, embed_dim)
        self.lstm = nn.LSTM(
            embed_dim, hidden_dim,
            num_layers=2, dropout=0.3,
            batch_first=True)
        self.fc = nn.Linear(
            hidden_dim, vocab_size)

    def forward(self, x):
        embed = self.embedding(x)
        lstm_out, (h_n, c_n) = self.lstm(embed)
        output = self.fc(lstm_out)
        return output

model = LSTMModel(10000, 100, 256)
\end{lstlisting}

\column{0.48\textwidth}
\textbf{Training Loop:}

\begin{lstlisting}[language=Python,basicstyle=\tiny\ttfamily]
optimizer = torch.optim.Adam(
    model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    for batch in dataloader:
        input_seq, target_seq = batch

        # Forward
        output = model(input_seq)
        loss = criterion(
            output.view(-1, vocab_size),
            target_seq.view(-1))

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), 1.0)

        optimizer.step()
\end{lstlisting}

\vspace{2mm}
\textbf{Key Parameters:}
\begin{itemize}
\item \texttt{num\_layers=2}: Stack 2 LSTMs
\item \texttt{dropout=0.3}: Regularization
\item \texttt{batch\_first=True}: (batch, seq, feat)
\item \texttt{clip\_grad\_norm\_}: Prevent explosion
\end{itemize}

\end{columns}
\end{frame}

\begin{frame}[t]{Summary: The Problem and LSTM Solution}
\begin{center}
\textbf{From Memory Problems to the LSTM Breakthrough}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{The Problem We Solved:}

\vspace{3mm}
\textbf{Challenge:} Next word prediction needs long context

\begin{itemize}
\item Example: ``I grew up in Paris \ldots speak fluent \_\_\_''
\item Need to remember ``Paris'' 18 words back
\item Connect location to language
\end{itemize}

\vspace{3mm}
\textbf{Previous Attempts Failed:}

\begin{itemize}
\item \textbf{N-grams}: Fixed 1-2 word window
  \begin{itemize}
  \item Can't see beyond immediate context
  \item Combinatorial explosion
  \end{itemize}
\item \textbf{RNNs}: Vanishing gradients
  \begin{itemize}
  \item Memory limited to 5-10 words
  \item Gradient: $0.5^{50} \approx 10^{-15}$
  \end{itemize}
\end{itemize}

\vspace{3mm}
\textbf{What We Needed:}
\begin{itemize}
\item Selective long-term memory (50-100+ words)
\item Learned control (what to remember/forget)
\item Stable gradients for training
\end{itemize}

\column{0.48\textwidth}
\textbf{The LSTM Solution:}

\vspace{3mm}
\textbf{Three Gates Control Flow:}

\begin{itemize}
\item {\color{forgetRed}\textbf{Forget gate}} $f_t$: Remove old information
\item {\color{inputGreen}\textbf{Input gate}} $i_t$: Add new information
\item {\color{outputBlue}\textbf{Output gate}} $o_t$: Reveal information
\item {\color{cellYellow}\textbf{Cell state}} $C_t$: Long-term storage
\end{itemize}

\vspace{3mm}
\textbf{The Mathematical Key:}

$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

\textbf{Addition} (not multiplication) = gradient highway

\vspace{3mm}
\textbf{Why It Works:}

\begin{itemize}
\item Cell state uses direct addition
\item No repeated matrix multiplication
\item Gradient: $\frac{\partial C_t}{\partial C_{t-1}} = f_t \approx 1$
\item Preserves information across 50-100+ steps
\end{itemize}

\vspace{3mm}
\textbf{Result:}
\begin{itemize}
\item 10x improvement in memory range
\item Stable training via gradient highway
\item Learned selective memory
\item Practical sequence modeling
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}The first architecture that truly solved the vanishing gradient problem}

\end{columns}
\end{frame}

\begin{frame}[t]{Summary: Impact and Practical Guidance}
\begin{center}
\textbf{Why LSTMs Matter and How to Use Them}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Historical Impact:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Breakthrough era:} 2015-2018
\item \textbf{Google Translate:} 60\% error reduction
\item \textbf{Speech recognition:} Word error rate halved
\item \textbf{Foundation:} Led to Transformers
\item \textbf{Citations:} 50,000+ papers
\end{itemize}

\vspace{3mm}
\textbf{Key Innovations:}

\begin{itemize}
\item \textbf{Cell state:} Separate memory path
\item \textbf{Gating:} Learned information control
\item \textbf{Gradient highway:} No vanishing
\item \textbf{Modular:} Stackable architecture
\end{itemize}

\vspace{3mm}
\textbf{When to Use LSTMs (2025):}

\begin{itemize}
\item \textbf{Time series:} Still state-of-the-art
\item \textbf{Mobile/Edge:} Efficient deployment
\item \textbf{Streaming:} Real-time processing
\item \textbf{Sequential generation:} Character-level
\item \textbf{Budget constraints:} When Transformers too expensive
\end{itemize}

\column{0.48\textwidth}
\textbf{Practical Implementation Tips:}

\vspace{3mm}
\textbf{Architecture:}
\begin{itemize}
\item Start: 2 layers, 256 hidden units
\item Small data: 1 layer, 128 units
\item Large data: 3-4 layers, 512-1024 units
\end{itemize}

\vspace{3mm}
\textbf{Training:}
\begin{itemize}
\item \textbf{Optimizer:} Adam (lr=0.001)
\item \textbf{Gradient clipping:} Essential (1.0-5.0)
\item \textbf{Dropout:} Between layers (0.3-0.5)
\item \textbf{Batch size:} 32-128
\item \textbf{Sequence length:} 50-100 tokens
\end{itemize}

\vspace{3mm}
\textbf{Debugging:}
\begin{itemize}
\item Monitor gate activations
\item Check gradient norms
\item Visualize cell state evolution
\item Start with tiny dataset (overfit first)
\end{itemize}

\vspace{3mm}
\textbf{Next Steps:}
\begin{itemize}
\item Implement LSTM from scratch
\item Study attention mechanism
\item Learn Transformers
\item Explore modern architectures
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}LSTMs: Foundation of modern sequence modeling}

\end{columns}
\end{frame}

\begin{frame}[t]{Essential Reading and Theory}
\begin{center}
\textbf{Foundational Resources for LSTM Understanding}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Must-Read Blog Posts:}

\vspace{3mm}
\textbf{1. Colah's Blog:} ``Understanding LSTM Networks''
\begin{itemize}
\item Best visual explanation available
\item Step-by-step diagrams
\item Intuitive gate explanations
\item URL: colah.github.io
\item \textbf{Start here!}
\end{itemize}

\vspace{3mm}
\textbf{2. Karpathy's Blog:} ``Unreasonable Effectiveness of RNNs''
\begin{itemize}
\item Character-level text generation
\item Real code examples
\item Intuitive explanations
\item URL: karpathy.github.io
\item Shows what's possible
\end{itemize}

\vspace{3mm}
\textbf{3. Distill.pub:} Visual explanations
\begin{itemize}
\item Interactive visualizations
\item Attention mechanisms
\item Modern architectures
\end{itemize}

\column{0.48\textwidth}
\textbf{Key Research Papers:}

\vspace{3mm}
\textbf{Original Paper:}
\begin{itemize}
\item Hochreiter \& Schmidhuber (1997)
\item ``Long Short-Term Memory''
\item Neural Computation
\item Introduced LSTM architecture
\end{itemize}

\vspace{3mm}
\textbf{Important Extensions:}
\begin{itemize}
\item Graves (2013): Generating sequences with RNNs
\item Cho et al. (2014): GRU architecture
\item Bahdanau et al. (2014): Attention mechanism
\item Graves et al. (2013): Speech recognition with LSTMs
\end{itemize}

\vspace{3mm}
\textbf{Online Courses:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Stanford CS224n}: NLP with Deep Learning
  \begin{itemize}
  \item Free lecture videos
  \item Assignments available
  \end{itemize}
\item \textbf{Fast.ai}: Practical Deep Learning
  \begin{itemize}
  \item Code-first approach
  \item Real applications
  \end{itemize}
\item \textbf{DeepLearning.AI}: Sequence Models (Coursera)
  \begin{itemize}
  \item Andrew Ng's course
  \item Comprehensive coverage
  \end{itemize}
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}Theory first, then practice}

\end{columns}
\end{frame}

\begin{frame}[t]{Practical Resources and Implementation}
\begin{center}
\textbf{Hands-On Learning Resources}
\end{center}
\vspace{3mm}

\begin{columns}[t]
\column{0.48\textwidth}
\textbf{Code Tutorials:}

\vspace{3mm}
\begin{itemize}
\item \textbf{PyTorch LSTM Tutorial}
  \begin{itemize}
  \item Official documentation
  \item Complete examples
  \item Best starting point
  \end{itemize}
\item \textbf{TensorFlow RNN Guide}
  \begin{itemize}
  \item Keras API examples
  \item Production-ready code
  \end{itemize}
\item \textbf{Keras LSTM Examples}
  \begin{itemize}
  \item High-level API
  \item Quick prototyping
  \end{itemize}
\end{itemize}

\vspace{3mm}
\textbf{Practice Datasets:}

\vspace{3mm}
\begin{itemize}
\item \textbf{Penn Treebank}: Language modeling benchmark
\item \textbf{IMDB Reviews}: Sentiment analysis (50K reviews)
\item \textbf{Time Series}: Stock prices, weather data
\item \textbf{Text8}: Character-level modeling (100MB text)
\end{itemize}

\column{0.48\textwidth}
\textbf{Your Learning Path:}

\vspace{3mm}
\begin{enumerate}
\item \textbf{Implement from scratch} (1-2 days)
  \begin{itemize}
  \item Understand every equation
  \item Debug by hand
  \end{itemize}
\item \textbf{Train on Penn Treebank} (1 day)
  \begin{itemize}
  \item Language modeling task
  \item Monitor perplexity
  \end{itemize}
\item \textbf{Visualize gate activations} (half day)
  \begin{itemize}
  \item See what gates learn
  \item Build intuition
  \end{itemize}
\item \textbf{Compare with GRU} (1 day)
  \begin{itemize}
  \item Same task, different architecture
  \item Understand trade-offs
  \end{itemize}
\item \textbf{Study Transformers} (1 week)
  \begin{itemize}
  \item Next evolution
  \item Self-attention mechanism
  \end{itemize}
\item \textbf{Explore modern architectures} (ongoing)
  \begin{itemize}
  \item BERT, GPT, T5
  \item Apply to real problems
  \end{itemize}
\end{enumerate}

\vspace{3mm}
\textbf{Essential Tools:}

\begin{itemize}
\item \textbf{PyTorch / TensorFlow}: Deep learning frameworks
\item \textbf{Weights \& Biases}: Experiment tracking
\item \textbf{TensorBoard}: Loss/activation visualization
\item \textbf{Jupyter}: Interactive development
\end{itemize}

\vspace{3mm}
{\footnotesize\color{annotGray}Best learning: Implement, experiment, fail, debug, understand}

\end{columns}
\end{frame}