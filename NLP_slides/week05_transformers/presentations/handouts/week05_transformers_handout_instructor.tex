\documentclass[8pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{hyperref}

% Colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{answercolor}{rgb}{0,0,0.8}

% Code style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Custom commands
\newcommand{\question}[1]{\vspace{0.5em}\noindent\textbf{Q: #1}\vspace{0.3em}}
\newcommand{\exercise}[1]{\vspace{0.5em}\noindent\textbf{Exercise: #1}\vspace{0.3em}}
\newcommand{\think}[1]{\vspace{0.3em}\noindent\textit{Think: #1}\vspace{0.3em}}
\newcommand{\answer}[1]{\textcolor{answercolor}{\textbf{Answer: #1}}}
\newcommand{\discovery}[1]{
    \begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Discovery]
    #1
    \end{tcolorbox}
}
\newcommand{\hint}[1]{
    \begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Hint]
    #1
    \end{tcolorbox}
}
\newcommand{\teaching}[1]{
    \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Teaching Note]
    #1
    \end{tcolorbox}
}

\title{Week 5: Attention Is All You Need - Transformers\\
\large Discovery-Based Learning Exercises (Instructor Version)}
\date{}

\begin{document}
\maketitle

\section*{Learning Objectives}
By the end of this session, students will:
\begin{itemize}
    \item Discover why RNNs have fundamental limitations
    \item Reinvent self-attention from first principles
    \item Design the multi-head attention mechanism
    \item Build a complete transformer architecture
    \item Understand positional encodings
\end{itemize}

\teaching{
This session uses discovery-based learning. Let students struggle with problems before revealing solutions. The "aha!" moments are crucial for deep understanding.
}

\hrule
\vspace{1em}

\section{Part 1: The Sequential Processing Problem (10 minutes)}

\subsection{The Waiting Game}

\exercise{Let's process this sentence with an RNN. Mark the dependencies:}

"The student who studied hard and completed all assignments passed the exam"

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Word} & \textbf{Step} & \textbf{Depends on previous steps?} \\
\hline
The & 1 & No \\
student & 2 & Yes (step 1) \\
who & 3 & \answer{Yes (steps 1-2)} \\
studied & 4 & \answer{Yes (steps 1-3)} \\
hard & 5 & \answer{Yes (steps 1-4)} \\
and & 6 & \answer{Yes (steps 1-5)} \\
completed & 7 & \answer{Yes (steps 1-6)} \\
all & 8 & \answer{Yes (steps 1-7)} \\
assignments & 9 & \answer{Yes (steps 1-8)} \\
passed & 10 & \answer{Yes (steps 1-9)} \\
the & 11 & \answer{Yes (steps 1-10)} \\
exam & 12 & \answer{Yes (steps 1-11)} \\
\hline
\end{tabular}
\end{center}

\question{Can we process "passed" before we process "assignments"? Why or why not in an RNN?}

\answer{No! In an RNN, each step depends on the hidden state from the previous step. This creates a sequential bottleneck - we must process words 1-9 before we can process word 10.}

\subsection{Parallelization Challenge}

\think{You have 12 GPUs. How many can you use simultaneously to process this sentence with an RNN?}

Answer: \answer{Only 1! RNN processing is inherently sequential.}

\question{What if you could look at ALL words at once instead of sequentially?}

\answer{We could use all 12 GPUs in parallel, achieving 12x speedup! This is the key insight behind transformers.}

\teaching{
Emphasize the parallelization limitation - this is the primary motivation for transformers. Modern GPUs have thousands of cores that RNNs can't fully utilize.
}

\section{Part 2: Inventing Self-Attention (15 minutes)}

\subsection{The Direct Connection Idea}

\exercise{Instead of passing information step-by-step, let's connect every word directly to every other word.}

\answer{Students should draw all possible bidirectional arrows: The$\leftrightarrow$cat, The$\leftrightarrow$sat, cat$\leftrightarrow$sat, plus self-connections.}

\question{How many connections did you draw? For a sentence with n words, how many connections would we need?}

Connections = \answer{$n^2$ (including self-connections)}

\teaching{
This quadratic complexity is important - it's both the strength (all connections) and weakness (computational cost) of transformers.
}

\subsection{Computing Relevance}

\exercise{For each word pair, assign a relevance score (0-1):}

When processing "sat", how relevant is each word?

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Query: "sat"} & \textbf{Key} & \textbf{Relevance Score} \\
\hline
sat looks at $\rightarrow$ & The & \answer{0.1} \\
sat looks at $\rightarrow$ & cat & \answer{0.7} \\
sat looks at $\rightarrow$ & sat & \answer{0.2} \\
\hline
\textbf{Total} & & 1.0 \\
\hline
\end{tabular}
\end{center}

\teaching{
Students' exact numbers will vary - that's fine! The key insight is that "cat" (the subject) should have high relevance to "sat" (the verb).
}

\subsection{The Three Roles}

\question{Each word needs to play three roles. Can you identify them?}

\begin{enumerate}
    \item \textbf{Q}\answer{uery}: The word asking "who is relevant to me?"
    \item \textbf{K}\answer{ey}: The word answering "here's what I offer"
    \item \textbf{V}\answer{alue}: The word providing "here's my actual information"
\end{enumerate}

\subsection{The Attention Formula}

\exercise{Design the attention mechanism. Fill in the steps:}

\begin{enumerate}
    \item Compute similarity: $Q \times K^T = $ \answer{attention scores}
    \item Scale by: $\frac{1}{\sqrt{d_k}}$ where $d_k = $ \answer{dimension of keys} (why scale? \answer{To prevent softmax saturation})
    \item Apply \answer{softmax} to get weights that sum to 1
    \item Multiply by \answer{V} to get final output
\end{enumerate}

\section{Part 3: Why Multiple Heads? (10 minutes)}

\subsection{Different Types of Relationships}

\exercise{Consider the sentence: "The bank by the river bank"}

First "bank" should attend to different words for different reasons:
\begin{itemize}
    \item Syntactic: "bank" is a \answer{noun}
    \item Semantic: "bank" means \answer{financial institution}
    \item Position: "bank" is the \answer{second} word
\end{itemize}

\question{Can a single attention pattern capture all these relationships?}

\answer{No! Different relationships require different attention patterns. This motivates multi-head attention.}

\subsection{Multi-Head Design}

\exercise{Design multi-head attention:}

\begin{enumerate}
    \item Number of parallel attentions: \answer{8-16} (typically 8)
    \item Each head size: $\frac{d_{model}}{n_{heads}}$ where $n_{heads} = $ \answer{8-16}
    \item How to combine outputs: \answer{Concatenate then linear projection}
\end{enumerate}

\teaching{
Explain that each head learns different patterns: one might focus on syntax, another on semantics, another on position, etc.
}

\section{Part 4: The Position Problem (10 minutes)}

\subsection{Order Blindness}

\question{What information is self-attention missing?}

\answer{Position/order information! Self-attention is permutation invariant - it produces the same result regardless of word order.}

\subsection{Encoding Position}

\exercise{Evaluate these approaches:}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Approach} & \textbf{Pros} & \textbf{Cons} \\
\hline
Add position number [1,2,3...] & Simple & \answer{Can't generalize beyond training length} \\
Learn position embeddings & Flexible & \answer{Fixed maximum length} \\
Use sin/cos waves & \answer{Generalizes to any length} & Complex \\
\hline
\end{tabular}
\end{center}

\subsection{Sinusoidal Encoding}

\exercise{Fill in the position encoding formula:}

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$ \answer{(uses sine for even dimensions)}
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$ \answer{(uses cosine for odd dimensions)}

\teaching{
The sinusoidal encoding creates unique patterns for each position and allows the model to learn relative positions through trigonometric identities.
}

\section{Part 5: Building the Complete Transformer (15 minutes)}

\subsection{Layer Design}

\exercise{Design one transformer layer. What components do we need?}

\begin{enumerate}
    \item \answer{Multi-head attention} (computes attention)
    \item \answer{Residual connection} (adds shortcut)
    \item \answer{Layer normalization} (normalizes)
    \item \answer{Feed-forward network} (processes each position)
    \item \answer{Residual connection} (another shortcut)
    \item \answer{Layer normalization} (normalizes again)
\end{enumerate}

\subsection{The Feed-Forward Network}

\question{After attention, why do we need position-wise feed-forward networks?}

\answer{Attention combines information from different positions. FFN processes each position independently, adding non-linearity and increasing model capacity.}

\subsection{Residual Connections}

\exercise{Why add the input to the output (residual/skip connections)?}

Benefits:
\begin{enumerate}
    \item Gradient flow: \answer{Prevents vanishing gradients in deep networks}
    \item Information preservation: \answer{Allows model to preserve original information}
    \item Training stability: \answer{Makes deep networks easier to optimize}
\end{enumerate}

\subsection{Stack and Scale}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Layers} & \textbf{Parameters} \\
\hline
BERT-Base & 12 & 110M \\
GPT-2 & \answer{48} & 1.5B \\
GPT-3 & \answer{96} & 175B \\
\hline
\end{tabular}
\end{center}

\section{Part 6: Advantages Analysis (10 minutes)}

\subsection{Parallelization}

\exercise{Compare processing time:}

100-word sequence:
\begin{itemize}
    \item RNN: 100 sequential steps = \answer{100} time units
    \item Transformer: \answer{1} parallel step(s) = \answer{1} time unit(s)
\end{itemize}

Speedup factor: \answer{100}$\times$

\subsection{Long-Range Dependencies}

\question{How many steps for word 1 to influence word 100?}

\begin{itemize}
    \item RNN: \answer{99} steps (through all intermediate)
    \item Transformer: \answer{1} step(s) (direct connection)
\end{itemize}

\teaching{
This direct connection is why transformers handle long-range dependencies better than RNNs, which suffer from gradient vanishing over long sequences.
}

\section{Coding Challenge: Build Your Own Attention}

\begin{lstlisting}[language=Python]
import numpy as np

def self_attention(Q, K, V):
    """
    Q, K, V: matrices of shape (seq_len, d_k)
    """
    d_k = Q.shape[1]
    
    # Step 1: Compute scores
    scores = np.dot(Q, K.T)  # ANSWER
    
    # Step 2: Scale
    scores = scores / np.sqrt(d_k)  # ANSWER
    
    # Step 3: Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))
    weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # ANSWER
    
    # Step 4: Apply to values
    output = np.dot(weights, V)  # ANSWER
    
    return output, weights

# Test it!
seq_len, d_k = 4, 8
Q = np.random.randn(seq_len, d_k)
K = np.random.randn(seq_len, d_k)
V = np.random.randn(seq_len, d_k)

output, weights = self_attention(Q, K, V)
print("Attention weights:", weights)
print("Do weights sum to 1?", np.allclose(weights.sum(axis=1), 1))
\end{lstlisting}

\teaching{
Have students implement this - it's the core of transformers! The actual implementation is surprisingly simple once they understand the concept.
}

\section{Reflection Questions}

\question{Why is the paper titled "Attention Is All You Need"?}

\answer{Because transformers completely replace RNNs/CNNs with attention mechanisms. No recurrence, no convolutions - just attention!}

\question{What tasks beyond NLP could benefit from transformers?}

\answer{Computer vision (ViT), protein folding (AlphaFold), music generation, time series forecasting, etc. Any sequential or structured data!}

\question{What are potential limitations of transformers?}

\answer{
\begin{itemize}
    \item Quadratic memory/compute complexity with sequence length
    \item Require lots of data and compute for training
    \item Lack of inherent inductive biases (need more data)
    \item Difficulty with very long sequences (though improving)
\end{itemize}
}

\hrule
\vspace{1em}

\section*{Teaching Summary}

Key concepts students should take away:
\begin{enumerate}
    \item \textbf{Parallelization}: Transformers process all positions simultaneously
    \item \textbf{Self-attention}: Direct connections between all positions
    \item \textbf{Multi-head}: Different heads for different relationships
    \item \textbf{Position encoding}: Necessary for sequence order
    \item \textbf{Scaling}: Transformers scale better than any previous architecture
\end{enumerate}

\teaching{
End with excitement about the transformer revolution! These concepts power ChatGPT, DALL-E, and virtually all modern AI systems. Students have just understood one of the most important innovations in AI history.
}

\end{document}