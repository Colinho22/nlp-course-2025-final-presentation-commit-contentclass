{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**Decoding Methods Implemented (6 total)**:\n",
    "1. Greedy - Fastest but repetitive\n",
    "2. Beam Search - Better quality, still deterministic\n",
    "3. Temperature - Simple randomness control\n",
    "4. Top-k - Fixed vocabulary filtering\n",
    "5. Nucleus (Top-p) - Dynamic filtering, modern standard\n",
    "6. **Contrastive (NEW)** - Explicit degeneration prevention\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Quality-Diversity Tradeoff**: No method dominates all others\n",
    "- **Task Matters**: Factual → Greedy/Beam, Creative → Nucleus/Contrastive\n",
    "- **Contrastive Search**: Best for long creative generation (prevents repetition)\n",
    "- **Nucleus Sampling**: Best general-purpose choice (GPT-4, ChatGPT, Claude use this)\n",
    "\n",
    "### Recommended Settings (2025)\n",
    "\n",
    "| Task Type | Method | Parameters |\n",
    "|-----------|--------|------------|\n",
    "| Factual Q&A | Greedy | T=0.1-0.3 |\n",
    "| Translation | Beam Search | width=4-5 |\n",
    "| Code | Greedy/Beam | T=0, width=3 |\n",
    "| Dialogue | Nucleus | p=0.85-0.95, T=0.7-0.9 |\n",
    "| Creative Writing | Nucleus | p=0.9-0.95, T=0.9-1.2 |\n",
    "| Long Stories | Contrastive | α=0.5-0.7, k=4-6 |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with hybrid methods (Nucleus + Temperature)\n",
    "- Try constrained decoding for structured outputs\n",
    "- Explore learned decoding strategies (RLHF, DPO - Week 10)\n",
    "- Implement speculative decoding for speed (advanced topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of task-method fit\n",
    "print(\"\\nTASK-METHOD FIT SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Task':<20} {'Method':<25} {'Diversity':<12} {'Repetition':<12}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for result in task_results:\n",
    "    print(f\"{result['task']:<20} {result['method']:<25} {result['diversity']:<12.3f} {result['repetition']:<12.1f}%\")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nKEY INSIGHT: Different tasks need different decoding strategies!\")\n",
    "print(\"  - Factual tasks: Use deterministic methods (Greedy, Beam)\")\n",
    "print(\"  - Creative tasks: Use stochastic methods (Nucleus, Contrastive)\")\n",
    "print(\"  - Long generation: Use Contrastive to prevent degeneration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 4 tasks with optimal methods\n",
    "tasks = [\n",
    "    {\n",
    "        'name': 'Factual QA',\n",
    "        'prompt': 'The capital of France is',\n",
    "        'optimal_method': 'Greedy or Low Temp',\n",
    "        'generate': lambda p: greedy_decode(p, 20)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Creative Story',\n",
    "        'prompt': 'In a distant galaxy',\n",
    "        'optimal_method': 'Contrastive (long text)',\n",
    "        'generate': lambda p: contrastive_search(p, alpha=0.6, k=4, max_length=60)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Dialogue',\n",
    "        'prompt': 'How are you today?',\n",
    "        'optimal_method': 'Nucleus (natural variation)',\n",
    "        'generate': lambda p: top_p_sampling(p, p=0.9, temperature=0.8, max_length=40)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Code Completion',\n",
    "        'prompt': 'def calculate_fibonacci(n):',\n",
    "        'optimal_method': 'Beam Search (correctness)',\n",
    "        'generate': lambda p: beam_search_decode(p, beam_size=3, max_length=50)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"TASK-SPECIFIC EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "task_results = []\n",
    "for task in tasks:\n",
    "    print(f\"\\nTask: {task['name']}\")\n",
    "    print(f\"Prompt: \\\"{task['prompt']}\\\"\")\n",
    "    print(f\"Optimal Method: {task['optimal_method']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    output = task['generate'](task['prompt'])\n",
    "    rep_rate = count_repeated_ngrams(output, n=3)[0]\n",
    "    diversity = calculate_distinct_n(output, n=2)\n",
    "    \n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"\\nMetrics: Diversity={diversity:.3f}, Repetition={rep_rate:.1f}%\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    task_results.append({\n",
    "        'task': task['name'],\n",
    "        'method': task['optimal_method'],\n",
    "        'diversity': diversity,\n",
    "        'repetition': rep_rate,\n",
    "        'output': output[:100] + '...' if len(output) > 100 else output\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 15: Task-Specific Experiments (NEW)\n",
    "\n",
    "Test optimal method for 4 different NLP tasks:\n",
    "1. **Factual QA**: Need correct answer\n",
    "2. **Creative Story**: Need diversity\n",
    "3. **Dialogue**: Need natural variation\n",
    "4. **Code Completion**: Need syntactic correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Quality-Diversity Pareto Frontier\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "method_colors = {\n",
    "    'Greedy': '#E74C3C', 'Beam-3': '#E74C3C',\n",
    "    'Temp=0.7': '#95A5A6', 'Top-k=40': '#95A5A6',\n",
    "    'Nucleus=0.9': '#27AE60', 'Contrastive': '#27AE60'\n",
    "}\n",
    "\n",
    "for result in results_all:\n",
    "    color = method_colors.get(result['method'], '#3333B2')\n",
    "    ax.scatter(result['diversity'], result['quality'], \n",
    "              s=400, c=color, alpha=0.7, edgecolors='black', linewidths=2,\n",
    "              label=result['method'])\n",
    "    \n",
    "    # Add method label\n",
    "    ax.annotate(result['method'], \n",
    "               (result['diversity'], result['quality']),\n",
    "               xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Optimal zone\n",
    "from matplotlib.patches import Rectangle\n",
    "optimal_zone = Rectangle((0.65, 75), 0.30, 20, \n",
    "                         linewidth=2, edgecolor='green', facecolor='green',\n",
    "                         alpha=0.1, linestyle='--')\n",
    "ax.add_patch(optimal_zone)\n",
    "ax.text(0.80, 87, 'Optimal\\nZone', ha='center', fontsize=11, \n",
    "       fontweight='bold', color='green')\n",
    "\n",
    "ax.set_xlabel('Diversity (Distinct-2)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Quality (100 - Repetition %)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('All 6 Methods: Quality-Diversity Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY OBSERVATIONS:\")\n",
    "print(\"- Greedy/Beam: High quality, low diversity (bottom-left)\")\n",
    "print(\"- Temperature: Variable quality-diversity\")\n",
    "print(\"- Nucleus/Contrastive: High quality AND high diversity (top-right - optimal!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_simple(text):\n",
    "    \"\"\"Approximate perplexity of generated text.\"\"\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "def full_evaluation(generate_fn, prompt, method_name, n_samples=5):\n",
    "    \"\"\"Complete quality-diversity evaluation.\"\"\"\n",
    "    outputs = [generate_fn(prompt, max_length=50) for _ in range(n_samples)]\n",
    "    \n",
    "    # Diversity metrics\n",
    "    distinct_1 = np.mean([calculate_distinct_n(out, 1) for out in outputs])\n",
    "    distinct_2 = np.mean([calculate_distinct_n(out, 2) for out in outputs])\n",
    "    repetition = np.mean([count_repeated_ngrams(out, 3)[0] for out in outputs])\n",
    "    \n",
    "    # Quality metric\n",
    "    perplexities = [compute_perplexity_simple(out) for out in outputs]\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'diversity': distinct_2,\n",
    "        'quality': 100 - repetition,  # Higher is better\n",
    "        'perplexity': avg_perplexity,\n",
    "        'distinct-1': distinct_1,\n",
    "        'distinct-2': distinct_2,\n",
    "        'repetition': repetition,\n",
    "        'sample': outputs[0]\n",
    "    }\n",
    "\n",
    "# Evaluate all 6 methods\n",
    "prompt_eval = \"The future of artificial intelligence\"\n",
    "\n",
    "all_methods = [\n",
    "    ('Greedy', lambda p, m: greedy_decode(p, m)),\n",
    "    ('Beam-3', lambda p, m: beam_search_decode(p, 3, m)),\n",
    "    ('Temp=0.7', lambda p, m: sample_with_temperature(p, 0.7, m)),\n",
    "    ('Top-k=40', lambda p, m: top_k_sampling(p, 40, 0.8, m)),\n",
    "    ('Nucleus=0.9', lambda p, m: top_p_sampling(p, 0.9, 0.8, m)),\n",
    "    ('Contrastive', lambda p, m: contrastive_search(p, 0.6, 4, m)),\n",
    "]\n",
    "\n",
    "print(\"COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Prompt: {prompt_eval}\")\n",
    "print(f\"Metrics: Distinct-2 (diversity), Quality (100-repetition%), Perplexity\\n\")\n",
    "\n",
    "results_all = []\n",
    "for method_name, method_fn in all_methods:\n",
    "    print(f\"Evaluating {method_name}...\")\n",
    "    result = full_evaluation(method_fn, prompt_eval, method_name, n_samples=3)\n",
    "    results_all.append(result)\n",
    "    \n",
    "    print(f\"  Diversity: {result['diversity']:.3f} | Quality: {result['quality']:.1f} | PPL: {result['perplexity']:.1f}\")\n",
    "    print(f\"  Sample: {result['sample'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Quality-Diversity Metrics Analysis (NEW)\n",
    "\n",
    "Automated analysis of quality-diversity tradeoffs across all 6 methods.\n",
    "\n",
    "We'll plot all methods on a 2D space to visualize the Pareto frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Contrastive vs Nucleus on long generation\n",
    "long_prompt = \"Once upon a time in a faraway land\"\n",
    "\n",
    "print(\"LONG GENERATION TEST (100 tokens)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Prompt: {long_prompt}\\n\")\n",
    "\n",
    "# Nucleus (may have repetition)\n",
    "nucleus_long = top_p_sampling(long_prompt, p=0.9, temperature=0.8, max_length=100)\n",
    "nucleus_rep = count_repeated_ngrams(nucleus_long, n=3)[0]\n",
    "\n",
    "print(f\"NUCLEUS (p=0.9, T=0.8):\")\n",
    "print(nucleus_long)\n",
    "print(f\"\\nRepetition rate: {nucleus_rep:.1f}%\")\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Contrastive (should have less repetition)\n",
    "contrastive_long = contrastive_search(long_prompt, alpha=0.6, k=4, max_length=100)\n",
    "contrastive_rep = count_repeated_ngrams(contrastive_long, n=3)[0]\n",
    "\n",
    "print(f\"CONTRASTIVE (α=0.6, k=4):\")\n",
    "print(contrastive_long)\n",
    "print(f\"\\nRepetition rate: {contrastive_rep:.1f}%\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nConclusion: Contrastive reduced repetition from {nucleus_rep:.1f}% to {contrastive_rep:.1f}%!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_search(prompt, alpha=0.6, k=4, max_length=50):\n",
    "    \"\"\"\n",
    "    Contrastive search decoding (Hugging Face 2024).\n",
    "    \n",
    "    Args:\n",
    "        alpha: Penalty weight (0=greedy, 0.6=balanced, 1.0=max diversity)\n",
    "        k: Number of candidates to consider\n",
    "    \n",
    "    Score = (1-alpha)*P(token) - alpha*max_similarity_to_context\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, output_hidden_states=True)\n",
    "            logits = outputs.logits\n",
    "            hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Get top-k candidates by probability\n",
    "        top_k_probs, top_k_ids = torch.topk(log_probs, k)\n",
    "        \n",
    "        # Get embeddings for context tokens\n",
    "        context_embeddings = hidden_states[0, :-1, :]  # All except last\n",
    "        \n",
    "        # Get embeddings for candidate tokens\n",
    "        with torch.no_grad():\n",
    "            # Create dummy sequences with each candidate\n",
    "            candidate_scores = []\n",
    "            \n",
    "            for i in range(k):\n",
    "                candidate_id = top_k_ids[0, i]\n",
    "                prob_score = top_k_probs[0, i].item()\n",
    "                \n",
    "                # For simplicity, use embedding from model\n",
    "                candidate_embedding = model.transformer.wte(candidate_id.unsqueeze(0))\n",
    "                \n",
    "                # Compute similarity to all context tokens\n",
    "                similarities = torch.cosine_similarity(\n",
    "                    candidate_embedding.expand(context_embeddings.size(0), -1),\n",
    "                    context_embeddings,\n",
    "                    dim=-1\n",
    "                )\n",
    "                \n",
    "                # Max similarity (worst case)\n",
    "                max_sim = similarities.max().item() if len(similarities) > 0 else 0.0\n",
    "                \n",
    "                # Contrastive score\n",
    "                score = (1 - alpha) * prob_score - alpha * max_sim\n",
    "                candidate_scores.append((score, candidate_id))\n",
    "        \n",
    "        # Select candidate with highest contrastive score\n",
    "        best_score, next_token = max(candidate_scores, key=lambda x: x[0])\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test contrastive search\n",
    "prompt = \"The weather is\"\n",
    "print(\"Testing Contrastive Search...\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for alpha in [0.0, 0.4, 0.6, 0.8]:\n",
    "    output = contrastive_search(prompt, alpha=alpha, k=4, max_length=40)\n",
    "    print(f\"α={alpha}: {output}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Contrastive Search (NEW 2025)\n",
    "\n",
    "Explicit degeneration prevention through similarity penalty.\n",
    "\n",
    "**Key Innovation**: Penalize tokens similar to recent context to avoid repetition in long generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Lab: Decoding Strategies\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement greedy decoding, beam search, and sampling methods\n",
    "- Compare decoding strategies on real language models\n",
    "- Understand temperature, top-k, and top-p parameters\n",
    "- Evaluate trade-offs between accuracy and creativity\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install transformers torch numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Week 9: Decoding Strategies Lab')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Pre-trained Model\n",
    "\n",
    "We'll use GPT-2 for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"The weather is\",\n",
    "    \"Once upon a time\",\n",
    "    \"In a shocking discovery\",\n",
    "    \"The capital of France is\",\n",
    "    \"def factorial(n):\"\n",
    "]\n",
    "\n",
    "print(\"Test prompts:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{i}. \\\"{prompt}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Greedy Decoding\n",
    "\n",
    "Always pick the highest probability token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(prompt, max_length=50):\n",
    "    \"\"\"Implement greedy decoding from scratch.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get last token logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Greedy: pick argmax\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "        \n",
    "        # Stop at end-of-sequence\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test greedy decoding\n",
    "prompt = test_prompts[0]\n",
    "output = greedy_decode(prompt, max_length=30)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test greedy on all prompts\n",
    "print(\"=\" * 60)\n",
    "print(\"GREEDY DECODING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    output = greedy_decode(prompt, max_length=30)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Analyze Repetition\n",
    "\n",
    "Count how many times greedy decoding produces repeated n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_repeated_ngrams(text, n=3):\n",
    "    \"\"\"Count repeated n-grams in text.\"\"\"\n",
    "    words = text.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    # Count unique vs total\n",
    "    unique_ngrams = len(set(ngrams))\n",
    "    total_ngrams = len(ngrams)\n",
    "    \n",
    "    repetition_rate = (1 - unique_ngrams / max(total_ngrams, 1)) * 100\n",
    "    \n",
    "    return repetition_rate, unique_ngrams, total_ngrams\n",
    "\n",
    "# Test on greedy output\n",
    "prompt = \"The weather is\"\n",
    "output = greedy_decode(prompt, max_length=50)\n",
    "rep_rate, unique, total = count_repeated_ngrams(output, n=3)\n",
    "\n",
    "print(f\"Text: {output}\")\n",
    "print(f\"\\n3-gram repetition rate: {rep_rate:.1f}%\")\n",
    "print(f\"Unique 3-grams: {unique}/{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Beam Search\n",
    "\n",
    "Keep track of multiple hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(prompt, beam_size=3, max_length=50):\n",
    "    \"\"\"Implement beam search decoding.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Initialize beam: (sequence, score)\n",
    "    beams = [(input_ids, 0.0)]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(seq)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Get top-k candidates\n",
    "            top_k_probs, top_k_ids = torch.topk(log_probs, beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                next_token = top_k_ids[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                next_score = score + top_k_probs[0, i].item()\n",
    "                next_seq = torch.cat([seq, next_token], dim=-1)\n",
    "                \n",
    "                all_candidates.append((next_seq, next_score))\n",
    "        \n",
    "        # Keep top beam_size\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        beams = ordered[:beam_size]\n",
    "        \n",
    "        # Check if best beam ended\n",
    "        if beams[0][0][0, -1].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Return best sequence\n",
    "    best_seq = beams[0][0]\n",
    "    return tokenizer.decode(best_seq[0], skip_special_tokens=True)\n",
    "\n",
    "# Test beam search\n",
    "prompt = test_prompts[0]\n",
    "output_beam = beam_search_decode(prompt, beam_size=3, max_length=30)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Beam Search (size=3): {output_beam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare greedy vs beam search\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "greedy_out = greedy_decode(prompt, max_length=40)\n",
    "beam3_out = beam_search_decode(prompt, beam_size=3, max_length=40)\n",
    "beam5_out = beam_search_decode(prompt, beam_size=5, max_length=40)\n",
    "\n",
    "print(\"Comparison: Greedy vs Beam Search\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Greedy:     {greedy_out}\\n\")\n",
    "print(f\"Beam-3:     {beam3_out}\\n\")\n",
    "print(f\"Beam-5:     {beam5_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Beam Size Analysis\n",
    "\n",
    "Test different beam sizes and measure diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test beam sizes 1, 3, 5, 10\n",
    "beam_sizes = [1, 3, 5, 10]\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "results = []\n",
    "for beam_size in beam_sizes:\n",
    "    output = beam_search_decode(prompt, beam_size=beam_size, max_length=40)\n",
    "    rep_rate, _, _ = count_repeated_ngrams(output, n=3)\n",
    "    results.append((beam_size, output, rep_rate))\n",
    "\n",
    "print(\"Beam Size Analysis\")\n",
    "print(\"=\" * 70)\n",
    "for beam_size, output, rep_rate in results:\n",
    "    print(f\"\\nBeam={beam_size} | Repetition: {rep_rate:.1f}%\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Temperature Sampling\n",
    "\n",
    "Control randomness with temperature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(prompt, temperature=1.0, max_length=50):\n",
    "    \"\"\"Generate text with temperature sampling.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        probs = torch.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sample from distribution\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test different temperatures\n",
    "prompt = \"The weather is\"\n",
    "temperatures = [0.1, 0.5, 0.7, 1.0, 1.5]\n",
    "\n",
    "print(\"Temperature Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    output = sample_with_temperature(prompt, temperature=temp, max_length=30)\n",
    "    print(f\"T={temp:<4}: {output}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple samples at same temperature\n",
    "prompt = \"Once upon a time\"\n",
    "temperature = 0.8\n",
    "n_samples = 5\n",
    "\n",
    "print(f\"Multiple samples with T={temperature}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for i in range(n_samples):\n",
    "    output = sample_with_temperature(prompt, temperature=temperature, max_length=30)\n",
    "    print(f\"Sample {i+1}: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Top-k Sampling\n",
    "\n",
    "Sample from top-k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(prompt, k=50, temperature=1.0, max_length=50):\n",
    "    \"\"\"Generate text with top-k sampling.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Get top-k\n",
    "        top_k_logits, top_k_indices = torch.topk(next_token_logits, k)\n",
    "        \n",
    "        # Sample from top-k\n",
    "        probs = torch.softmax(top_k_logits, dim=-1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "        next_token = top_k_indices[0, next_token_idx]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test different k values\n",
    "prompt = \"In a shocking discovery\"\n",
    "k_values = [5, 20, 50, 100]\n",
    "\n",
    "print(\"Top-k Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for k in k_values:\n",
    "    output = top_k_sampling(prompt, k=k, temperature=0.8, max_length=30)\n",
    "    print(f\"k={k:<4}: {output}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Top-p (Nucleus) Sampling\n",
    "\n",
    "Sample from nucleus with cumulative probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(prompt, p=0.9, temperature=1.0, max_length=50):\n",
    "    \"\"\"Generate text with top-p (nucleus) sampling.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Sort probabilities\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        \n",
    "        # Find cutoff for nucleus\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        cutoff_idx = torch.where(cumsum_probs >= p)[0][0] + 1\n",
    "        \n",
    "        # Keep only nucleus\n",
    "        nucleus_probs = sorted_probs[:, :cutoff_idx]\n",
    "        nucleus_indices = sorted_indices[:, :cutoff_idx]\n",
    "        \n",
    "        # Renormalize and sample\n",
    "        nucleus_probs = nucleus_probs / nucleus_probs.sum()\n",
    "        next_token_idx = torch.multinomial(nucleus_probs, num_samples=1)\n",
    "        next_token = nucleus_indices[0, next_token_idx]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test different p values\n",
    "prompt = \"The future of technology\"\n",
    "p_values = [0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "\n",
    "print(\"Top-p Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for p in p_values:\n",
    "    output = top_p_sampling(prompt, p=p, temperature=0.8, max_length=30)\n",
    "    print(f\"p={p:<5}: {output}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Comprehensive Comparison\n",
    "\n",
    "Compare all methods on the same prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_methods(prompt, max_length=40):\n",
    "    \"\"\"Compare all decoding strategies.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Greedy\n",
    "    results['Greedy'] = greedy_decode(prompt, max_length)\n",
    "    \n",
    "    # Beam search\n",
    "    results['Beam-3'] = beam_search_decode(prompt, beam_size=3, max_length=max_length)\n",
    "    \n",
    "    # Temperature sampling\n",
    "    results['T=0.5'] = sample_with_temperature(prompt, temperature=0.5, max_length=max_length)\n",
    "    results['T=1.0'] = sample_with_temperature(prompt, temperature=1.0, max_length=max_length)\n",
    "    \n",
    "    # Top-k\n",
    "    results['Top-k=40'] = top_k_sampling(prompt, k=40, temperature=0.8, max_length=max_length)\n",
    "    \n",
    "    # Top-p\n",
    "    results['Top-p=0.9'] = top_p_sampling(prompt, p=0.9, temperature=0.8, max_length=max_length)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on multiple prompts\n",
    "for prompt in test_prompts:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = compare_all_methods(prompt, max_length=35)\n",
    "    \n",
    "    for method, output in results.items():\n",
    "        print(f\"\\n{method:<15}: {output}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Diversity and Quality Metrics\n",
    "\n",
    "Calculate distinct-n and repetition rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distinct_n(text, n=2):\n",
    "    \"\"\"Calculate distinct-n metric (diversity).\"\"\"\n",
    "    words = text.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    if len(ngrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def evaluate_decoding_method(generate_fn, prompt, n_samples=5, max_length=40):\n",
    "    \"\"\"Evaluate a decoding method.\"\"\"\n",
    "    outputs = [generate_fn(prompt, max_length=max_length) for _ in range(n_samples)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    distinct_1 = np.mean([calculate_distinct_n(out, n=1) for out in outputs])\n",
    "    distinct_2 = np.mean([calculate_distinct_n(out, n=2) for out in outputs])\n",
    "    repetition = np.mean([count_repeated_ngrams(out, n=3)[0] for out in outputs])\n",
    "    \n",
    "    return {\n",
    "        'distinct-1': distinct_1,\n",
    "        'distinct-2': distinct_2,\n",
    "        'repetition': repetition,\n",
    "        'samples': outputs\n",
    "    }\n",
    "\n",
    "# Compare methods\n",
    "prompt = \"The weather is\"\n",
    "\n",
    "methods = [\n",
    "    ('Greedy', lambda p, m: greedy_decode(p, m)),\n",
    "    ('T=0.7', lambda p, m: sample_with_temperature(p, 0.7, m)),\n",
    "    ('T=1.0', lambda p, m: sample_with_temperature(p, 1.0, m)),\n",
    "    ('Top-p=0.9', lambda p, m: top_p_sampling(p, 0.9, 0.8, m)),\n",
    "]\n",
    "\n",
    "print(\"Method Evaluation\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for method_name, method_fn in methods:\n",
    "    results = evaluate_decoding_method(method_fn, prompt, n_samples=5, max_length=30)\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Distinct-1: {results['distinct-1']:.3f}\")\n",
    "    print(f\"  Distinct-2: {results['distinct-2']:.3f}\")\n",
    "    print(f\"  Repetition: {results['repetition']:.1f}%\")\n",
    "    print(f\"  Sample: {results['samples'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Visualization\n",
    "\n",
    "Plot diversity vs quality trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for all methods\n",
    "methods_data = []\n",
    "prompt = \"The future of AI\"\n",
    "\n",
    "test_methods = [\n",
    "    ('Greedy', lambda p, m: greedy_decode(p, m)),\n",
    "    ('Beam-3', lambda p, m: beam_search_decode(p, 3, m)),\n",
    "    ('T=0.5', lambda p, m: sample_with_temperature(p, 0.5, m)),\n",
    "    ('T=0.8', lambda p, m: sample_with_temperature(p, 0.8, m)),\n",
    "    ('T=1.2', lambda p, m: sample_with_temperature(p, 1.2, m)),\n",
    "    ('Top-k=40', lambda p, m: top_k_sampling(p, 40, 0.8, m)),\n",
    "    ('Top-p=0.9', lambda p, m: top_p_sampling(p, 0.9, 0.8, m)),\n",
    "]\n",
    "\n",
    "for name, fn in test_methods:\n",
    "    metrics = evaluate_decoding_method(fn, prompt, n_samples=3, max_length=30)\n",
    "    methods_data.append({\n",
    "        'name': name,\n",
    "        'diversity': metrics['distinct-2'],\n",
    "        'repetition': metrics['repetition']\n",
    "    })\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for data in methods_data:\n",
    "    ax.scatter(data['diversity'], 100 - data['repetition'], s=200, alpha=0.7)\n",
    "    ax.annotate(data['name'], (data['diversity'], 100 - data['repetition']),\n",
    "               xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Diversity (Distinct-2)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Quality (100 - Repetition %)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Decoding Strategy Trade-off: Diversity vs Quality', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Real-World Task - Story Writing\n",
    "\n",
    "Compare methods on creative writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story writing task\n",
    "story_prompt = \"In a distant galaxy\"\n",
    "story_length = 80\n",
    "\n",
    "print(\"CREATIVE WRITING COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Prompt: {story_prompt}\\n\")\n",
    "\n",
    "# Greedy (boring)\n",
    "greedy_story = greedy_decode(story_prompt, max_length=story_length)\n",
    "print(\"GREEDY (deterministic, repetitive):\")\n",
    "print(greedy_story)\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Sampling (creative)\n",
    "sampling_story = top_p_sampling(story_prompt, p=0.95, temperature=0.9, max_length=story_length)\n",
    "print(\"TOP-P SAMPLING (creative, diverse):\")\n",
    "print(sampling_story)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Real-World Task - Factual QA\n",
    "\n",
    "Compare methods on factual questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factual QA task\n",
    "factual_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Water boils at\",\n",
    "    \"The speed of light is\",\n",
    "]\n",
    "\n",
    "print(\"FACTUAL QA COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for qa_prompt in factual_prompts:\n",
    "    greedy_ans = greedy_decode(qa_prompt, max_length=15)\n",
    "    sampling_ans = sample_with_temperature(qa_prompt, temperature=1.0, max_length=15)\n",
    "    \n",
    "    print(f\"\\nQ: {qa_prompt}\")\n",
    "    print(f\"Greedy:   {greedy_ans}\")\n",
    "    print(f\"Sampling: {sampling_ans}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nConclusion: Greedy is better for factual QA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Summary and Recommendations\n",
    "\n",
    "What did we learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "KEY FINDINGS:\n",
    "============\n",
    "\n",
    "1. GREEDY DECODING:\n",
    "   ✓ Fast and deterministic\n",
    "   ✗ Repetitive and boring\n",
    "   Best for: Factual QA, translation\n",
    "\n",
    "2. BEAM SEARCH:\n",
    "   ✓ Better than greedy\n",
    "   ✗ Still deterministic\n",
    "   Best for: Code generation, structured output\n",
    "\n",
    "3. TEMPERATURE SAMPLING:\n",
    "   ✓ Controls creativity\n",
    "   ✓ Easy to understand\n",
    "   Best for: Most creative tasks\n",
    "\n",
    "4. TOP-K SAMPLING:\n",
    "   ✓ Prevents tail sampling\n",
    "   ✗ Fixed vocabulary size\n",
    "   Best for: Moderate creativity\n",
    "\n",
    "5. TOP-P SAMPLING:\n",
    "   ✓ Adapts to distribution\n",
    "   ✓ Most robust\n",
    "   Best for: General use (combine with temperature)\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "===============\n",
    "- Factual QA:      Greedy or T=0.1-0.3\n",
    "- Translation:     Beam-4 or T=0.3\n",
    "- Code:            Beam-5 + T=0.2\n",
    "- Dialogue:        T=0.7-0.9, p=0.9\n",
    "- Creative:        T=0.9-1.2, p=0.95\n",
    "\n",
    "DEFAULT SETTINGS:\n",
    "================\n",
    "If unsure, use: T=0.7, p=0.9\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
